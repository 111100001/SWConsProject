{
  "dataset/_kmeans.py": [
    {
      "type": "FunctionDef",
      "name": "kmeans_plusplus",
      "md_content": [
        "**kmeans_plusplus**: The function of kmeans_plusplus is to initialize cluster centers for the k-means clustering algorithm using the k-means++ method.\n\n**parameters**: The parameters of this Function.\n· X: {array-like, sparse matrix} of shape (n_samples, n_features) - The data to pick seeds from.\n· n_clusters: int - The number of centroids to initialize.\n· sample_weight: array-like of shape (n_samples,), default=None - The weights for each observation in `X`. If `None`, all observations are assigned equal weight. `sample_weight` is ignored if `init` is a callable or a user provided array.\n· x_squared_norms: array-like of shape (n_samples,), default=None - Squared Euclidean norm of each data point.\n· random_state: int or RandomState instance, default=None - Determines random number generation for centroid initialization. Pass an int for reproducible output across multiple function calls.\n· n_local_trials: int, default=None - The number of seeding trials for each center (except the first), of which the one reducing inertia the most is greedily chosen. Set to None to make the number of trials depend logarithmically on the number of seeds (2+log(k)) which is the recommended setting.\n\n**Code Description**: The kmeans_plusplus function is designed to select initial cluster centers for the k-means clustering algorithm in an efficient manner. It employs the k-means++ initialization method, which significantly improves the convergence speed of the k-means algorithm by carefully selecting the initial centroids. \n\nThe function begins by validating the input data `X` and checking that the number of samples is greater than or equal to the number of clusters specified. It also verifies the provided `sample_weight` and calculates the squared Euclidean norms of the data points if not provided. The random state is set for reproducibility of results.\n\nThe core of the function involves calling the private helper function _kmeans_plusplus, which performs the actual initialization of the centroids. This helper function selects the first centroid randomly from the dataset, and subsequent centroids are chosen based on their distance from existing centroids, with a probability proportional to the squared distance. This method ensures that points further away from existing centers have a higher chance of being selected, which is crucial for effective clustering.\n\nThe kmeans_plusplus function ultimately returns two outputs: the initialized centers and their corresponding indices in the original dataset. This function serves as a public interface for initializing cluster centers and is utilized within the k-means algorithm implementation, ensuring that the clustering process starts with a well-informed selection of initial centroids.\n\n**Note**: It is essential to ensure that the number of samples in `X` is greater than or equal to the number of clusters specified. The function assumes that prior validation of the data has been conducted.\n\n**Output Example**: A possible return value of the function could be:\ncenters: array([[10, 2],\n                [1, 0]])\nindices: array([3, 2])"
      ],
      "code_start_line": 72,
      "code_end_line": 175,
      "params": [
        "X",
        "n_clusters"
      ],
      "have_return": true,
      "code_content": "def kmeans_plusplus(\n    X,\n    n_clusters,\n    *,\n    sample_weight=None,\n    x_squared_norms=None,\n    random_state=None,\n    n_local_trials=None,\n):\n    \"\"\"Init n_clusters seeds according to k-means++.\n\n    .. versionadded:: 0.24\n\n    Parameters\n    ----------\n    X : {array-like, sparse matrix} of shape (n_samples, n_features)\n        The data to pick seeds from.\n\n    n_clusters : int\n        The number of centroids to initialize.\n\n    sample_weight : array-like of shape (n_samples,), default=None\n        The weights for each observation in `X`. If `None`, all observations\n        are assigned equal weight. `sample_weight` is ignored if `init`\n        is a callable or a user provided array.\n\n        .. versionadded:: 1.3\n\n    x_squared_norms : array-like of shape (n_samples,), default=None\n        Squared Euclidean norm of each data point.\n\n    random_state : int or RandomState instance, default=None\n        Determines random number generation for centroid initialization. Pass\n        an int for reproducible output across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    n_local_trials : int, default=None\n        The number of seeding trials for each center (except the first),\n        of which the one reducing inertia the most is greedily chosen.\n        Set to None to make the number of trials depend logarithmically\n        on the number of seeds (2+log(k)) which is the recommended setting.\n        Setting to 1 disables the greedy cluster selection and recovers the\n        vanilla k-means++ algorithm which was empirically shown to work less\n        well than its greedy variant.\n\n    Returns\n    -------\n    centers : ndarray of shape (n_clusters, n_features)\n        The initial centers for k-means.\n\n    indices : ndarray of shape (n_clusters,)\n        The index location of the chosen centers in the data array X. For a\n        given index and center, X[index] = center.\n\n    Notes\n    -----\n    Selects initial cluster centers for k-mean clustering in a smart way\n    to speed up convergence. see: Arthur, D. and Vassilvitskii, S.\n    \"k-means++: the advantages of careful seeding\". ACM-SIAM symposium\n    on Discrete algorithms. 2007\n\n    Examples\n    --------\n\n    >>> from sklearn.cluster import kmeans_plusplus\n    >>> import numpy as np\n    >>> X = np.array([[1, 2], [1, 4], [1, 0],\n    ...               [10, 2], [10, 4], [10, 0]])\n    >>> centers, indices = kmeans_plusplus(X, n_clusters=2, random_state=0)\n    >>> centers\n    array([[10,  2],\n           [ 1,  0]])\n    >>> indices\n    array([3, 2])\n    \"\"\"\n    # Check data\n    check_array(X, accept_sparse=\"csr\", dtype=[np.float64, np.float32])\n    sample_weight = _check_sample_weight(sample_weight, X, dtype=X.dtype)\n\n    if X.shape[0] < n_clusters:\n        raise ValueError(\n            f\"n_samples={X.shape[0]} should be >= n_clusters={n_clusters}.\"\n        )\n\n    # Check parameters\n    if x_squared_norms is None:\n        x_squared_norms = row_norms(X, squared=True)\n    else:\n        x_squared_norms = check_array(x_squared_norms, dtype=X.dtype, ensure_2d=False)\n\n    if x_squared_norms.shape[0] != X.shape[0]:\n        raise ValueError(\n            f\"The length of x_squared_norms {x_squared_norms.shape[0]} should \"\n            f\"be equal to the length of n_samples {X.shape[0]}.\"\n        )\n\n    random_state = check_random_state(random_state)\n\n    # Call private k-means++\n    centers, indices = _kmeans_plusplus(\n        X, n_clusters, x_squared_norms, sample_weight, random_state, n_local_trials\n    )\n\n    return centers, indices\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "dataset/_kmeans.py/_kmeans_plusplus"
      ],
      "special_reference_type": [
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "_kmeans_plusplus",
      "md_content": [
        "**_kmeans_plusplus**: The function of _kmeans_plusplus is to initialize cluster centers for the k-means clustering algorithm using the k-means++ method.\n\n**parameters**: The parameters of this Function.\n· X: {ndarray, sparse matrix} of shape (n_samples, n_features) - The data to pick seeds for.\n· n_clusters: int - The number of seeds to choose.\n· sample_weight: ndarray of shape (n_samples,) - The weights for each observation in `X`.\n· x_squared_norms: ndarray of shape (n_samples,) - Squared Euclidean norm of each data point.\n· random_state: RandomState instance - The generator used to initialize the centers.\n· n_local_trials: int, default=None - The number of seeding trials for each center (except the first).\n\n**Code Description**: The _kmeans_plusplus function is a computational component designed to facilitate the initialization of cluster centers in the k-means clustering algorithm by employing the k-means++ strategy. This method is particularly advantageous as it selects initial cluster centers in a way that is expected to improve the convergence speed of the algorithm.\n\nThe function begins by determining the number of samples and features from the input data `X`. It then initializes an empty array to hold the cluster centers. If the number of local trials is not specified, it defaults to a logarithmic function of the number of clusters, which helps in selecting better initial centers.\n\nThe first cluster center is chosen randomly from the data points, weighted by the provided sample weights. The function then calculates the squared distances from this center to all other points in the dataset, which is crucial for the selection of subsequent centers. The remaining centers are chosen based on their distance from the existing centers, with a probability proportional to the squared distance, ensuring that points farther away from existing centers have a higher chance of being selected.\n\nThe function also handles sparse matrices appropriately, converting them to dense arrays when necessary. Finally, it returns the initialized centers and their corresponding indices in the original dataset.\n\nThis function is called by the kmeans_plusplus function, which serves as a public interface for initializing cluster centers. The kmeans_plusplus function performs preliminary checks on the input data and parameters before delegating the actual initialization task to _kmeans_plusplus. Additionally, it is utilized within the _init_centroids method of the _BaseKMeans class, which is responsible for computing the initial centroids based on the specified initialization method.\n\n**Note**: It is important to ensure that the number of samples in `X` is greater than or equal to the number of clusters specified. The function assumes that prior validation of the data has been conducted.\n\n**Output Example**: A possible return value of the function could be:\ncenters: array([[10, 2],\n                [1, 0]])\nindices: array([3, 2])"
      ],
      "code_start_line": 178,
      "code_end_line": 276,
      "params": [
        "X",
        "n_clusters",
        "x_squared_norms",
        "sample_weight",
        "random_state",
        "n_local_trials"
      ],
      "have_return": true,
      "code_content": "def _kmeans_plusplus(\n    X, n_clusters, x_squared_norms, sample_weight, random_state, n_local_trials=None\n):\n    \"\"\"Computational component for initialization of n_clusters by\n    k-means++. Prior validation of data is assumed.\n\n    Parameters\n    ----------\n    X : {ndarray, sparse matrix} of shape (n_samples, n_features)\n        The data to pick seeds for.\n\n    n_clusters : int\n        The number of seeds to choose.\n\n    sample_weight : ndarray of shape (n_samples,)\n        The weights for each observation in `X`.\n\n    x_squared_norms : ndarray of shape (n_samples,)\n        Squared Euclidean norm of each data point.\n\n    random_state : RandomState instance\n        The generator used to initialize the centers.\n        See :term:`Glossary <random_state>`.\n\n    n_local_trials : int, default=None\n        The number of seeding trials for each center (except the first),\n        of which the one reducing inertia the most is greedily chosen.\n        Set to None to make the number of trials depend logarithmically\n        on the number of seeds (2+log(k)); this is the default.\n\n    Returns\n    -------\n    centers : ndarray of shape (n_clusters, n_features)\n        The initial centers for k-means.\n\n    indices : ndarray of shape (n_clusters,)\n        The index location of the chosen centers in the data array X. For a\n        given index and center, X[index] = center.\n    \"\"\"\n    n_samples, n_features = X.shape\n\n    centers = np.empty((n_clusters, n_features), dtype=X.dtype)\n\n    # Set the number of local seeding trials if none is given\n    if n_local_trials is None:\n        # This is what Arthur/Vassilvitskii tried, but did not report\n        # specific results for other than mentioning in the conclusion\n        # that it helped.\n        n_local_trials = 2 + int(np.log(n_clusters))\n\n    # Pick first center randomly and track index of point\n    center_id = random_state.choice(n_samples, p=sample_weight / sample_weight.sum())\n    indices = np.full(n_clusters, -1, dtype=int)\n    if sp.issparse(X):\n        centers[0] = X[[center_id]].toarray()\n    else:\n        centers[0] = X[center_id]\n    indices[0] = center_id\n\n    # Initialize list of closest distances and calculate current potential\n    closest_dist_sq = _euclidean_distances(\n        centers[0, np.newaxis], X, Y_norm_squared=x_squared_norms, squared=True\n    )\n    current_pot = closest_dist_sq @ sample_weight\n\n    # Pick the remaining n_clusters-1 points\n    for c in range(1, n_clusters):\n        # Choose center candidates by sampling with probability proportional\n        # to the squared distance to the closest existing center\n        rand_vals = random_state.uniform(size=n_local_trials) * current_pot\n        candidate_ids = np.searchsorted(\n            stable_cumsum(sample_weight * closest_dist_sq), rand_vals\n        )\n        # XXX: numerical imprecision can result in a candidate_id out of range\n        np.clip(candidate_ids, None, closest_dist_sq.size - 1, out=candidate_ids)\n\n        # Compute distances to center candidates\n        distance_to_candidates = _euclidean_distances(\n            X[candidate_ids], X, Y_norm_squared=x_squared_norms, squared=True\n        )\n\n        # update closest distances squared and potential for each candidate\n        np.minimum(closest_dist_sq, distance_to_candidates, out=distance_to_candidates)\n        candidates_pot = distance_to_candidates @ sample_weight.reshape(-1, 1)\n\n        # Decide which candidate is the best\n        best_candidate = np.argmin(candidates_pot)\n        current_pot = candidates_pot[best_candidate]\n        closest_dist_sq = distance_to_candidates[best_candidate]\n        best_candidate = candidate_ids[best_candidate]\n\n        # Permanently add best center candidate found in local tries\n        if sp.issparse(X):\n            centers[c] = X[[best_candidate]].toarray()\n        else:\n            centers[c] = X[best_candidate]\n        indices[c] = best_candidate\n\n    return centers, indices\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "dataset/_kmeans.py/kmeans_plusplus",
        "dataset/_kmeans.py/_BaseKMeans/_init_centroids"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "_tolerance",
      "md_content": [
        "**_tolerance**: The function of _tolerance is to calculate a tolerance value based on the dataset provided.\n\n**parameters**: The parameters of this Function.\n· parameter1: X - The input dataset, which can be either a dense or sparse matrix.\n· parameter2: tol - A scalar value representing the tolerance factor.\n\n**Code Description**: The _tolerance function computes a tolerance value that is dependent on the characteristics of the dataset X. If the tol parameter is set to zero, the function immediately returns zero, indicating no tolerance. For datasets represented as sparse matrices, the function calculates the variances of the features using the mean_variance_axis function, which is assumed to return both the mean and variance along the specified axis. For dense matrices, it directly computes the variances using NumPy's var function. Finally, the function returns the mean of these variances multiplied by the tol parameter, effectively scaling the tolerance based on the variability of the dataset.\n\nThis function is called within the _check_params_vs_input method of the _BaseKMeans class. In this context, it is used to set the instance variable self._tol, which represents the computed tolerance for the KMeans clustering algorithm. The _check_params_vs_input method ensures that the number of samples in the dataset is sufficient relative to the number of clusters specified. By calculating the tolerance using the _tolerance function, it helps in determining the convergence criteria for the clustering process, thereby influencing the behavior of the KMeans algorithm during its execution.\n\n**Note**: It is important to ensure that the tol parameter is set appropriately, as a value of zero will lead to a tolerance of zero, which may affect the clustering results. Additionally, the function is designed to handle both sparse and dense datasets, making it versatile for different types of input data.\n\n**Output Example**: For a dataset X with variances [0.5, 1.0, 1.5] and a tol value of 0.1, the function would return (0.5 + 1.0 + 1.5) / 3 * 0.1 = 0.1."
      ],
      "code_start_line": 283,
      "code_end_line": 291,
      "params": [
        "X",
        "tol"
      ],
      "have_return": true,
      "code_content": "def _tolerance(X, tol):\n    \"\"\"Return a tolerance which is dependent on the dataset.\"\"\"\n    if tol == 0:\n        return 0\n    if sp.issparse(X):\n        variances = mean_variance_axis(X, axis=0)[1]\n    else:\n        variances = np.var(X, axis=0)\n    return np.mean(variances) * tol\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "dataset/_kmeans.py/_BaseKMeans/_check_params_vs_input"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "k_means",
      "md_content": [
        "**k_means**: The function of k_means is to perform K-means clustering algorithm on a given dataset.\n\n**parameters**: The parameters of this Function.\n· X: {array-like, sparse matrix} of shape (n_samples, n_features) - The observations to cluster. The data will be converted to C ordering, which may cause a memory copy if the data is not C-contiguous.\n· n_clusters: int - The number of clusters to form as well as the number of centroids to generate.\n· sample_weight: array-like of shape (n_samples,), default=None - The weights for each observation in X. If None, all observations are assigned equal weight. sample_weight is not used during initialization if init is a callable or a user-provided array.\n· init: {'k-means++', 'random'}, callable or array-like of shape (n_clusters, n_features), default='k-means++' - Method for initialization of cluster centers.\n· n_init: 'auto' or int, default=\"auto\" - Number of times the k-means algorithm will be run with different centroid seeds.\n· max_iter: int, default=300 - Maximum number of iterations of the k-means algorithm to run.\n· verbose: bool, default=False - Verbosity mode.\n· tol: float, default=1e-4 - Relative tolerance with regards to Frobenius norm of the difference in the cluster centers of two consecutive iterations to declare convergence.\n· random_state: int, RandomState instance or None, default=None - Determines random number generation for centroid initialization.\n· copy_x: bool, default=True - When pre-computing distances, indicates whether to modify the original data.\n· algorithm: {\"lloyd\", \"elkan\"}, default=\"lloyd\" - K-means algorithm to use, either \"lloyd\" or \"elkan\".\n· return_n_iter: bool, default=False - Whether or not to return the number of iterations.\n\n**Code Description**: The k_means function serves as a high-level interface for performing K-means clustering. It accepts a dataset X and various parameters that control the clustering process. The function initializes an instance of the KMeans class with the provided parameters and calls its fit method to execute the clustering algorithm. The KMeans class is responsible for managing the clustering process, including the initialization of cluster centers, the iterative assignment of data points to clusters, and the updating of cluster centers until convergence is achieved or the maximum number of iterations is reached.\n\nThe k_means function returns the final cluster centroids, the labels indicating the closest centroid for each observation, and the inertia, which measures how tightly the clusters are packed. If the return_n_iter parameter is set to True, it also returns the number of iterations taken to reach the final clustering solution.\n\nThe relationship between k_means and its callees is straightforward: k_means acts as a user-friendly wrapper around the KMeans class, allowing users to perform clustering without needing to directly interact with the class's internal methods.\n\n**Note**: It is essential to ensure that the input data X is properly formatted and that the parameters are set according to the desired clustering behavior. The choice of initialization method and the number of initializations can significantly impact the performance and outcome of the K-means clustering process.\n\n**Output Example**: A possible return value from the k_means function could be:\n```python\n(array([[10.,  2.],\n         [ 1.,  2.]]), \n array([1, 1, 1, 0, 0, 0], dtype=int32), \n 16.0)\n```\nThis output indicates the coordinates of the cluster centers, the cluster assignment for each sample in the input data, and the final inertia value."
      ],
      "code_start_line": 302,
      "code_end_line": 457,
      "params": [
        "X",
        "n_clusters"
      ],
      "have_return": true,
      "code_content": "def k_means(\n    X,\n    n_clusters,\n    *,\n    sample_weight=None,\n    init=\"k-means++\",\n    n_init=\"auto\",\n    max_iter=300,\n    verbose=False,\n    tol=1e-4,\n    random_state=None,\n    copy_x=True,\n    algorithm=\"lloyd\",\n    return_n_iter=False,\n):\n    \"\"\"Perform K-means clustering algorithm.\n\n    Read more in the :ref:`User Guide <k_means>`.\n\n    Parameters\n    ----------\n    X : {array-like, sparse matrix} of shape (n_samples, n_features)\n        The observations to cluster. It must be noted that the data\n        will be converted to C ordering, which will cause a memory copy\n        if the given data is not C-contiguous.\n\n    n_clusters : int\n        The number of clusters to form as well as the number of\n        centroids to generate.\n\n    sample_weight : array-like of shape (n_samples,), default=None\n        The weights for each observation in `X`. If `None`, all observations\n        are assigned equal weight. `sample_weight` is not used during\n        initialization if `init` is a callable or a user provided array.\n\n    init : {'k-means++', 'random'}, callable or array-like of shape \\\n            (n_clusters, n_features), default='k-means++'\n        Method for initialization:\n\n        - `'k-means++'` : selects initial cluster centers for k-mean\n          clustering in a smart way to speed up convergence. See section\n          Notes in k_init for more details.\n        - `'random'`: choose `n_clusters` observations (rows) at random from data\n          for the initial centroids.\n        - If an array is passed, it should be of shape `(n_clusters, n_features)`\n          and gives the initial centers.\n        - If a callable is passed, it should take arguments `X`, `n_clusters` and a\n          random state and return an initialization.\n\n    n_init : 'auto' or int, default=\"auto\"\n        Number of time the k-means algorithm will be run with different\n        centroid seeds. The final results will be the best output of\n        n_init consecutive runs in terms of inertia.\n\n        When `n_init='auto'`, the number of runs depends on the value of init:\n        10 if using `init='random'` or `init` is a callable;\n        1 if using `init='k-means++'` or `init` is an array-like.\n\n        .. versionadded:: 1.2\n           Added 'auto' option for `n_init`.\n\n        .. versionchanged:: 1.4\n           Default value for `n_init` changed to `'auto'`.\n\n    max_iter : int, default=300\n        Maximum number of iterations of the k-means algorithm to run.\n\n    verbose : bool, default=False\n        Verbosity mode.\n\n    tol : float, default=1e-4\n        Relative tolerance with regards to Frobenius norm of the difference\n        in the cluster centers of two consecutive iterations to declare\n        convergence.\n\n    random_state : int, RandomState instance or None, default=None\n        Determines random number generation for centroid initialization. Use\n        an int to make the randomness deterministic.\n        See :term:`Glossary <random_state>`.\n\n    copy_x : bool, default=True\n        When pre-computing distances it is more numerically accurate to center\n        the data first. If `copy_x` is True (default), then the original data is\n        not modified. If False, the original data is modified, and put back\n        before the function returns, but small numerical differences may be\n        introduced by subtracting and then adding the data mean. Note that if\n        the original data is not C-contiguous, a copy will be made even if\n        `copy_x` is False. If the original data is sparse, but not in CSR format,\n        a copy will be made even if `copy_x` is False.\n\n    algorithm : {\"lloyd\", \"elkan\"}, default=\"lloyd\"\n        K-means algorithm to use. The classical EM-style algorithm is `\"lloyd\"`.\n        The `\"elkan\"` variation can be more efficient on some datasets with\n        well-defined clusters, by using the triangle inequality. However it's\n        more memory intensive due to the allocation of an extra array of shape\n        `(n_samples, n_clusters)`.\n\n        .. versionchanged:: 0.18\n            Added Elkan algorithm\n\n        .. versionchanged:: 1.1\n            Renamed \"full\" to \"lloyd\", and deprecated \"auto\" and \"full\".\n            Changed \"auto\" to use \"lloyd\" instead of \"elkan\".\n\n    return_n_iter : bool, default=False\n        Whether or not to return the number of iterations.\n\n    Returns\n    -------\n    centroid : ndarray of shape (n_clusters, n_features)\n        Centroids found at the last iteration of k-means.\n\n    label : ndarray of shape (n_samples,)\n        The `label[i]` is the code or index of the centroid the\n        i'th observation is closest to.\n\n    inertia : float\n        The final value of the inertia criterion (sum of squared distances to\n        the closest centroid for all observations in the training set).\n\n    best_n_iter : int\n        Number of iterations corresponding to the best results.\n        Returned only if `return_n_iter` is set to True.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.cluster import k_means\n    >>> X = np.array([[1, 2], [1, 4], [1, 0],\n    ...               [10, 2], [10, 4], [10, 0]])\n    >>> centroid, label, inertia = k_means(\n    ...     X, n_clusters=2, n_init=\"auto\", random_state=0\n    ... )\n    >>> centroid\n    array([[10.,  2.],\n           [ 1.,  2.]])\n    >>> label\n    array([1, 1, 1, 0, 0, 0], dtype=int32)\n    >>> inertia\n    16.0\n    \"\"\"\n    est = KMeans(\n        n_clusters=n_clusters,\n        init=init,\n        n_init=n_init,\n        max_iter=max_iter,\n        verbose=verbose,\n        tol=tol,\n        random_state=random_state,\n        copy_x=copy_x,\n        algorithm=algorithm,\n    ).fit(X, sample_weight=sample_weight)\n    if return_n_iter:\n        return est.cluster_centers_, est.labels_, est.inertia_, est.n_iter_\n    else:\n        return est.cluster_centers_, est.labels_, est.inertia_\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "dataset/_kmeans.py/KMeans",
        "dataset/_kmeans.py/KMeans/fit"
      ],
      "special_reference_type": [
        false,
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "_kmeans_single_elkan",
      "md_content": [
        "**_kmeans_single_elkan**: The function of _kmeans_single_elkan is to perform a single run of the k-means clustering algorithm using the Elkan variant, which is optimized for speed and efficiency.\n\n**parameters**: The parameters of this Function.\n· X: {ndarray, sparse matrix} of shape (n_samples, n_features) - The observations to cluster. If a sparse matrix is provided, it must be in CSR format.\n· sample_weight: array-like of shape (n_samples,) - The weights for each observation in X.\n· centers_init: ndarray of shape (n_clusters, n_features) - The initial centers for the clusters.\n· max_iter: int, default=300 - Maximum number of iterations of the k-means algorithm to run.\n· verbose: bool, default=False - Verbosity mode to control the output of the function.\n· tol: float, default=1e-4 - Relative tolerance with respect to the Frobenius norm of the difference in the cluster centers of two consecutive iterations to declare convergence.\n· n_threads: int, default=1 - The number of OpenMP threads to use for the computation, allowing for parallelism on the main Cython loop.\n\n**Code Description**: The _kmeans_single_elkan function executes a single iteration of the k-means clustering algorithm using the Elkan method, which is designed to reduce the number of distance calculations needed during the clustering process. The function begins by initializing several buffers to store intermediate results, such as the new cluster centers, weights in clusters, and labels for each sample. It checks whether the input data X is sparse or dense and sets up the appropriate functions for initialization and iteration accordingly.\n\nThe function then enters a loop that runs for a maximum of max_iter iterations. In each iteration, it updates the cluster centers and assigns labels to the samples based on their proximity to the centers. The function also checks for convergence by comparing the current labels with the previous labels and by evaluating the shift in cluster centers against the specified tolerance. If convergence is achieved, the loop breaks early.\n\nAfter the iterations, the function performs a final assignment of labels to ensure that they match the updated cluster centers. It calculates the final inertia, which is the sum of squared distances from each sample to its closest cluster center, and returns the labels, inertia, final cluster centers, and the number of iterations run.\n\nThis function is called by the fit method of the KMeans class, which is responsible for computing k-means clustering. The fit method validates the input data, initializes the cluster centers, and then calls _kmeans_single_elkan to perform the clustering. The results from _kmeans_single_elkan are used to determine the best clustering configuration based on inertia and distinct clusters.\n\n**Note**: It is important to ensure that the input data X is in the correct format (either dense or sparse) and that the sample weights are appropriately defined. The function is optimized for performance, especially when using multiple threads, which can significantly speed up the clustering process.\n\n**Output Example**: \nAn example of the return value from the function could look like this:\n```python\nlabels = np.array([0, 1, 0, 1, 0])\ninertia = 12.34\ncentroids = np.array([[1.0, 2.0], [3.0, 4.0]])\nn_iter = 5\n``` \nThis output indicates the labels assigned to each sample, the final inertia value, the coordinates of the cluster centroids, and the number of iterations completed during the clustering process."
      ],
      "code_start_line": 460,
      "code_end_line": 622,
      "params": [
        "X",
        "sample_weight",
        "centers_init",
        "max_iter",
        "verbose",
        "tol",
        "n_threads"
      ],
      "have_return": true,
      "code_content": "def _kmeans_single_elkan(\n    X,\n    sample_weight,\n    centers_init,\n    max_iter=300,\n    verbose=False,\n    tol=1e-4,\n    n_threads=1,\n):\n    \"\"\"A single run of k-means elkan, assumes preparation completed prior.\n\n    Parameters\n    ----------\n    X : {ndarray, sparse matrix} of shape (n_samples, n_features)\n        The observations to cluster. If sparse matrix, must be in CSR format.\n\n    sample_weight : array-like of shape (n_samples,)\n        The weights for each observation in X.\n\n    centers_init : ndarray of shape (n_clusters, n_features)\n        The initial centers.\n\n    max_iter : int, default=300\n        Maximum number of iterations of the k-means algorithm to run.\n\n    verbose : bool, default=False\n        Verbosity mode.\n\n    tol : float, default=1e-4\n        Relative tolerance with regards to Frobenius norm of the difference\n        in the cluster centers of two consecutive iterations to declare\n        convergence.\n        It's not advised to set `tol=0` since convergence might never be\n        declared due to rounding errors. Use a very small number instead.\n\n    n_threads : int, default=1\n        The number of OpenMP threads to use for the computation. Parallelism is\n        sample-wise on the main cython loop which assigns each sample to its\n        closest center.\n\n    Returns\n    -------\n    centroid : ndarray of shape (n_clusters, n_features)\n        Centroids found at the last iteration of k-means.\n\n    label : ndarray of shape (n_samples,)\n        label[i] is the code or index of the centroid the\n        i'th observation is closest to.\n\n    inertia : float\n        The final value of the inertia criterion (sum of squared distances to\n        the closest centroid for all observations in the training set).\n\n    n_iter : int\n        Number of iterations run.\n    \"\"\"\n    n_samples = X.shape[0]\n    n_clusters = centers_init.shape[0]\n\n    # Buffers to avoid new allocations at each iteration.\n    centers = centers_init\n    centers_new = np.zeros_like(centers)\n    weight_in_clusters = np.zeros(n_clusters, dtype=X.dtype)\n    labels = np.full(n_samples, -1, dtype=np.int32)\n    labels_old = labels.copy()\n    center_half_distances = euclidean_distances(centers) / 2\n    distance_next_center = np.partition(\n        np.asarray(center_half_distances), kth=1, axis=0\n    )[1]\n    upper_bounds = np.zeros(n_samples, dtype=X.dtype)\n    lower_bounds = np.zeros((n_samples, n_clusters), dtype=X.dtype)\n    center_shift = np.zeros(n_clusters, dtype=X.dtype)\n\n    if sp.issparse(X):\n        init_bounds = init_bounds_sparse\n        elkan_iter = elkan_iter_chunked_sparse\n        _inertia = _inertia_sparse\n    else:\n        init_bounds = init_bounds_dense\n        elkan_iter = elkan_iter_chunked_dense\n        _inertia = _inertia_dense\n\n    init_bounds(\n        X,\n        centers,\n        center_half_distances,\n        labels,\n        upper_bounds,\n        lower_bounds,\n        n_threads=n_threads,\n    )\n\n    strict_convergence = False\n\n    for i in range(max_iter):\n        elkan_iter(\n            X,\n            sample_weight,\n            centers,\n            centers_new,\n            weight_in_clusters,\n            center_half_distances,\n            distance_next_center,\n            upper_bounds,\n            lower_bounds,\n            labels,\n            center_shift,\n            n_threads,\n        )\n\n        # compute new pairwise distances between centers and closest other\n        # center of each center for next iterations\n        center_half_distances = euclidean_distances(centers_new) / 2\n        distance_next_center = np.partition(\n            np.asarray(center_half_distances), kth=1, axis=0\n        )[1]\n\n        if verbose:\n            inertia = _inertia(X, sample_weight, centers, labels, n_threads)\n            print(f\"Iteration {i}, inertia {inertia}\")\n\n        centers, centers_new = centers_new, centers\n\n        if np.array_equal(labels, labels_old):\n            # First check the labels for strict convergence.\n            if verbose:\n                print(f\"Converged at iteration {i}: strict convergence.\")\n            strict_convergence = True\n            break\n        else:\n            # No strict convergence, check for tol based convergence.\n            center_shift_tot = (center_shift**2).sum()\n            if center_shift_tot <= tol:\n                if verbose:\n                    print(\n                        f\"Converged at iteration {i}: center shift \"\n                        f\"{center_shift_tot} within tolerance {tol}.\"\n                    )\n                break\n\n        labels_old[:] = labels\n\n    if not strict_convergence:\n        # rerun E-step so that predicted labels match cluster centers\n        elkan_iter(\n            X,\n            sample_weight,\n            centers,\n            centers,\n            weight_in_clusters,\n            center_half_distances,\n            distance_next_center,\n            upper_bounds,\n            lower_bounds,\n            labels,\n            center_shift,\n            n_threads,\n            update_centers=False,\n        )\n\n    inertia = _inertia(X, sample_weight, centers, labels, n_threads)\n\n    return labels, inertia, centers, i + 1\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "dataset/_kmeans.py/KMeans/fit"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "_kmeans_single_lloyd",
      "md_content": [
        "**_kmeans_single_lloyd**: The function of _kmeans_single_lloyd is to perform a single run of the k-means clustering algorithm using the Lloyd's method.\n\n**parameters**: The parameters of this Function.\n· X: {ndarray, sparse matrix} of shape (n_samples, n_features) - The observations to cluster. If a sparse matrix is provided, it must be in CSR format.  \n· sample_weight: ndarray of shape (n_samples,) - The weights for each observation in X.  \n· centers_init: ndarray of shape (n_clusters, n_features) - The initial centers for the clusters.  \n· max_iter: int, default=300 - The maximum number of iterations of the k-means algorithm to run.  \n· verbose: bool, default=False - Controls the verbosity of the output during the execution of the algorithm.  \n· tol: float, default=1e-4 - The relative tolerance with respect to the Frobenius norm of the difference in the cluster centers of two consecutive iterations, used to declare convergence.  \n· n_threads: int, default=1 - The number of OpenMP threads to use for computation, allowing for parallel processing on the main Cython loop.\n\n**Code Description**: The _kmeans_single_lloyd function implements the Lloyd's algorithm for k-means clustering. It begins by initializing several buffers to store the current and new cluster centers, labels for each observation, and other necessary variables. The function checks if the input data X is sparse or dense and assigns the appropriate iteration function and inertia calculation method accordingly.\n\nThe algorithm iterates up to max_iter times, updating the cluster centers and labels based on the distances of the observations to the centers. If the labels do not change between iterations, strict convergence is declared. If the change in cluster centers is below the specified tolerance (tol), convergence is also declared. The function finally calculates the inertia, which is the sum of squared distances from each observation to its closest cluster center, and returns the final labels, inertia, cluster centers, and the number of iterations run.\n\nThis function is called by the fit method of the KMeans class in the same module. During the fitting process, the KMeans class prepares the data and initializes the cluster centers before invoking _kmeans_single_lloyd to perform the clustering. The results from this function are then used to determine the best clustering configuration based on inertia and distinct clusters found.\n\n**Note**: It is important to ensure that the input data X is properly formatted and that the sample weights are correctly specified. The function assumes that the necessary preparations have been completed prior to its invocation.\n\n**Output Example**: A possible return value of the function could be:\n- labels: array([0, 1, 0, 1, 0])  # Indicating the closest centroid for each observation\n- inertia: 12.34  # The final inertia value\n- centers: array([[1.5, 2.5], [3.5, 4.5]])  # Final cluster centers\n- n_iter: 10  # Number of iterations run"
      ],
      "code_start_line": 625,
      "code_end_line": 756,
      "params": [
        "X",
        "sample_weight",
        "centers_init",
        "max_iter",
        "verbose",
        "tol",
        "n_threads"
      ],
      "have_return": true,
      "code_content": "def _kmeans_single_lloyd(\n    X,\n    sample_weight,\n    centers_init,\n    max_iter=300,\n    verbose=False,\n    tol=1e-4,\n    n_threads=1,\n):\n    \"\"\"A single run of k-means lloyd, assumes preparation completed prior.\n\n    Parameters\n    ----------\n    X : {ndarray, sparse matrix} of shape (n_samples, n_features)\n        The observations to cluster. If sparse matrix, must be in CSR format.\n\n    sample_weight : ndarray of shape (n_samples,)\n        The weights for each observation in X.\n\n    centers_init : ndarray of shape (n_clusters, n_features)\n        The initial centers.\n\n    max_iter : int, default=300\n        Maximum number of iterations of the k-means algorithm to run.\n\n    verbose : bool, default=False\n        Verbosity mode\n\n    tol : float, default=1e-4\n        Relative tolerance with regards to Frobenius norm of the difference\n        in the cluster centers of two consecutive iterations to declare\n        convergence.\n        It's not advised to set `tol=0` since convergence might never be\n        declared due to rounding errors. Use a very small number instead.\n\n    n_threads : int, default=1\n        The number of OpenMP threads to use for the computation. Parallelism is\n        sample-wise on the main cython loop which assigns each sample to its\n        closest center.\n\n    Returns\n    -------\n    centroid : ndarray of shape (n_clusters, n_features)\n        Centroids found at the last iteration of k-means.\n\n    label : ndarray of shape (n_samples,)\n        label[i] is the code or index of the centroid the\n        i'th observation is closest to.\n\n    inertia : float\n        The final value of the inertia criterion (sum of squared distances to\n        the closest centroid for all observations in the training set).\n\n    n_iter : int\n        Number of iterations run.\n    \"\"\"\n    n_clusters = centers_init.shape[0]\n\n    # Buffers to avoid new allocations at each iteration.\n    centers = centers_init\n    centers_new = np.zeros_like(centers)\n    labels = np.full(X.shape[0], -1, dtype=np.int32)\n    labels_old = labels.copy()\n    weight_in_clusters = np.zeros(n_clusters, dtype=X.dtype)\n    center_shift = np.zeros(n_clusters, dtype=X.dtype)\n\n    if sp.issparse(X):\n        lloyd_iter = lloyd_iter_chunked_sparse\n        _inertia = _inertia_sparse\n    else:\n        lloyd_iter = lloyd_iter_chunked_dense\n        _inertia = _inertia_dense\n\n    strict_convergence = False\n\n    # Threadpoolctl context to limit the number of threads in second level of\n    # nested parallelism (i.e. BLAS) to avoid oversubscription.\n    with threadpool_limits(limits=1, user_api=\"blas\"):\n        for i in range(max_iter):\n            lloyd_iter(\n                X,\n                sample_weight,\n                centers,\n                centers_new,\n                weight_in_clusters,\n                labels,\n                center_shift,\n                n_threads,\n            )\n\n            if verbose:\n                inertia = _inertia(X, sample_weight, centers, labels, n_threads)\n                print(f\"Iteration {i}, inertia {inertia}.\")\n\n            centers, centers_new = centers_new, centers\n\n            if np.array_equal(labels, labels_old):\n                # First check the labels for strict convergence.\n                if verbose:\n                    print(f\"Converged at iteration {i}: strict convergence.\")\n                strict_convergence = True\n                break\n            else:\n                # No strict convergence, check for tol based convergence.\n                center_shift_tot = (center_shift**2).sum()\n                if center_shift_tot <= tol:\n                    if verbose:\n                        print(\n                            f\"Converged at iteration {i}: center shift \"\n                            f\"{center_shift_tot} within tolerance {tol}.\"\n                        )\n                    break\n\n            labels_old[:] = labels\n\n        if not strict_convergence:\n            # rerun E-step so that predicted labels match cluster centers\n            lloyd_iter(\n                X,\n                sample_weight,\n                centers,\n                centers,\n                weight_in_clusters,\n                labels,\n                center_shift,\n                n_threads,\n                update_centers=False,\n            )\n\n    inertia = _inertia(X, sample_weight, centers, labels, n_threads)\n\n    return labels, inertia, centers, i + 1\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "dataset/_kmeans.py/KMeans/fit"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "_labels_inertia",
      "md_content": [
        "**_labels_inertia**: The function of _labels_inertia is to compute the labels and the inertia of the given samples and centers in the K-means EM algorithm.\n\n**parameters**: The parameters of this Function.\n· X: {ndarray, sparse matrix} of shape (n_samples, n_features) - The input samples to assign to the labels. If sparse matrix, must be in CSR format.  \n· sample_weight: ndarray of shape (n_samples,) - The weights for each observation in X.  \n· centers: ndarray of shape (n_clusters, n_features) - The cluster centers.  \n· n_threads: int, default=1 - The number of OpenMP threads to use for the computation. Parallelism is sample-wise on the main cython loop which assigns each sample to its closest center.  \n· return_inertia: bool, default=True - Whether to compute and return the inertia.  \n\n**Code Description**: The _labels_inertia function is a critical component of the K-means clustering algorithm, specifically designed to perform the expectation step (E step) of the K-means EM algorithm. It takes a dataset X and a set of cluster centers and computes the closest cluster for each sample in X, assigning labels accordingly. The function also calculates the inertia, which is the sum of squared distances from each sample to its nearest cluster center, if requested.\n\nThe function begins by determining the number of samples and clusters based on the input data shapes. It initializes an array to hold the labels for each sample, setting them to -1 initially. Depending on whether the input data X is a sparse matrix or a dense array, it selects the appropriate implementation for label assignment and inertia calculation.\n\nThe label assignment is performed by calling either the _labels function for sparse or dense data. After the labels are assigned, if the return_inertia parameter is set to True, the function computes the inertia using the selected inertia calculation method and returns both the labels and the inertia value. If return_inertia is False, only the labels are returned.\n\nThis function is called by other functions within the K-means implementation, such as _mini_batch_step, which utilizes _labels_inertia to first assign labels to the samples before updating the cluster centers. This relationship highlights the function's role in the overall K-means algorithm, where accurate label assignment is essential for effective clustering.\n\n**Note**: It is important to ensure that the input data X is in the correct format (ndarray or CSR sparse matrix) and that the sample weights are appropriately defined. The function is optimized for performance with the option to utilize multiple threads for computation.\n\n**Output Example**: A possible return value of the function when called with a dataset of 5 samples and 3 clusters might look like this:  \nLabels: [0, 1, 0, 2, 1]  \nInertia: 12.34"
      ],
      "code_start_line": 759,
      "code_end_line": 826,
      "params": [
        "X",
        "sample_weight",
        "centers",
        "n_threads",
        "return_inertia"
      ],
      "have_return": true,
      "code_content": "def _labels_inertia(X, sample_weight, centers, n_threads=1, return_inertia=True):\n    \"\"\"E step of the K-means EM algorithm.\n\n    Compute the labels and the inertia of the given samples and centers.\n\n    Parameters\n    ----------\n    X : {ndarray, sparse matrix} of shape (n_samples, n_features)\n        The input samples to assign to the labels. If sparse matrix, must\n        be in CSR format.\n\n    sample_weight : ndarray of shape (n_samples,)\n        The weights for each observation in X.\n\n    x_squared_norms : ndarray of shape (n_samples,)\n        Precomputed squared euclidean norm of each data point, to speed up\n        computations.\n\n    centers : ndarray of shape (n_clusters, n_features)\n        The cluster centers.\n\n    n_threads : int, default=1\n        The number of OpenMP threads to use for the computation. Parallelism is\n        sample-wise on the main cython loop which assigns each sample to its\n        closest center.\n\n    return_inertia : bool, default=True\n        Whether to compute and return the inertia.\n\n    Returns\n    -------\n    labels : ndarray of shape (n_samples,)\n        The resulting assignment.\n\n    inertia : float\n        Sum of squared distances of samples to their closest cluster center.\n        Inertia is only returned if return_inertia is True.\n    \"\"\"\n    n_samples = X.shape[0]\n    n_clusters = centers.shape[0]\n\n    labels = np.full(n_samples, -1, dtype=np.int32)\n    center_shift = np.zeros(n_clusters, dtype=centers.dtype)\n\n    if sp.issparse(X):\n        _labels = lloyd_iter_chunked_sparse\n        _inertia = _inertia_sparse\n    else:\n        _labels = lloyd_iter_chunked_dense\n        _inertia = _inertia_dense\n\n    _labels(\n        X,\n        sample_weight,\n        centers,\n        centers_new=None,\n        weight_in_clusters=None,\n        labels=labels,\n        center_shift=center_shift,\n        n_threads=n_threads,\n        update_centers=False,\n    )\n\n    if return_inertia:\n        inertia = _inertia(X, sample_weight, centers, labels, n_threads)\n        return labels, inertia\n\n    return labels\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "dataset/_kmeans.py/_labels_inertia_threadpool_limit",
        "dataset/_kmeans.py/_mini_batch_step"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "_labels_inertia_threadpool_limit",
      "md_content": [
        "**_labels_inertia_threadpool_limit**: The function of _labels_inertia_threadpool_limit is to compute the labels and inertia of the given samples and centers in a controlled thread pool context.\n\n**parameters**: The parameters of this Function.\n· X: {ndarray, sparse matrix} of shape (n_samples, n_features) - The input samples to assign to the labels. If sparse matrix, must be in CSR format.  \n· sample_weight: ndarray of shape (n_samples,) - The weights for each observation in X.  \n· centers: ndarray of shape (n_clusters, n_features) - The cluster centers.  \n· n_threads: int, default=1 - The number of OpenMP threads to use for the computation. Parallelism is sample-wise on the main cython loop which assigns each sample to its closest center.  \n· return_inertia: bool, default=True - Whether to compute and return the inertia.  \n\n**Code Description**: The _labels_inertia_threadpool_limit function serves as a wrapper around the _labels_inertia function, specifically designed to operate within a thread pool limit context. This function ensures that the computation of labels and inertia adheres to a specified threading limit, which is particularly useful in environments where resource management is critical.\n\nUpon invocation, the function sets the thread pool limit to 1 for the BLAS user API, ensuring that the underlying computations do not exceed this limit. It then calls the _labels_inertia function, passing along the parameters it received. The _labels_inertia function is responsible for computing the labels for each sample in the dataset X by determining the closest cluster center and, if requested, calculating the inertia, which quantifies the compactness of the clusters.\n\nThe _labels_inertia_threadpool_limit function is called by several methods within the K-means implementation, including the predict method of the _BaseKMeans class, the score method, and the fit and partial_fit methods of the MiniBatchKMeans class. In these contexts, it is used to obtain the labels for new data points or to evaluate the clustering performance on validation sets. The function's design allows it to maintain efficient resource usage while performing essential computations for the K-means algorithm.\n\n**Note**: It is important to ensure that the input data X is in the correct format (ndarray or CSR sparse matrix) and that the sample weights are appropriately defined. The function is optimized for performance with the option to utilize multiple threads for computation, but it enforces a limit to prevent excessive resource consumption.\n\n**Output Example**: A possible return value of the function when called with a dataset of 5 samples and 3 clusters might look like this:  \nLabels: [0, 1, 0, 2, 1]  \nInertia: 12.34"
      ],
      "code_start_line": 829,
      "code_end_line": 836,
      "params": [
        "X",
        "sample_weight",
        "centers",
        "n_threads",
        "return_inertia"
      ],
      "have_return": true,
      "code_content": "def _labels_inertia_threadpool_limit(\n    X, sample_weight, centers, n_threads=1, return_inertia=True\n):\n    \"\"\"Same as _labels_inertia but in a threadpool_limits context.\"\"\"\n    with threadpool_limits(limits=1, user_api=\"blas\"):\n        result = _labels_inertia(X, sample_weight, centers, n_threads, return_inertia)\n\n    return result\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "dataset/_kmeans.py/_BaseKMeans/predict",
        "dataset/_kmeans.py/_BaseKMeans/score",
        "dataset/_kmeans.py/MiniBatchKMeans/fit",
        "dataset/_kmeans.py/MiniBatchKMeans/partial_fit"
      ],
      "reference_who": [
        "dataset/_kmeans.py/_labels_inertia"
      ],
      "special_reference_type": [
        false
      ]
    },
    {
      "type": "ClassDef",
      "name": "_BaseKMeans",
      "md_content": [
        "**_BaseKMeans**: The function of _BaseKMeans is to serve as a base class for KMeans and MiniBatchKMeans clustering algorithms, providing shared functionality and parameter validation.\n\n**attributes**: The attributes of this Class.\n· n_clusters: The number of clusters to form.\n· init: Method for initialization of cluster centers.\n· n_init: Number of times the k-means algorithm will be run with different centroid seeds.\n· max_iter: Maximum number of iterations of the k-means algorithm for a single run.\n· tol: Relative tolerance with regards to Frobenius norm of the difference in the cluster centers of two consecutive iterations to declare convergence.\n· verbose: Verbosity mode.\n· random_state: Determines random number generation for centroid initialization.\n· _parameter_constraints: A dictionary that defines the constraints for the parameters.\n\n**Code Description**: The _BaseKMeans class is an abstract base class that provides the foundational structure for implementing KMeans clustering algorithms. It inherits from several mixins and base classes, including ClassNamePrefixFeaturesOutMixin, TransformerMixin, ClusterMixin, BaseEstimator, and ABC, which equip it with various functionalities such as transformation, clustering, and parameter validation.\n\nThe class defines a set of parameter constraints that ensure the validity of the input parameters when initializing the clustering algorithm. The constructor initializes the parameters and performs validation checks to ensure that the number of samples in the input data is greater than or equal to the number of clusters specified.\n\nThe class includes several methods that are crucial for the clustering process:\n- _check_params_vs_input: Validates the input parameters against the data provided, ensuring that the number of clusters does not exceed the number of samples and that the initialization method is appropriate.\n- _validate_center_shape: Checks if the shape of the initial cluster centers is compatible with the input data and the specified number of clusters.\n- _init_centroids: Computes the initial centroids based on the specified initialization method, which can be 'k-means++', 'random', or a user-defined callable or array.\n- fit_predict, predict, fit_transform, and transform: These methods provide the functionality to fit the model to the data, predict cluster assignments, and transform the data into a cluster-distance space.\n\nThe _BaseKMeans class is designed to be subclassed by specific implementations of KMeans, such as the KMeans and MiniBatchKMeans classes. These subclasses inherit the core functionality of _BaseKMeans while adding specific features and optimizations relevant to their respective algorithms. For instance, KMeans implements the standard batch KMeans algorithm, while MiniBatchKMeans is optimized for large datasets by processing data in smaller batches.\n\n**Note**: When using the _BaseKMeans class, it is essential to ensure that the input data meets the specified constraints, particularly regarding the number of samples and clusters. Additionally, users should be aware of the initialization methods and their implications on the clustering results.\n\n**Output Example**: A possible return value from the fit_predict method could be an array of cluster labels, such as:\n```\narray([0, 0, 1, 1, 0, 1], dtype=int32)\n``` \nThis output indicates the cluster assignment for each sample in the input data, where each unique integer represents a different cluster."
      ],
      "code_start_line": 839,
      "code_end_line": 1210,
      "params": [],
      "have_return": true,
      "code_content": "class _BaseKMeans(\n    ClassNamePrefixFeaturesOutMixin, TransformerMixin, ClusterMixin, BaseEstimator, ABC\n):\n    \"\"\"Base class for KMeans and MiniBatchKMeans\"\"\"\n\n    _parameter_constraints: dict = {\n        \"n_clusters\": [Interval(Integral, 1, None, closed=\"left\")],\n        \"init\": [StrOptions({\"k-means++\", \"random\"}), callable, \"array-like\"],\n        \"n_init\": [\n            StrOptions({\"auto\"}),\n            Interval(Integral, 1, None, closed=\"left\"),\n        ],\n        \"max_iter\": [Interval(Integral, 1, None, closed=\"left\")],\n        \"tol\": [Interval(Real, 0, None, closed=\"left\")],\n        \"verbose\": [\"verbose\"],\n        \"random_state\": [\"random_state\"],\n    }\n\n    def __init__(\n        self,\n        n_clusters,\n        *,\n        init,\n        n_init,\n        max_iter,\n        tol,\n        verbose,\n        random_state,\n    ):\n        self.n_clusters = n_clusters\n        self.init = init\n        self.max_iter = max_iter\n        self.tol = tol\n        self.n_init = n_init\n        self.verbose = verbose\n        self.random_state = random_state\n\n    def _check_params_vs_input(self, X, default_n_init=None):\n        # n_clusters\n        if X.shape[0] < self.n_clusters:\n            raise ValueError(\n                f\"n_samples={X.shape[0]} should be >= n_clusters={self.n_clusters}.\"\n            )\n\n        # tol\n        self._tol = _tolerance(X, self.tol)\n\n        # n-init\n        if self.n_init == \"auto\":\n            if isinstance(self.init, str) and self.init == \"k-means++\":\n                self._n_init = 1\n            elif isinstance(self.init, str) and self.init == \"random\":\n                self._n_init = default_n_init\n            elif callable(self.init):\n                self._n_init = default_n_init\n            else:  # array-like\n                self._n_init = 1\n        else:\n            self._n_init = self.n_init\n\n        if _is_arraylike_not_scalar(self.init) and self._n_init != 1:\n            warnings.warn(\n                (\n                    \"Explicit initial center position passed: performing only\"\n                    f\" one init in {self.__class__.__name__} instead of \"\n                    f\"n_init={self._n_init}.\"\n                ),\n                RuntimeWarning,\n                stacklevel=2,\n            )\n            self._n_init = 1\n\n    @abstractmethod\n    def _warn_mkl_vcomp(self, n_active_threads):\n        \"\"\"Issue an estimator specific warning when vcomp and mkl are both present\n\n        This method is called by `_check_mkl_vcomp`.\n        \"\"\"\n\n    def _check_mkl_vcomp(self, X, n_samples):\n        \"\"\"Check when vcomp and mkl are both present\"\"\"\n        # The BLAS call inside a prange in lloyd_iter_chunked_dense is known to\n        # cause a small memory leak when there are less chunks than the number\n        # of available threads. It only happens when the OpenMP library is\n        # vcomp (microsoft OpenMP) and the BLAS library is MKL. see #18653\n        if sp.issparse(X):\n            return\n\n        n_active_threads = int(np.ceil(n_samples / CHUNK_SIZE))\n        if n_active_threads < self._n_threads:\n            modules = threadpool_info()\n            has_vcomp = \"vcomp\" in [module[\"prefix\"] for module in modules]\n            has_mkl = (\"mkl\", \"intel\") in [\n                (module[\"internal_api\"], module.get(\"threading_layer\", None))\n                for module in modules\n            ]\n            if has_vcomp and has_mkl:\n                self._warn_mkl_vcomp(n_active_threads)\n\n    def _validate_center_shape(self, X, centers):\n        \"\"\"Check if centers is compatible with X and n_clusters.\"\"\"\n        if centers.shape[0] != self.n_clusters:\n            raise ValueError(\n                f\"The shape of the initial centers {centers.shape} does not \"\n                f\"match the number of clusters {self.n_clusters}.\"\n            )\n        if centers.shape[1] != X.shape[1]:\n            raise ValueError(\n                f\"The shape of the initial centers {centers.shape} does not \"\n                f\"match the number of features of the data {X.shape[1]}.\"\n            )\n\n    def _check_test_data(self, X):\n        X = self._validate_data(\n            X,\n            accept_sparse=\"csr\",\n            reset=False,\n            dtype=[np.float64, np.float32],\n            order=\"C\",\n            accept_large_sparse=False,\n        )\n        return X\n\n    def _init_centroids(\n        self,\n        X,\n        x_squared_norms,\n        init,\n        random_state,\n        sample_weight,\n        init_size=None,\n        n_centroids=None,\n    ):\n        \"\"\"Compute the initial centroids.\n\n        Parameters\n        ----------\n        X : {ndarray, sparse matrix} of shape (n_samples, n_features)\n            The input samples.\n\n        x_squared_norms : ndarray of shape (n_samples,)\n            Squared euclidean norm of each data point. Pass it if you have it\n            at hands already to avoid it being recomputed here.\n\n        init : {'k-means++', 'random'}, callable or ndarray of shape \\\n                (n_clusters, n_features)\n            Method for initialization.\n\n        random_state : RandomState instance\n            Determines random number generation for centroid initialization.\n            See :term:`Glossary <random_state>`.\n\n        sample_weight : ndarray of shape (n_samples,)\n            The weights for each observation in X. `sample_weight` is not used\n            during initialization if `init` is a callable or a user provided\n            array.\n\n        init_size : int, default=None\n            Number of samples to randomly sample for speeding up the\n            initialization (sometimes at the expense of accuracy).\n\n        n_centroids : int, default=None\n            Number of centroids to initialize.\n            If left to 'None' the number of centroids will be equal to\n            number of clusters to form (self.n_clusters).\n\n        Returns\n        -------\n        centers : ndarray of shape (n_clusters, n_features)\n            Initial centroids of clusters.\n        \"\"\"\n        n_samples = X.shape[0]\n        n_clusters = self.n_clusters if n_centroids is None else n_centroids\n\n        if init_size is not None and init_size < n_samples:\n            init_indices = random_state.randint(0, n_samples, init_size)\n            X = X[init_indices]\n            x_squared_norms = x_squared_norms[init_indices]\n            n_samples = X.shape[0]\n            sample_weight = sample_weight[init_indices]\n\n        if isinstance(init, str) and init == \"k-means++\":\n            centers, _ = _kmeans_plusplus(\n                X,\n                n_clusters,\n                random_state=random_state,\n                x_squared_norms=x_squared_norms,\n                sample_weight=sample_weight,\n            )\n        elif isinstance(init, str) and init == \"random\":\n            seeds = random_state.choice(\n                n_samples,\n                size=n_clusters,\n                replace=False,\n                p=sample_weight / sample_weight.sum(),\n            )\n            centers = X[seeds]\n        elif _is_arraylike_not_scalar(self.init):\n            centers = init\n        elif callable(init):\n            centers = init(X, n_clusters, random_state=random_state)\n            centers = check_array(centers, dtype=X.dtype, copy=False, order=\"C\")\n            self._validate_center_shape(X, centers)\n\n        if sp.issparse(centers):\n            centers = centers.toarray()\n\n        return centers\n\n    def fit_predict(self, X, y=None, sample_weight=None):\n        \"\"\"Compute cluster centers and predict cluster index for each sample.\n\n        Convenience method; equivalent to calling fit(X) followed by\n        predict(X).\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            New data to transform.\n\n        y : Ignored\n            Not used, present here for API consistency by convention.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            The weights for each observation in X. If None, all observations\n            are assigned equal weight.\n\n        Returns\n        -------\n        labels : ndarray of shape (n_samples,)\n            Index of the cluster each sample belongs to.\n        \"\"\"\n        return self.fit(X, sample_weight=sample_weight).labels_\n\n    def predict(self, X, sample_weight=\"deprecated\"):\n        \"\"\"Predict the closest cluster each sample in X belongs to.\n\n        In the vector quantization literature, `cluster_centers_` is called\n        the code book and each value returned by `predict` is the index of\n        the closest code in the code book.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            New data to predict.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            The weights for each observation in X. If None, all observations\n            are assigned equal weight.\n\n            .. deprecated:: 1.3\n               The parameter `sample_weight` is deprecated in version 1.3\n               and will be removed in 1.5.\n\n        Returns\n        -------\n        labels : ndarray of shape (n_samples,)\n            Index of the cluster each sample belongs to.\n        \"\"\"\n        check_is_fitted(self)\n\n        X = self._check_test_data(X)\n        if not (isinstance(sample_weight, str) and sample_weight == \"deprecated\"):\n            warnings.warn(\n                (\n                    \"'sample_weight' was deprecated in version 1.3 and \"\n                    \"will be removed in 1.5.\"\n                ),\n                FutureWarning,\n            )\n            sample_weight = _check_sample_weight(sample_weight, X, dtype=X.dtype)\n        else:\n            sample_weight = _check_sample_weight(None, X, dtype=X.dtype)\n\n        labels = _labels_inertia_threadpool_limit(\n            X,\n            sample_weight,\n            self.cluster_centers_,\n            n_threads=self._n_threads,\n            return_inertia=False,\n        )\n\n        return labels\n\n    def fit_transform(self, X, y=None, sample_weight=None):\n        \"\"\"Compute clustering and transform X to cluster-distance space.\n\n        Equivalent to fit(X).transform(X), but more efficiently implemented.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            New data to transform.\n\n        y : Ignored\n            Not used, present here for API consistency by convention.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            The weights for each observation in X. If None, all observations\n            are assigned equal weight.\n\n        Returns\n        -------\n        X_new : ndarray of shape (n_samples, n_clusters)\n            X transformed in the new space.\n        \"\"\"\n        return self.fit(X, sample_weight=sample_weight)._transform(X)\n\n    def transform(self, X):\n        \"\"\"Transform X to a cluster-distance space.\n\n        In the new space, each dimension is the distance to the cluster\n        centers. Note that even if X is sparse, the array returned by\n        `transform` will typically be dense.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            New data to transform.\n\n        Returns\n        -------\n        X_new : ndarray of shape (n_samples, n_clusters)\n            X transformed in the new space.\n        \"\"\"\n        check_is_fitted(self)\n\n        X = self._check_test_data(X)\n        return self._transform(X)\n\n    def _transform(self, X):\n        \"\"\"Guts of transform method; no input validation.\"\"\"\n        return euclidean_distances(X, self.cluster_centers_)\n\n    def score(self, X, y=None, sample_weight=None):\n        \"\"\"Opposite of the value of X on the K-means objective.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            New data.\n\n        y : Ignored\n            Not used, present here for API consistency by convention.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            The weights for each observation in X. If None, all observations\n            are assigned equal weight.\n\n        Returns\n        -------\n        score : float\n            Opposite of the value of X on the K-means objective.\n        \"\"\"\n        check_is_fitted(self)\n\n        X = self._check_test_data(X)\n        sample_weight = _check_sample_weight(sample_weight, X, dtype=X.dtype)\n\n        _, scores = _labels_inertia_threadpool_limit(\n            X, sample_weight, self.cluster_centers_, self._n_threads\n        )\n        return -scores\n\n    def _more_tags(self):\n        return {\n            \"_xfail_checks\": {\n                \"check_sample_weights_invariance\": (\n                    \"zero sample_weight is not equivalent to removing samples\"\n                ),\n            },\n        }\n",
      "name_column": 6,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "dataset/_kmeans.py/KMeans",
        "dataset/_kmeans.py/MiniBatchKMeans"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [
        "**__init__**: The function of __init__ is to initialize an instance of the BaseKMeans class with specified parameters for clustering.\n\n**parameters**: The parameters of this Function.\n· n_clusters: The number of clusters to form as well as the number of centroids to generate.\n· init: Method for initialization of the centroids. This can be a string or an array.\n· n_init: Number of times the k-means algorithm will be run with different centroid seeds.\n· max_iter: Maximum number of iterations of the k-means algorithm for a single run.\n· tol: Relative tolerance with regards to inertia to declare convergence.\n· verbose: Controls the verbosity of the output during the fitting process.\n· random_state: Determines random number generation for centroid initialization. Use an int for reproducibility.\n\n**Code Description**: The __init__ function serves as the constructor for the BaseKMeans class. It takes several parameters that dictate how the k-means clustering algorithm will operate. The parameter n_clusters specifies how many clusters the algorithm should identify in the dataset. The init parameter determines how the initial centroids are chosen, which can significantly affect the outcome of the clustering. The n_init parameter indicates how many times the algorithm should be executed with different initial centroid seeds to ensure a more reliable result. The max_iter parameter sets a limit on the number of iterations for each run of the algorithm, while tol provides a threshold for determining when the algorithm has converged. The verbose parameter allows users to control the level of detail in the output messages during the fitting process, which can be useful for debugging or understanding the algorithm's progress. Lastly, the random_state parameter ensures that the results can be reproduced by controlling the randomness in the initialization process.\n\n**Note**: It is important to provide appropriate values for these parameters to achieve optimal clustering results. Users should be aware that the choice of n_clusters can significantly influence the performance and outcome of the k-means algorithm. Additionally, setting a random_state can help in obtaining consistent results across different runs."
      ],
      "code_start_line": 857,
      "code_end_line": 874,
      "params": [
        "self",
        "n_clusters"
      ],
      "have_return": false,
      "code_content": "    def __init__(\n        self,\n        n_clusters,\n        *,\n        init,\n        n_init,\n        max_iter,\n        tol,\n        verbose,\n        random_state,\n    ):\n        self.n_clusters = n_clusters\n        self.init = init\n        self.max_iter = max_iter\n        self.tol = tol\n        self.n_init = n_init\n        self.verbose = verbose\n        self.random_state = random_state\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "_check_params_vs_input",
      "md_content": [
        "**_check_params_vs_input**: The function of _check_params_vs_input is to validate the input parameters against the dataset provided to ensure that the KMeans clustering algorithm can operate correctly.\n\n**parameters**: The parameters of this Function.\n· parameter1: X - The input dataset, which is expected to be a two-dimensional array-like structure representing the samples and features.\n· parameter2: default_n_init - An optional integer that specifies the default number of initializations to use if the initialization method is set to \"random\" or a callable.\n\n**Code Description**: The _check_params_vs_input function performs several critical checks and assignments related to the parameters used in the KMeans clustering algorithm. \n\nFirst, it verifies that the number of samples in the dataset X (represented by X.shape[0]) is greater than or equal to the number of clusters specified by self.n_clusters. If this condition is not met, a ValueError is raised, indicating that the number of samples must be sufficient to form the requested clusters.\n\nNext, the function calculates the tolerance value by calling the _tolerance function, passing the dataset X and the tolerance parameter self.tol. This computed tolerance is stored in the instance variable self._tol, which is essential for determining the convergence criteria during the clustering process.\n\nThe function then assesses the initialization method specified by self.init and determines the appropriate number of initializations (self._n_init) to perform. If self.n_init is set to \"auto\", the function checks the type of self.init. If it is a string and equals \"k-means++\", it sets self._n_init to 1. If it is \"random\", it assigns the value of default_n_init to self._n_init. If self.init is a callable, it also uses default_n_init. For any other case, including when self.init is an array-like structure, it defaults to 1.\n\nAdditionally, if self.init is an array-like structure and self._n_init is not equal to 1, a warning is issued to inform the user that only one initialization will be performed, despite the specified n_init value. This warning is raised as a RuntimeWarning, indicating that the behavior of the algorithm may differ from the user's expectations.\n\nThis function is integral to the operation of the KMeans algorithm, as it ensures that the parameters are correctly set up before the clustering process begins, thereby influencing the algorithm's performance and results.\n\n**Note**: It is crucial to ensure that the dataset X has enough samples relative to the number of clusters specified. Additionally, the initialization method and the number of initializations should be chosen carefully, as they can significantly impact the convergence and final clustering results."
      ],
      "code_start_line": 876,
      "code_end_line": 909,
      "params": [
        "self",
        "X",
        "default_n_init"
      ],
      "have_return": false,
      "code_content": "    def _check_params_vs_input(self, X, default_n_init=None):\n        # n_clusters\n        if X.shape[0] < self.n_clusters:\n            raise ValueError(\n                f\"n_samples={X.shape[0]} should be >= n_clusters={self.n_clusters}.\"\n            )\n\n        # tol\n        self._tol = _tolerance(X, self.tol)\n\n        # n-init\n        if self.n_init == \"auto\":\n            if isinstance(self.init, str) and self.init == \"k-means++\":\n                self._n_init = 1\n            elif isinstance(self.init, str) and self.init == \"random\":\n                self._n_init = default_n_init\n            elif callable(self.init):\n                self._n_init = default_n_init\n            else:  # array-like\n                self._n_init = 1\n        else:\n            self._n_init = self.n_init\n\n        if _is_arraylike_not_scalar(self.init) and self._n_init != 1:\n            warnings.warn(\n                (\n                    \"Explicit initial center position passed: performing only\"\n                    f\" one init in {self.__class__.__name__} instead of \"\n                    f\"n_init={self._n_init}.\"\n                ),\n                RuntimeWarning,\n                stacklevel=2,\n            )\n            self._n_init = 1\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "dataset/_kmeans.py/_tolerance"
      ],
      "special_reference_type": [
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "_warn_mkl_vcomp",
      "md_content": [
        "**_warn_mkl_vcomp**: The function of _warn_mkl_vcomp is to issue a specific warning related to the use of vcomp and MKL libraries when both are present in the environment.\n\n**parameters**: The parameters of this Function.\n· n_active_threads: An integer representing the number of active threads that are currently being utilized.\n\n**Code Description**: The _warn_mkl_vcomp function is designed to issue a warning when both the vcomp (Microsoft OpenMP) and MKL (Intel Math Kernel Library) libraries are detected in the execution environment. This function is invoked by the _check_mkl_vcomp method, which is responsible for checking the compatibility of these libraries during the execution of KMeans clustering algorithms.\n\nThe _check_mkl_vcomp method first assesses whether the input data X is sparse. If it is not sparse, the method calculates the number of active threads based on the number of samples and a predefined chunk size. It then checks if the number of active threads is less than the number of threads configured for the KMeans instance. If this condition is met, the method retrieves information about the threading modules in use and checks for the presence of both vcomp and MKL.\n\nIf both libraries are present, the _warn_mkl_vcomp function is called with the number of active threads as an argument. This warning mechanism is crucial as it addresses a known issue where the combination of these libraries can lead to performance degradation or memory leaks during the execution of the KMeans algorithm.\n\n**Note**: It is important for developers to be aware of the implications of using both vcomp and MKL together, as this can affect the stability and performance of the KMeans clustering process. Proper configuration and awareness of the threading libraries in use are essential to avoid potential issues."
      ],
      "code_start_line": 912,
      "code_end_line": 916,
      "params": [
        "self",
        "n_active_threads"
      ],
      "have_return": false,
      "code_content": "    def _warn_mkl_vcomp(self, n_active_threads):\n        \"\"\"Issue an estimator specific warning when vcomp and mkl are both present\n\n        This method is called by `_check_mkl_vcomp`.\n        \"\"\"\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "dataset/_kmeans.py/_BaseKMeans/_check_mkl_vcomp"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "_check_mkl_vcomp",
      "md_content": [
        "**_check_mkl_vcomp**: The function of _check_mkl_vcomp is to verify the compatibility of the vcomp and MKL libraries when both are present in the execution environment.\n\n**parameters**: The parameters of this Function.\n· X: The input data, which can be either a dense array or a sparse matrix, representing the samples to be clustered.\n· n_samples: An integer representing the total number of samples in the input data X.\n\n**Code Description**: The _check_mkl_vcomp function is designed to assess the compatibility of the vcomp (Microsoft OpenMP) and MKL (Intel Math Kernel Library) libraries during the execution of KMeans clustering algorithms. This function is particularly important as it addresses a known issue where the combination of these libraries can lead to performance degradation or memory leaks.\n\nThe function begins by checking if the input data X is sparse using the `sp.issparse` method. If X is sparse, the function exits early, as the compatibility check is not necessary for sparse data. \n\nNext, the function calculates the number of active threads based on the number of samples and a predefined chunk size (CHUNK_SIZE). It then compares the number of active threads with the number of threads configured for the KMeans instance (self._n_threads). If the number of active threads is less than the configured threads, the function proceeds to gather information about the threading modules currently in use.\n\nThe function retrieves the threading module information using the `threadpool_info()` function. It checks for the presence of both vcomp and MKL by examining the module information. Specifically, it looks for \"vcomp\" in the list of module prefixes and checks if the tuple (\"mkl\", \"intel\") is present in the internal API and threading layer information.\n\nIf both vcomp and MKL are detected, the function invokes the _warn_mkl_vcomp method, passing the number of active threads as an argument. This warning mechanism is crucial for informing users about the potential issues that may arise from using both libraries simultaneously.\n\nThe _check_mkl_vcomp function is called within the fit method of both the KMeans and MiniBatchKMeans classes. In the KMeans fit method, it is called after the input data has been validated and before the clustering algorithm is executed. In the MiniBatchKMeans fit method, it is called similarly, ensuring that the compatibility check is performed before processing the mini-batches of data. Additionally, it is also invoked in the partial_fit method of MiniBatchKMeans to ensure compatibility during incremental updates.\n\n**Note**: Developers should be aware of the implications of using both vcomp and MKL together, as this can affect the stability and performance of the KMeans clustering process. Proper configuration and awareness of the threading libraries in use are essential to avoid potential issues.\n\n**Output Example**: The function does not return a value but may trigger a warning if both vcomp and MKL are detected in the environment. An example warning message could be: \"Warning: Both vcomp and MKL are present. This may lead to performance issues.\""
      ],
      "code_start_line": 918,
      "code_end_line": 936,
      "params": [
        "self",
        "X",
        "n_samples"
      ],
      "have_return": true,
      "code_content": "    def _check_mkl_vcomp(self, X, n_samples):\n        \"\"\"Check when vcomp and mkl are both present\"\"\"\n        # The BLAS call inside a prange in lloyd_iter_chunked_dense is known to\n        # cause a small memory leak when there are less chunks than the number\n        # of available threads. It only happens when the OpenMP library is\n        # vcomp (microsoft OpenMP) and the BLAS library is MKL. see #18653\n        if sp.issparse(X):\n            return\n\n        n_active_threads = int(np.ceil(n_samples / CHUNK_SIZE))\n        if n_active_threads < self._n_threads:\n            modules = threadpool_info()\n            has_vcomp = \"vcomp\" in [module[\"prefix\"] for module in modules]\n            has_mkl = (\"mkl\", \"intel\") in [\n                (module[\"internal_api\"], module.get(\"threading_layer\", None))\n                for module in modules\n            ]\n            if has_vcomp and has_mkl:\n                self._warn_mkl_vcomp(n_active_threads)\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "dataset/_kmeans.py/KMeans/fit",
        "dataset/_kmeans.py/MiniBatchKMeans/fit",
        "dataset/_kmeans.py/MiniBatchKMeans/partial_fit"
      ],
      "reference_who": [
        "dataset/_kmeans.py/_BaseKMeans/_warn_mkl_vcomp"
      ],
      "special_reference_type": [
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "_validate_center_shape",
      "md_content": [
        "**_validate_center_shape**: The function of _validate_center_shape is to check if the shape of the initial cluster centers is compatible with the input data and the specified number of clusters.\n\n**parameters**: The parameters of this Function.\n· X: {ndarray, sparse matrix} of shape (n_samples, n_features) - The input data samples for which the clustering is performed.\n· centers: ndarray of shape (n_clusters, n_features) - The initial centers of the clusters that need to be validated.\n\n**Code Description**: The _validate_center_shape function is designed to ensure that the shape of the provided initial cluster centers aligns with the expected dimensions based on the input data and the number of clusters specified in the model. It performs two primary checks:\n\n1. It verifies that the number of rows in the centers array matches the number of clusters (self.n_clusters). If there is a mismatch, it raises a ValueError indicating that the shape of the initial centers does not correspond to the expected number of clusters.\n\n2. It checks that the number of columns in the centers array matches the number of features in the input data (X). If this condition is not met, it raises a ValueError, stating that the shape of the initial centers does not match the number of features in the input data.\n\nThis function is called within the _init_centroids method, which is responsible for computing the initial centroids for the clustering algorithm. When the initialization method is set to a callable or a user-provided array, the _validate_center_shape function is invoked to ensure that the provided centers are valid before proceeding with the clustering process.\n\nAdditionally, the _validate_center_shape function is also utilized in the fit methods of both the KMeans and MiniBatchKMeans classes. In these methods, it validates the initial cluster centers when they are provided as an array. This ensures that the clustering algorithms operate with valid configurations, preventing runtime errors that could arise from incompatible shapes.\n\n**Note**: It is essential to ensure that the initial centers provided to the clustering algorithm conform to the expected dimensions to avoid exceptions during the clustering process. Users should be aware of the shape requirements when initializing cluster centers, especially when using custom initialization methods."
      ],
      "code_start_line": 938,
      "code_end_line": 949,
      "params": [
        "self",
        "X",
        "centers"
      ],
      "have_return": false,
      "code_content": "    def _validate_center_shape(self, X, centers):\n        \"\"\"Check if centers is compatible with X and n_clusters.\"\"\"\n        if centers.shape[0] != self.n_clusters:\n            raise ValueError(\n                f\"The shape of the initial centers {centers.shape} does not \"\n                f\"match the number of clusters {self.n_clusters}.\"\n            )\n        if centers.shape[1] != X.shape[1]:\n            raise ValueError(\n                f\"The shape of the initial centers {centers.shape} does not \"\n                f\"match the number of features of the data {X.shape[1]}.\"\n            )\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "dataset/_kmeans.py/_BaseKMeans/_init_centroids",
        "dataset/_kmeans.py/KMeans/fit",
        "dataset/_kmeans.py/MiniBatchKMeans/fit",
        "dataset/_kmeans.py/MiniBatchKMeans/partial_fit"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "_check_test_data",
      "md_content": [
        "**_check_test_data**: The function of _check_test_data is to validate and prepare the input data for further processing.\n\n**parameters**: The parameters of this Function.\n· X: {array-like, sparse matrix} of shape (n_samples, n_features) - The input data that needs to be validated.\n\n**Code Description**: The _check_test_data function is responsible for ensuring that the input data X is in an acceptable format before it is used in other methods of the _BaseKMeans class. It utilizes the _validate_data method to perform several checks and transformations on the input data. The parameters passed to _validate_data specify that the function accepts sparse matrices in the Compressed Sparse Row (CSR) format, allows for both float64 and float32 data types, and does not reset any internal state. Additionally, it ensures that large sparse matrices are not accepted.\n\nThis function is called by several other methods within the _BaseKMeans class, including predict, transform, and score. In each of these methods, _check_test_data is invoked to validate the input data before any clustering operations are performed. This ensures that the data being processed is in the correct format and meets the necessary requirements, thereby preventing potential errors during execution.\n\n**Note**: It is important to ensure that the input data X conforms to the specified data types and formats, as violations may lead to exceptions or incorrect behavior in the clustering algorithms.\n\n**Output Example**: A possible return value of the function could be a validated NumPy array of shape (n_samples, n_features) containing the input data in the specified format, ready for further processing in the K-means clustering methods."
      ],
      "code_start_line": 951,
      "code_end_line": 960,
      "params": [
        "self",
        "X"
      ],
      "have_return": true,
      "code_content": "    def _check_test_data(self, X):\n        X = self._validate_data(\n            X,\n            accept_sparse=\"csr\",\n            reset=False,\n            dtype=[np.float64, np.float32],\n            order=\"C\",\n            accept_large_sparse=False,\n        )\n        return X\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "dataset/_kmeans.py/_BaseKMeans/predict",
        "dataset/_kmeans.py/_BaseKMeans/transform",
        "dataset/_kmeans.py/_BaseKMeans/score"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "_init_centroids",
      "md_content": [
        "**_init_centroids**: The function of _init_centroids is to compute the initial centroids for the k-means clustering algorithm based on the specified initialization method.\n\n**parameters**: The parameters of this Function.\n· X: {ndarray, sparse matrix} of shape (n_samples, n_features) - The input samples for which the centroids are to be initialized.\n· x_squared_norms: ndarray of shape (n_samples,) - Squared Euclidean norm of each data point. This can be passed directly to avoid recomputation.\n· init: {'k-means++', 'random'}, callable or ndarray of shape (n_clusters, n_features) - Method for initialization of centroids.\n· random_state: RandomState instance - Controls the randomness of centroid initialization.\n· sample_weight: ndarray of shape (n_samples,) - The weights for each observation in X. This is not used during initialization if `init` is a callable or a user-provided array.\n· init_size: int, default=None - Number of samples to randomly sample for speeding up the initialization process.\n· n_centroids: int, default=None - Number of centroids to initialize. If None, it defaults to the number of clusters specified in the model.\n\n**Code Description**: The _init_centroids function is responsible for calculating the initial cluster centers for the k-means clustering algorithm. It begins by determining the number of samples and the number of clusters to initialize. If an `init_size` is provided and is less than the total number of samples, a random subset of samples is selected to speed up the initialization process.\n\nThe function supports multiple methods for initializing centroids:\n1. If `init` is set to \"k-means++\", it calls the _kmeans_plusplus function, which selects initial centers in a way that is expected to improve the convergence speed of the k-means algorithm.\n2. If `init` is set to \"random\", it randomly selects samples from the input data as the initial centroids, weighted by the provided sample weights.\n3. If `init` is an array-like structure, it uses the provided array as the initial centers.\n4. If `init` is a callable function, it invokes this function to generate the initial centers.\n\nAfter determining the initial centers, the function checks if the centers are in a sparse format and converts them to a dense array if necessary. The computed centers are then returned.\n\nThis function is called by the fit methods of both the KMeans and MiniBatchKMeans classes. In these methods, it is used to initialize the centroids before running the k-means algorithm. The fit methods validate the input data and parameters, and they ensure that the initialization is performed correctly, leveraging the _init_centroids function to set up the initial state for clustering.\n\n**Note**: It is crucial to ensure that the number of samples in `X` is greater than or equal to the number of clusters specified. Users should also be aware that the choice of initialization method can significantly impact the performance and outcome of the k-means clustering process.\n\n**Output Example**: A possible return value of the function could be:\ncenters: array([[10, 2],\n                [1, 0]])"
      ],
      "code_start_line": 962,
      "code_end_line": 1046,
      "params": [
        "self",
        "X",
        "x_squared_norms",
        "init",
        "random_state",
        "sample_weight",
        "init_size",
        "n_centroids"
      ],
      "have_return": true,
      "code_content": "    def _init_centroids(\n        self,\n        X,\n        x_squared_norms,\n        init,\n        random_state,\n        sample_weight,\n        init_size=None,\n        n_centroids=None,\n    ):\n        \"\"\"Compute the initial centroids.\n\n        Parameters\n        ----------\n        X : {ndarray, sparse matrix} of shape (n_samples, n_features)\n            The input samples.\n\n        x_squared_norms : ndarray of shape (n_samples,)\n            Squared euclidean norm of each data point. Pass it if you have it\n            at hands already to avoid it being recomputed here.\n\n        init : {'k-means++', 'random'}, callable or ndarray of shape \\\n                (n_clusters, n_features)\n            Method for initialization.\n\n        random_state : RandomState instance\n            Determines random number generation for centroid initialization.\n            See :term:`Glossary <random_state>`.\n\n        sample_weight : ndarray of shape (n_samples,)\n            The weights for each observation in X. `sample_weight` is not used\n            during initialization if `init` is a callable or a user provided\n            array.\n\n        init_size : int, default=None\n            Number of samples to randomly sample for speeding up the\n            initialization (sometimes at the expense of accuracy).\n\n        n_centroids : int, default=None\n            Number of centroids to initialize.\n            If left to 'None' the number of centroids will be equal to\n            number of clusters to form (self.n_clusters).\n\n        Returns\n        -------\n        centers : ndarray of shape (n_clusters, n_features)\n            Initial centroids of clusters.\n        \"\"\"\n        n_samples = X.shape[0]\n        n_clusters = self.n_clusters if n_centroids is None else n_centroids\n\n        if init_size is not None and init_size < n_samples:\n            init_indices = random_state.randint(0, n_samples, init_size)\n            X = X[init_indices]\n            x_squared_norms = x_squared_norms[init_indices]\n            n_samples = X.shape[0]\n            sample_weight = sample_weight[init_indices]\n\n        if isinstance(init, str) and init == \"k-means++\":\n            centers, _ = _kmeans_plusplus(\n                X,\n                n_clusters,\n                random_state=random_state,\n                x_squared_norms=x_squared_norms,\n                sample_weight=sample_weight,\n            )\n        elif isinstance(init, str) and init == \"random\":\n            seeds = random_state.choice(\n                n_samples,\n                size=n_clusters,\n                replace=False,\n                p=sample_weight / sample_weight.sum(),\n            )\n            centers = X[seeds]\n        elif _is_arraylike_not_scalar(self.init):\n            centers = init\n        elif callable(init):\n            centers = init(X, n_clusters, random_state=random_state)\n            centers = check_array(centers, dtype=X.dtype, copy=False, order=\"C\")\n            self._validate_center_shape(X, centers)\n\n        if sp.issparse(centers):\n            centers = centers.toarray()\n\n        return centers\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "dataset/_kmeans.py/KMeans/fit",
        "dataset/_kmeans.py/MiniBatchKMeans/fit",
        "dataset/_kmeans.py/MiniBatchKMeans/partial_fit"
      ],
      "reference_who": [
        "dataset/_kmeans.py/_kmeans_plusplus",
        "dataset/_kmeans.py/_BaseKMeans/_validate_center_shape"
      ],
      "special_reference_type": [
        false,
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "fit_predict",
      "md_content": [
        "**fit_predict**: The function of fit_predict is to compute cluster centers and predict the cluster index for each sample.\n\n**parameters**: The parameters of this Function.\n· X: {array-like, sparse matrix} of shape (n_samples, n_features) - New data to transform.\n· y: Ignored - Not used, present here for API consistency by convention.\n· sample_weight: array-like of shape (n_samples,), default=None - The weights for each observation in X. If None, all observations are assigned equal weight.\n\n**Code Description**: The fit_predict function serves as a convenience method that combines two operations: fitting the model to the provided data and predicting the cluster indices for that data. It first calls the fit method on the input data X, which computes the necessary parameters for clustering, such as the cluster centers. The sample_weight parameter allows users to assign different weights to each observation, which can influence the clustering outcome. After fitting the model, the function retrieves the labels of the clusters assigned to each sample from the fitted model and returns these labels as an ndarray of shape (n_samples,). This method is particularly useful for users who want to perform clustering and obtain the results in a single step.\n\n**Note**: It is important to note that the parameter y is not utilized in this function; it is included solely for maintaining consistency with the API conventions. Users should ensure that the input data X is appropriately shaped and formatted to avoid errors during execution.\n\n**Output Example**: A possible return value of the fit_predict function could be an array like [0, 1, 0, 2, 1], indicating the cluster index for each of the samples in the input data."
      ],
      "code_start_line": 1048,
      "code_end_line": 1071,
      "params": [
        "self",
        "X",
        "y",
        "sample_weight"
      ],
      "have_return": true,
      "code_content": "    def fit_predict(self, X, y=None, sample_weight=None):\n        \"\"\"Compute cluster centers and predict cluster index for each sample.\n\n        Convenience method; equivalent to calling fit(X) followed by\n        predict(X).\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            New data to transform.\n\n        y : Ignored\n            Not used, present here for API consistency by convention.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            The weights for each observation in X. If None, all observations\n            are assigned equal weight.\n\n        Returns\n        -------\n        labels : ndarray of shape (n_samples,)\n            Index of the cluster each sample belongs to.\n        \"\"\"\n        return self.fit(X, sample_weight=sample_weight).labels_\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "predict",
      "md_content": [
        "**predict**: The function of predict is to determine the closest cluster for each sample in the input data X.\n\n**parameters**: The parameters of this Function.\n· X: {array-like, sparse matrix} of shape (n_samples, n_features) - New data to predict the cluster labels for.  \n· sample_weight: array-like of shape (n_samples,), default=\"deprecated\" - The weights for each observation in X. If None, all observations are assigned equal weight. This parameter is deprecated and will be removed in future versions.\n\n**Code Description**: The predict function is designed to assign each sample in the input data X to the nearest cluster based on the cluster centers established during the fitting process. Initially, the function checks if the model has been fitted using the check_is_fitted method, ensuring that the necessary parameters are available for making predictions.\n\nNext, the input data X is validated and prepared for processing through the _check_test_data method, which ensures that the data is in the correct format and meets the requirements for further analysis. The function also handles the sample_weight parameter, which is marked as deprecated. If the sample_weight is not set to the deprecated string, a warning is issued to inform the user of its impending removal in future versions. The sample weights are then validated using the _check_sample_weight function, which ensures that they are compatible with the input data.\n\nThe core functionality of the predict method is executed by calling the _labels_inertia_threadpool_limit function. This function computes the labels for the samples in X by determining which cluster center is closest to each sample. It operates within a controlled thread pool context to optimize performance and resource usage. The labels returned indicate the index of the cluster each sample belongs to.\n\nThe predict method is integral to the K-means clustering process, as it allows users to classify new data points based on the clusters identified during the fitting phase. It is commonly used in scenarios where new observations need to be categorized according to previously established clusters.\n\n**Note**: Users should be aware that the sample_weight parameter is deprecated and should avoid using it in future implementations. Additionally, it is crucial to ensure that the input data X is formatted correctly to prevent errors during execution.\n\n**Output Example**: A possible return value of the function when called with a dataset of 5 samples might look like this:  \nLabels: [0, 1, 0, 2, 1]"
      ],
      "code_start_line": 1073,
      "code_end_line": 1121,
      "params": [
        "self",
        "X",
        "sample_weight"
      ],
      "have_return": true,
      "code_content": "    def predict(self, X, sample_weight=\"deprecated\"):\n        \"\"\"Predict the closest cluster each sample in X belongs to.\n\n        In the vector quantization literature, `cluster_centers_` is called\n        the code book and each value returned by `predict` is the index of\n        the closest code in the code book.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            New data to predict.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            The weights for each observation in X. If None, all observations\n            are assigned equal weight.\n\n            .. deprecated:: 1.3\n               The parameter `sample_weight` is deprecated in version 1.3\n               and will be removed in 1.5.\n\n        Returns\n        -------\n        labels : ndarray of shape (n_samples,)\n            Index of the cluster each sample belongs to.\n        \"\"\"\n        check_is_fitted(self)\n\n        X = self._check_test_data(X)\n        if not (isinstance(sample_weight, str) and sample_weight == \"deprecated\"):\n            warnings.warn(\n                (\n                    \"'sample_weight' was deprecated in version 1.3 and \"\n                    \"will be removed in 1.5.\"\n                ),\n                FutureWarning,\n            )\n            sample_weight = _check_sample_weight(sample_weight, X, dtype=X.dtype)\n        else:\n            sample_weight = _check_sample_weight(None, X, dtype=X.dtype)\n\n        labels = _labels_inertia_threadpool_limit(\n            X,\n            sample_weight,\n            self.cluster_centers_,\n            n_threads=self._n_threads,\n            return_inertia=False,\n        )\n\n        return labels\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "dataset/_kmeans.py/_labels_inertia_threadpool_limit",
        "dataset/_kmeans.py/_BaseKMeans/_check_test_data"
      ],
      "special_reference_type": [
        false,
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "fit_transform",
      "md_content": [
        "**fit_transform**: The function of fit_transform is to compute clustering and transform the input data X into a cluster-distance space efficiently.\n\n**parameters**: The parameters of this Function.\n· X: {array-like, sparse matrix} of shape (n_samples, n_features) - New data to transform.\n· y: Ignored - Not used, present here for API consistency by convention.\n· sample_weight: array-like of shape (n_samples,), default=None - The weights for each observation in X. If None, all observations are assigned equal weight.\n\n**Code Description**: The fit_transform method is designed to perform clustering on the input data X and subsequently transform it into a new space that reflects the distances to the identified clusters. This method is equivalent to first fitting the model to the data using the fit method and then transforming the data using the transform method. However, it is implemented in a more efficient manner to optimize performance. The input parameter X represents the data to be clustered, while the parameter y is included for consistency with other methods but is not utilized in this function. The sample_weight parameter allows for the specification of weights for each observation, enabling the user to influence the clustering process based on the importance of individual samples. If sample_weight is not provided, all samples are treated equally. The method returns an ndarray of shape (n_samples, n_clusters), which contains the transformed data in the new cluster-distance space.\n\n**Note**: It is important to ensure that the input data X is in the correct format (array-like or sparse matrix) and has the appropriate shape. The sample_weight parameter is optional, but when used, it should match the number of samples in X.\n\n**Output Example**: A possible return value of the fit_transform method could be an ndarray such as:\n[[0.1, 0.9, 0.0],\n [0.2, 0.8, 0.0],\n [0.0, 0.0, 1.0]] \nThis output represents the distances of each sample to the respective clusters in the transformed space."
      ],
      "code_start_line": 1123,
      "code_end_line": 1145,
      "params": [
        "self",
        "X",
        "y",
        "sample_weight"
      ],
      "have_return": true,
      "code_content": "    def fit_transform(self, X, y=None, sample_weight=None):\n        \"\"\"Compute clustering and transform X to cluster-distance space.\n\n        Equivalent to fit(X).transform(X), but more efficiently implemented.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            New data to transform.\n\n        y : Ignored\n            Not used, present here for API consistency by convention.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            The weights for each observation in X. If None, all observations\n            are assigned equal weight.\n\n        Returns\n        -------\n        X_new : ndarray of shape (n_samples, n_clusters)\n            X transformed in the new space.\n        \"\"\"\n        return self.fit(X, sample_weight=sample_weight)._transform(X)\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "transform",
      "md_content": [
        "**transform**: The function of transform is to convert input data into a new space representing distances to cluster centers.\n\n**parameters**: The parameters of this Function.\n· X: {array-like, sparse matrix} of shape (n_samples, n_features) - New data to transform.\n\n**Code Description**: The transform method is a key function within the _BaseKMeans class that facilitates the transformation of input data X into a cluster-distance space. In this transformed space, each dimension corresponds to the distance from the data points to the identified cluster centers. The method begins by ensuring that the model has been fitted with the training data through the call to check_is_fitted(self). This is crucial as it verifies that the necessary cluster centers are available for distance calculations.\n\nFollowing this check, the method invokes _check_test_data(X) to validate and prepare the input data. This function ensures that the input data X is in an acceptable format, allowing for both dense and sparse representations. Once the data has been validated, the method proceeds to call _transform(X), which is responsible for computing the actual distances between the input data points and the cluster centers.\n\nThe _transform function operates without performing any input validation, relying on the prior checks conducted by the transform method. It utilizes the euclidean_distances function to calculate the distances, returning a new array where each row corresponds to a sample from X and each column corresponds to a cluster center.\n\nThis structured approach, where transform manages data validation and preparation while _transform focuses solely on distance computation, ensures a clear separation of responsibilities within the code. As a result, users can confidently transform their data into a format suitable for further analysis or clustering operations.\n\n**Note**: It is essential to ensure that the model is fitted before invoking the transform method. Additionally, the input data X must conform to the specified formats and types to avoid potential errors during execution.\n\n**Output Example**: A possible return value of the transform function could be a 2D NumPy array where each row represents a sample from X and each column corresponds to a cluster center, with values indicating the distances. For example, if there are 3 samples and 2 clusters, the output might look like:\n```\narray([[1.5, 2.0],\n       [0.5, 1.2],\n       [3.1, 0.8]])\n```"
      ],
      "code_start_line": 1147,
      "code_end_line": 1167,
      "params": [
        "self",
        "X"
      ],
      "have_return": true,
      "code_content": "    def transform(self, X):\n        \"\"\"Transform X to a cluster-distance space.\n\n        In the new space, each dimension is the distance to the cluster\n        centers. Note that even if X is sparse, the array returned by\n        `transform` will typically be dense.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            New data to transform.\n\n        Returns\n        -------\n        X_new : ndarray of shape (n_samples, n_clusters)\n            X transformed in the new space.\n        \"\"\"\n        check_is_fitted(self)\n\n        X = self._check_test_data(X)\n        return self._transform(X)\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "dataset/_kmeans.py/_BaseKMeans/_check_test_data",
        "dataset/_kmeans.py/_BaseKMeans/_transform"
      ],
      "special_reference_type": [
        false,
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "_transform",
      "md_content": [
        "**_transform**: The function of _transform is to compute the distances between input data points and the cluster centers.\n\n**parameters**: The parameters of this Function.\n· X: array-like or sparse matrix of shape (n_samples, n_features) representing the new data to transform.\n\n**Code Description**: The _transform function is a core component of the clustering algorithm that calculates the Euclidean distances from each data point in the input array X to the cluster centers stored in the attribute self.cluster_centers_. This function does not perform any input validation, which means it assumes that the input data X has already been checked and is in the correct format. \n\nThe _transform function is called by the transform method of the _BaseKMeans class. The transform method serves as a public interface for users to convert their data into a new space where each dimension corresponds to the distance from the data points to the identified cluster centers. Before invoking _transform, the transform method ensures that the model has been fitted with the training data by calling check_is_fitted(self) and also checks the test data format with _check_test_data(X). This layered approach allows for a clean separation of concerns, where _transform focuses solely on the distance computation, while transform handles data validation and preparation.\n\n**Note**: It is important to ensure that the model is fitted before calling the transform method, as the _transform function relies on the cluster centers being available.\n\n**Output Example**: A possible return value of the _transform function could be a 2D NumPy array where each row corresponds to a sample from X and each column corresponds to a cluster center, with the values representing the distances. For instance, if there are 3 samples and 2 clusters, the output might look like:\n```\narray([[1.5, 2.0],\n       [0.5, 1.2],\n       [3.1, 0.8]])\n```"
      ],
      "code_start_line": 1169,
      "code_end_line": 1171,
      "params": [
        "self",
        "X"
      ],
      "have_return": true,
      "code_content": "    def _transform(self, X):\n        \"\"\"Guts of transform method; no input validation.\"\"\"\n        return euclidean_distances(X, self.cluster_centers_)\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "dataset/_kmeans.py/_BaseKMeans/transform"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "score",
      "md_content": [
        "**score**: The function of score is to compute the opposite of the value of X on the K-means objective.\n\n**parameters**: The parameters of this Function.\n· X: {array-like, sparse matrix} of shape (n_samples, n_features) - New data to evaluate against the K-means model.  \n· y: Ignored - This parameter is not used and is present for API consistency by convention.  \n· sample_weight: array-like of shape (n_samples,), default=None - The weights for each observation in X. If None, all observations are assigned equal weight.\n\n**Code Description**: The score function is designed to evaluate the performance of the K-means clustering algorithm on a given dataset X. It begins by ensuring that the K-means model has been fitted to the data through the call to check_is_fitted(self). This is a crucial step as it verifies that the model has learned the cluster centers from the training data before any scoring can occur.\n\nNext, the function validates the input data X by calling the _check_test_data method. This method ensures that the data is in an acceptable format, either as a dense array or a sparse matrix in CSR format, and prepares it for further processing. The sample weights are also checked and validated through the _check_sample_weight function, which ensures that they are appropriately defined and match the shape of the input data.\n\nThe core of the scoring mechanism is executed by the _labels_inertia_threadpool_limit function. This function computes the labels and inertia for the input data X based on the cluster centers learned during the fitting process. It operates within a controlled threading context to optimize performance while managing resource usage. The function returns the inertia score, which quantifies the compactness of the clusters formed by the K-means algorithm.\n\nFinally, the score function returns the negative of the computed inertia score. This is because, in the context of K-means, a lower inertia indicates a better clustering solution, and thus the function provides the opposite value to reflect this relationship.\n\nThe score function is typically called after fitting the K-means model to new data, allowing users to evaluate how well the model performs on unseen samples.\n\n**Note**: It is essential to ensure that the input data X is formatted correctly and that the sample weights are defined if used. The function is optimized for performance but requires that the model has been fitted prior to invocation.\n\n**Output Example**: A possible return value of the function when called with a dataset of 5 samples might look like this:  \nScore: -12.34"
      ],
      "code_start_line": 1173,
      "code_end_line": 1201,
      "params": [
        "self",
        "X",
        "y",
        "sample_weight"
      ],
      "have_return": true,
      "code_content": "    def score(self, X, y=None, sample_weight=None):\n        \"\"\"Opposite of the value of X on the K-means objective.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            New data.\n\n        y : Ignored\n            Not used, present here for API consistency by convention.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            The weights for each observation in X. If None, all observations\n            are assigned equal weight.\n\n        Returns\n        -------\n        score : float\n            Opposite of the value of X on the K-means objective.\n        \"\"\"\n        check_is_fitted(self)\n\n        X = self._check_test_data(X)\n        sample_weight = _check_sample_weight(sample_weight, X, dtype=X.dtype)\n\n        _, scores = _labels_inertia_threadpool_limit(\n            X, sample_weight, self.cluster_centers_, self._n_threads\n        )\n        return -scores\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "dataset/_kmeans.py/_labels_inertia_threadpool_limit",
        "dataset/_kmeans.py/_BaseKMeans/_check_test_data"
      ],
      "special_reference_type": [
        false,
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "_more_tags",
      "md_content": [
        "**_more_tags**: The function of _more_tags is to provide additional metadata regarding the behavior of the class, specifically related to sample weight invariance checks.\n\n**parameters**: The parameters of this Function.\n· There are no parameters for this function.\n\n**Code Description**: The _more_tags function returns a dictionary containing specific tags that provide insights into the behavior of the class it belongs to. In this case, it includes a key \"_xfail_checks\" which indicates that there is a known issue with the handling of sample weights. The dictionary specifies that the check for sample weight invariance is expected to fail, with a description stating that \"zero sample_weight is not equivalent to removing samples.\" This suggests that when sample weights are set to zero, the model does not behave as if those samples were entirely excluded from the dataset, which is an important consideration for users implementing this functionality.\n\n**Note**: It is important for users to be aware of this behavior when using the class, as it may affect the results of their model training and evaluation. Understanding this limitation can help in making informed decisions regarding data preprocessing and model configuration.\n\n**Output Example**: The return value of the _more_tags function would appear as follows:\n{\n    \"_xfail_checks\": {\n        \"check_sample_weights_invariance\": (\n            \"zero sample_weight is not equivalent to removing samples\"\n        ),\n    },\n}"
      ],
      "code_start_line": 1203,
      "code_end_line": 1210,
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def _more_tags(self):\n        return {\n            \"_xfail_checks\": {\n                \"check_sample_weights_invariance\": (\n                    \"zero sample_weight is not equivalent to removing samples\"\n                ),\n            },\n        }\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "ClassDef",
      "name": "KMeans",
      "md_content": [
        "**KMeans**: The function of KMeans is to perform K-Means clustering, a popular algorithm used for partitioning a dataset into distinct groups based on feature similarity.\n\n**attributes**: The attributes of this Class.\n· n_clusters: The number of clusters to form as well as the number of centroids to generate.\n· init: Method for initialization of cluster centers.\n· n_init: Number of times the k-means algorithm will be run with different centroid seeds.\n· max_iter: Maximum number of iterations of the k-means algorithm for a single run.\n· tol: Relative tolerance with regards to Frobenius norm of the difference in the cluster centers of two consecutive iterations to declare convergence.\n· verbose: Verbosity mode.\n· random_state: Determines random number generation for centroid initialization.\n· copy_x: When pre-computing distances, indicates whether to modify the original data.\n· algorithm: K-means algorithm to use, either \"lloyd\" or \"elkan\".\n\n**Code Description**: The KMeans class is an implementation of the K-Means clustering algorithm, inheriting from the _BaseKMeans class, which provides foundational functionality and parameter validation. The KMeans class allows users to specify the number of clusters (n_clusters) they wish to form, the method for initializing cluster centers (init), and various other parameters that control the behavior of the algorithm.\n\nUpon initialization, the KMeans class sets up the parameters and checks them against the input data using the inherited methods from _BaseKMeans. The fit method computes the K-Means clustering by iterating through the data, adjusting the cluster centers based on the assigned labels until convergence is achieved or the maximum number of iterations is reached.\n\nThe KMeans class also includes methods for predicting cluster labels for new data points and transforming the data into a cluster-distance space. It maintains attributes that store the final cluster centers, labels for each data point, the inertia (a measure of how tightly the clusters are packed), and the number of iterations run.\n\nThe KMeans class is called by the k_means function, which serves as a convenient interface for users to perform clustering without directly interacting with the class. The k_means function initializes an instance of KMeans with the specified parameters and calls its fit method to perform the clustering. The results, including the final centroids, labels, and inertia, are returned to the user.\n\n**Note**: When using the KMeans class, it is essential to ensure that the input data meets the specified constraints, particularly regarding the number of samples and clusters. Users should also be aware of the initialization methods and their implications on the clustering results, as well as the potential for the algorithm to converge to local minima.\n\n**Output Example**: A possible return value from the fit method could be:\n```\ncluster_centers_: array([[10.,  2.],\n                          [ 1.,  2.]])\nlabels_: array([1, 1, 1, 0, 0, 0], dtype=int32)\ninertia_: 16.0\nn_iter_: 5\n``` \nThis output indicates the coordinates of the cluster centers, the cluster assignment for each sample in the input data, the final inertia value, and the number of iterations taken to converge."
      ],
      "code_start_line": 1213,
      "code_end_line": 1580,
      "params": [],
      "have_return": true,
      "code_content": "class KMeans(_BaseKMeans):\n    \"\"\"K-Means clustering.\n\n    Read more in the :ref:`User Guide <k_means>`.\n\n    Parameters\n    ----------\n\n    n_clusters : int, default=8\n        The number of clusters to form as well as the number of\n        centroids to generate.\n\n        For an example of how to choose an optimal value for `n_clusters` refer to\n        :ref:`sphx_glr_auto_examples_cluster_plot_kmeans_silhouette_analysis.py`.\n\n    init : {'k-means++', 'random'}, callable or array-like of shape \\\n            (n_clusters, n_features), default='k-means++'\n        Method for initialization:\n\n        * 'k-means++' : selects initial cluster centroids using sampling \\\n            based on an empirical probability distribution of the points' \\\n            contribution to the overall inertia. This technique speeds up \\\n            convergence. The algorithm implemented is \"greedy k-means++\". It \\\n            differs from the vanilla k-means++ by making several trials at \\\n            each sampling step and choosing the best centroid among them.\n\n        * 'random': choose `n_clusters` observations (rows) at random from \\\n        data for the initial centroids.\n\n        * If an array is passed, it should be of shape (n_clusters, n_features)\\\n        and gives the initial centers.\n\n        * If a callable is passed, it should take arguments X, n_clusters and a\\\n        random state and return an initialization.\n\n        For an example of how to use the different `init` strategy, see the example\n        entitled :ref:`sphx_glr_auto_examples_cluster_plot_kmeans_digits.py`.\n\n    n_init : 'auto' or int, default='auto'\n        Number of times the k-means algorithm is run with different centroid\n        seeds. The final results is the best output of `n_init` consecutive runs\n        in terms of inertia. Several runs are recommended for sparse\n        high-dimensional problems (see :ref:`kmeans_sparse_high_dim`).\n\n        When `n_init='auto'`, the number of runs depends on the value of init:\n        10 if using `init='random'` or `init` is a callable;\n        1 if using `init='k-means++'` or `init` is an array-like.\n\n        .. versionadded:: 1.2\n           Added 'auto' option for `n_init`.\n\n        .. versionchanged:: 1.4\n           Default value for `n_init` changed to `'auto'`.\n\n    max_iter : int, default=300\n        Maximum number of iterations of the k-means algorithm for a\n        single run.\n\n    tol : float, default=1e-4\n        Relative tolerance with regards to Frobenius norm of the difference\n        in the cluster centers of two consecutive iterations to declare\n        convergence.\n\n    verbose : int, default=0\n        Verbosity mode.\n\n    random_state : int, RandomState instance or None, default=None\n        Determines random number generation for centroid initialization. Use\n        an int to make the randomness deterministic.\n        See :term:`Glossary <random_state>`.\n\n    copy_x : bool, default=True\n        When pre-computing distances it is more numerically accurate to center\n        the data first. If copy_x is True (default), then the original data is\n        not modified. If False, the original data is modified, and put back\n        before the function returns, but small numerical differences may be\n        introduced by subtracting and then adding the data mean. Note that if\n        the original data is not C-contiguous, a copy will be made even if\n        copy_x is False. If the original data is sparse, but not in CSR format,\n        a copy will be made even if copy_x is False.\n\n    algorithm : {\"lloyd\", \"elkan\"}, default=\"lloyd\"\n        K-means algorithm to use. The classical EM-style algorithm is `\"lloyd\"`.\n        The `\"elkan\"` variation can be more efficient on some datasets with\n        well-defined clusters, by using the triangle inequality. However it's\n        more memory intensive due to the allocation of an extra array of shape\n        `(n_samples, n_clusters)`.\n\n        .. versionchanged:: 0.18\n            Added Elkan algorithm\n\n        .. versionchanged:: 1.1\n            Renamed \"full\" to \"lloyd\", and deprecated \"auto\" and \"full\".\n            Changed \"auto\" to use \"lloyd\" instead of \"elkan\".\n\n    Attributes\n    ----------\n    cluster_centers_ : ndarray of shape (n_clusters, n_features)\n        Coordinates of cluster centers. If the algorithm stops before fully\n        converging (see ``tol`` and ``max_iter``), these will not be\n        consistent with ``labels_``.\n\n    labels_ : ndarray of shape (n_samples,)\n        Labels of each point\n\n    inertia_ : float\n        Sum of squared distances of samples to their closest cluster center,\n        weighted by the sample weights if provided.\n\n    n_iter_ : int\n        Number of iterations run.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    MiniBatchKMeans : Alternative online implementation that does incremental\n        updates of the centers positions using mini-batches.\n        For large scale learning (say n_samples > 10k) MiniBatchKMeans is\n        probably much faster than the default batch implementation.\n\n    Notes\n    -----\n    The k-means problem is solved using either Lloyd's or Elkan's algorithm.\n\n    The average complexity is given by O(k n T), where n is the number of\n    samples and T is the number of iteration.\n\n    The worst case complexity is given by O(n^(k+2/p)) with\n    n = n_samples, p = n_features.\n    Refer to :doi:`\"How slow is the k-means method?\" D. Arthur and S. Vassilvitskii -\n    SoCG2006.<10.1145/1137856.1137880>` for more details.\n\n    In practice, the k-means algorithm is very fast (one of the fastest\n    clustering algorithms available), but it falls in local minima. That's why\n    it can be useful to restart it several times.\n\n    If the algorithm stops before fully converging (because of ``tol`` or\n    ``max_iter``), ``labels_`` and ``cluster_centers_`` will not be consistent,\n    i.e. the ``cluster_centers_`` will not be the means of the points in each\n    cluster. Also, the estimator will reassign ``labels_`` after the last\n    iteration to make ``labels_`` consistent with ``predict`` on the training\n    set.\n\n    Examples\n    --------\n\n    >>> from sklearn.cluster import KMeans\n    >>> import numpy as np\n    >>> X = np.array([[1, 2], [1, 4], [1, 0],\n    ...               [10, 2], [10, 4], [10, 0]])\n    >>> kmeans = KMeans(n_clusters=2, random_state=0, n_init=\"auto\").fit(X)\n    >>> kmeans.labels_\n    array([1, 1, 1, 0, 0, 0], dtype=int32)\n    >>> kmeans.predict([[0, 0], [12, 3]])\n    array([1, 0], dtype=int32)\n    >>> kmeans.cluster_centers_\n    array([[10.,  2.],\n           [ 1.,  2.]])\n\n    For a more detailed example of K-Means using the iris dataset see\n    :ref:`sphx_glr_auto_examples_cluster_plot_cluster_iris.py`.\n\n    For examples of common problems with K-Means and how to address them see\n    :ref:`sphx_glr_auto_examples_cluster_plot_kmeans_assumptions.py`.\n\n    For an example of how to use K-Means to perform color quantization see\n    :ref:`sphx_glr_auto_examples_cluster_plot_color_quantization.py`.\n\n    For a demonstration of how K-Means can be used to cluster text documents see\n    :ref:`sphx_glr_auto_examples_text_plot_document_clustering.py`.\n\n    For a comparison between K-Means and MiniBatchKMeans refer to example\n    :ref:`sphx_glr_auto_examples_cluster_plot_mini_batch_kmeans.py`.\n    \"\"\"\n\n    _parameter_constraints: dict = {\n        **_BaseKMeans._parameter_constraints,\n        \"copy_x\": [\"boolean\"],\n        \"algorithm\": [StrOptions({\"lloyd\", \"elkan\"})],\n    }\n\n    def __init__(\n        self,\n        n_clusters=8,\n        *,\n        init=\"k-means++\",\n        n_init=\"auto\",\n        max_iter=300,\n        tol=1e-4,\n        verbose=0,\n        random_state=None,\n        copy_x=True,\n        algorithm=\"lloyd\",\n    ):\n        super().__init__(\n            n_clusters=n_clusters,\n            init=init,\n            n_init=n_init,\n            max_iter=max_iter,\n            tol=tol,\n            verbose=verbose,\n            random_state=random_state,\n        )\n\n        self.copy_x = copy_x\n        self.algorithm = algorithm\n\n    def _check_params_vs_input(self, X):\n        super()._check_params_vs_input(X, default_n_init=10)\n\n        self._algorithm = self.algorithm\n        if self._algorithm == \"elkan\" and self.n_clusters == 1:\n            warnings.warn(\n                (\n                    \"algorithm='elkan' doesn't make sense for a single \"\n                    \"cluster. Using 'lloyd' instead.\"\n                ),\n                RuntimeWarning,\n            )\n            self._algorithm = \"lloyd\"\n\n    def _warn_mkl_vcomp(self, n_active_threads):\n        \"\"\"Warn when vcomp and mkl are both present\"\"\"\n        warnings.warn(\n            \"KMeans is known to have a memory leak on Windows \"\n            \"with MKL, when there are less chunks than available \"\n            \"threads. You can avoid it by setting the environment\"\n            f\" variable OMP_NUM_THREADS={n_active_threads}.\"\n        )\n\n    @_fit_context(prefer_skip_nested_validation=True)\n    def fit(self, X, y=None, sample_weight=None):\n        \"\"\"Compute k-means clustering.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Training instances to cluster. It must be noted that the data\n            will be converted to C ordering, which will cause a memory\n            copy if the given data is not C-contiguous.\n            If a sparse matrix is passed, a copy will be made if it's not in\n            CSR format.\n\n        y : Ignored\n            Not used, present here for API consistency by convention.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            The weights for each observation in X. If None, all observations\n            are assigned equal weight. `sample_weight` is not used during\n            initialization if `init` is a callable or a user provided array.\n\n            .. versionadded:: 0.20\n\n        Returns\n        -------\n        self : object\n            Fitted estimator.\n        \"\"\"\n        X = self._validate_data(\n            X,\n            accept_sparse=\"csr\",\n            dtype=[np.float64, np.float32],\n            order=\"C\",\n            copy=self.copy_x,\n            accept_large_sparse=False,\n        )\n\n        self._check_params_vs_input(X)\n\n        random_state = check_random_state(self.random_state)\n        sample_weight = _check_sample_weight(sample_weight, X, dtype=X.dtype)\n        self._n_threads = _openmp_effective_n_threads()\n\n        # Validate init array\n        init = self.init\n        init_is_array_like = _is_arraylike_not_scalar(init)\n        if init_is_array_like:\n            init = check_array(init, dtype=X.dtype, copy=True, order=\"C\")\n            self._validate_center_shape(X, init)\n\n        # subtract of mean of x for more accurate distance computations\n        if not sp.issparse(X):\n            X_mean = X.mean(axis=0)\n            # The copy was already done above\n            X -= X_mean\n\n            if init_is_array_like:\n                init -= X_mean\n\n        # precompute squared norms of data points\n        x_squared_norms = row_norms(X, squared=True)\n\n        if self._algorithm == \"elkan\":\n            kmeans_single = _kmeans_single_elkan\n        else:\n            kmeans_single = _kmeans_single_lloyd\n            self._check_mkl_vcomp(X, X.shape[0])\n\n        best_inertia, best_labels = None, None\n\n        for i in range(self._n_init):\n            # Initialize centers\n            centers_init = self._init_centroids(\n                X,\n                x_squared_norms=x_squared_norms,\n                init=init,\n                random_state=random_state,\n                sample_weight=sample_weight,\n            )\n            if self.verbose:\n                print(\"Initialization complete\")\n\n            # run a k-means once\n            labels, inertia, centers, n_iter_ = kmeans_single(\n                X,\n                sample_weight,\n                centers_init,\n                max_iter=self.max_iter,\n                verbose=self.verbose,\n                tol=self._tol,\n                n_threads=self._n_threads,\n            )\n\n            # determine if these results are the best so far\n            # we chose a new run if it has a better inertia and the clustering is\n            # different from the best so far (it's possible that the inertia is\n            # slightly better even if the clustering is the same with potentially\n            # permuted labels, due to rounding errors)\n            if best_inertia is None or (\n                inertia < best_inertia\n                and not _is_same_clustering(labels, best_labels, self.n_clusters)\n            ):\n                best_labels = labels\n                best_centers = centers\n                best_inertia = inertia\n                best_n_iter = n_iter_\n\n        if not sp.issparse(X):\n            if not self.copy_x:\n                X += X_mean\n            best_centers += X_mean\n\n        distinct_clusters = len(set(best_labels))\n        if distinct_clusters < self.n_clusters:\n            warnings.warn(\n                \"Number of distinct clusters ({}) found smaller than \"\n                \"n_clusters ({}). Possibly due to duplicate points \"\n                \"in X.\".format(distinct_clusters, self.n_clusters),\n                ConvergenceWarning,\n                stacklevel=2,\n            )\n\n        self.cluster_centers_ = best_centers\n        self._n_features_out = self.cluster_centers_.shape[0]\n        self.labels_ = best_labels\n        self.inertia_ = best_inertia\n        self.n_iter_ = best_n_iter\n        return self\n",
      "name_column": 6,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "dataset/_kmeans.py/k_means"
      ],
      "reference_who": [
        "dataset/_kmeans.py/_BaseKMeans"
      ],
      "special_reference_type": [
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [
        "**__init__**: The function of __init__ is to initialize an instance of the KMeans class with specified parameters for clustering.\n\n**parameters**: The parameters of this Function.\n· n_clusters: The number of clusters to form as well as the number of centroids to generate. Default is 8.  \n· init: Method for initialization. Default is \"k-means++\", which helps in selecting initial cluster centers in a smart way to speed up convergence.  \n· n_init: Number of times the k-means algorithm will be run with different centroid seeds. Default is \"auto\".  \n· max_iter: Maximum number of iterations of the k-means algorithm for a single run. Default is 300.  \n· tol: Relative tolerance with regards to inertia to declare convergence. Default is 1e-4.  \n· verbose: Verbosity mode. Default is 0, which means no output.  \n· random_state: Determines random number generation for centroid initialization. Use an int for reproducibility. Default is None.  \n· copy_x: If True, the original data will be copied; else, it may be overwritten. Default is True.  \n· algorithm: K-means algorithm to use. Default is \"lloyd\", which is the standard algorithm.\n\n**Code Description**: The __init__ function serves as the constructor for the KMeans class, allowing users to create an instance of the KMeans clustering algorithm with customizable parameters. The function begins by calling the constructor of the parent class using `super().__init__`, passing essential parameters such as `n_clusters`, `init`, `n_init`, `max_iter`, `tol`, `verbose`, and `random_state`. This ensures that the base class is properly initialized with these values. The function then sets additional attributes specific to the KMeans class: `copy_x`, which determines whether to copy the input data or not, and `algorithm`, which specifies the algorithm to be used for clustering. This initialization process is crucial for configuring the behavior of the KMeans instance according to the user's requirements.\n\n**Note**: It is important to choose the number of clusters (`n_clusters`) wisely, as it significantly affects the clustering results. Additionally, setting `random_state` can help in achieving reproducible results when running the algorithm multiple times."
      ],
      "code_start_line": 1404,
      "code_end_line": 1428,
      "params": [
        "self",
        "n_clusters"
      ],
      "have_return": false,
      "code_content": "    def __init__(\n        self,\n        n_clusters=8,\n        *,\n        init=\"k-means++\",\n        n_init=\"auto\",\n        max_iter=300,\n        tol=1e-4,\n        verbose=0,\n        random_state=None,\n        copy_x=True,\n        algorithm=\"lloyd\",\n    ):\n        super().__init__(\n            n_clusters=n_clusters,\n            init=init,\n            n_init=n_init,\n            max_iter=max_iter,\n            tol=tol,\n            verbose=verbose,\n            random_state=random_state,\n        )\n\n        self.copy_x = copy_x\n        self.algorithm = algorithm\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "_check_params_vs_input",
      "md_content": [
        "**_check_params_vs_input**: The function of _check_params_vs_input is to validate the input parameters against the expected configuration for the KMeans clustering algorithm.\n\n**parameters**: The parameters of this Function.\n· X: {array-like, sparse matrix} of shape (n_samples, n_features) - The input data to be validated, which contains the training instances for clustering.\n\n**Code Description**: The _check_params_vs_input function is responsible for ensuring that the input data X is compatible with the parameters set for the KMeans algorithm. It first calls the superclass method to perform initial checks on the input data, passing a default value for n_init (number of initializations) set to 10. This ensures that the input data adheres to the expected format and structure required by the KMeans algorithm.\n\nFollowing the superclass validation, the function checks the algorithm specified for clustering. If the algorithm is set to \"elkan\" and the number of clusters (n_clusters) is equal to 1, a warning is issued. This warning indicates that using the \"elkan\" algorithm is not appropriate for a single cluster scenario, and the algorithm is automatically switched to \"lloyd\". This adjustment is crucial because the \"elkan\" algorithm is optimized for scenarios with multiple clusters and may not function correctly with only one cluster.\n\nThe _check_params_vs_input function is called within the fit method of the KMeans class. The fit method is responsible for computing the k-means clustering by validating the input data, initializing centroids, and running the clustering algorithm. By validating the parameters against the input data, _check_params_vs_input ensures that the clustering process is set up correctly before any computations are performed, thus preventing potential errors during execution.\n\n**Note**: It is important to ensure that the input data X is in the correct format and structure before calling the fit method, as this will directly affect the performance and outcome of the clustering process. Additionally, users should be aware of the implications of using different algorithms based on the number of clusters specified."
      ],
      "code_start_line": 1430,
      "code_end_line": 1442,
      "params": [
        "self",
        "X"
      ],
      "have_return": false,
      "code_content": "    def _check_params_vs_input(self, X):\n        super()._check_params_vs_input(X, default_n_init=10)\n\n        self._algorithm = self.algorithm\n        if self._algorithm == \"elkan\" and self.n_clusters == 1:\n            warnings.warn(\n                (\n                    \"algorithm='elkan' doesn't make sense for a single \"\n                    \"cluster. Using 'lloyd' instead.\"\n                ),\n                RuntimeWarning,\n            )\n            self._algorithm = \"lloyd\"\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "dataset/_kmeans.py/KMeans/fit"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "_warn_mkl_vcomp",
      "md_content": [
        "**_warn_mkl_vcomp**: The function of _warn_mkl_vcomp is to issue a warning regarding potential memory leaks when using KMeans with MKL on Windows.\n\n**parameters**: The parameters of this Function.\n· n_active_threads: An integer representing the number of active threads that are available for processing.\n\n**Code Description**: The _warn_mkl_vcomp function is designed to alert users when both the Intel Math Kernel Library (MKL) and the vcomp library are present in the environment while using the KMeans algorithm. This situation can lead to a memory leak on Windows systems if the number of chunks processed is less than the number of available threads. The function takes a single parameter, n_active_threads, which indicates how many threads are actively being utilized. When invoked, the function generates a warning message that informs the user of the potential issue and provides a recommendation to set the environment variable OMP_NUM_THREADS to the value of n_active_threads. This adjustment is suggested as a means to mitigate the risk of memory leaks during the execution of the KMeans algorithm.\n\n**Note**: It is important for users to heed the warning generated by this function, especially when running KMeans on Windows with MKL. Proper configuration of the OMP_NUM_THREADS environment variable can help prevent performance degradation and memory-related issues."
      ],
      "code_start_line": 1444,
      "code_end_line": 1451,
      "params": [
        "self",
        "n_active_threads"
      ],
      "have_return": false,
      "code_content": "    def _warn_mkl_vcomp(self, n_active_threads):\n        \"\"\"Warn when vcomp and mkl are both present\"\"\"\n        warnings.warn(\n            \"KMeans is known to have a memory leak on Windows \"\n            \"with MKL, when there are less chunks than available \"\n            \"threads. You can avoid it by setting the environment\"\n            f\" variable OMP_NUM_THREADS={n_active_threads}.\"\n        )\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "fit",
      "md_content": [
        "**fit**: The function of fit is to compute k-means clustering.\n\n**parameters**: The parameters of this Function.\n· X: {array-like, sparse matrix} of shape (n_samples, n_features) - Training instances to cluster. The data will be converted to C ordering, which may cause a memory copy if the data is not C-contiguous. If a sparse matrix is passed, a copy will be made if it's not in CSR format.\n· y: Ignored - Not used, present here for API consistency by convention.\n· sample_weight: array-like of shape (n_samples,), default=None - The weights for each observation in X. If None, all observations are assigned equal weight. `sample_weight` is not used during initialization if `init` is a callable or a user-provided array.\n\n**Code Description**: The fit method is responsible for executing the k-means clustering algorithm. It begins by validating the input data X using the _validate_data method, ensuring that the data is in an acceptable format and type. The method checks for the compatibility of the input data with the parameters set for the KMeans algorithm through the _check_params_vs_input method.\n\nOnce the data is validated, the method initializes the random state for reproducibility and checks the sample weights using the _check_sample_weight function. It also determines the number of effective threads for computation.\n\nThe method then validates the initial cluster centers if provided, ensuring they conform to the expected shape using the _validate_center_shape method. If the input data is dense, it subtracts the mean of X to enhance the accuracy of distance computations.\n\nThe fit method precomputes the squared norms of the data points and selects the appropriate k-means algorithm (either Elkan or Lloyd) based on the specified parameters. It then enters a loop to perform multiple initializations of the cluster centers, running the k-means algorithm for each initialization.\n\nDuring each iteration, the method initializes the cluster centers and calls the selected k-means function (_kmeans_single_elkan or _kmeans_single_lloyd) to perform the clustering. After each run, it evaluates the results based on inertia and updates the best labels and centers if the current run yields better results.\n\nFinally, the method checks for distinct clusters and raises a warning if the number of distinct clusters found is smaller than the specified number of clusters. It assigns the best cluster centers, labels, inertia, and the number of iterations to the respective attributes of the KMeans instance and returns the fitted estimator.\n\nThe fit method is called by the k_means function, which serves as a high-level interface for performing k-means clustering. The k_means function initializes a KMeans instance and calls its fit method to execute the clustering process.\n\n**Note**: It is essential to ensure that the input data X is properly formatted and that the sample weights are correctly specified. The choice of initialization method and the number of initializations can significantly impact the performance and outcome of the k-means clustering process.\n\n**Output Example**: A possible return value from the fit method could be:\n```python\nKMeans(n_clusters=3, init='k-means++', n_init=10, max_iter=300, random_state=42)\n```\nThis output indicates that the KMeans instance has been fitted with the specified parameters, ready for further analysis or predictions."
      ],
      "code_start_line": 1454,
      "code_end_line": 1580,
      "params": [
        "self",
        "X",
        "y",
        "sample_weight"
      ],
      "have_return": true,
      "code_content": "    def fit(self, X, y=None, sample_weight=None):\n        \"\"\"Compute k-means clustering.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Training instances to cluster. It must be noted that the data\n            will be converted to C ordering, which will cause a memory\n            copy if the given data is not C-contiguous.\n            If a sparse matrix is passed, a copy will be made if it's not in\n            CSR format.\n\n        y : Ignored\n            Not used, present here for API consistency by convention.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            The weights for each observation in X. If None, all observations\n            are assigned equal weight. `sample_weight` is not used during\n            initialization if `init` is a callable or a user provided array.\n\n            .. versionadded:: 0.20\n\n        Returns\n        -------\n        self : object\n            Fitted estimator.\n        \"\"\"\n        X = self._validate_data(\n            X,\n            accept_sparse=\"csr\",\n            dtype=[np.float64, np.float32],\n            order=\"C\",\n            copy=self.copy_x,\n            accept_large_sparse=False,\n        )\n\n        self._check_params_vs_input(X)\n\n        random_state = check_random_state(self.random_state)\n        sample_weight = _check_sample_weight(sample_weight, X, dtype=X.dtype)\n        self._n_threads = _openmp_effective_n_threads()\n\n        # Validate init array\n        init = self.init\n        init_is_array_like = _is_arraylike_not_scalar(init)\n        if init_is_array_like:\n            init = check_array(init, dtype=X.dtype, copy=True, order=\"C\")\n            self._validate_center_shape(X, init)\n\n        # subtract of mean of x for more accurate distance computations\n        if not sp.issparse(X):\n            X_mean = X.mean(axis=0)\n            # The copy was already done above\n            X -= X_mean\n\n            if init_is_array_like:\n                init -= X_mean\n\n        # precompute squared norms of data points\n        x_squared_norms = row_norms(X, squared=True)\n\n        if self._algorithm == \"elkan\":\n            kmeans_single = _kmeans_single_elkan\n        else:\n            kmeans_single = _kmeans_single_lloyd\n            self._check_mkl_vcomp(X, X.shape[0])\n\n        best_inertia, best_labels = None, None\n\n        for i in range(self._n_init):\n            # Initialize centers\n            centers_init = self._init_centroids(\n                X,\n                x_squared_norms=x_squared_norms,\n                init=init,\n                random_state=random_state,\n                sample_weight=sample_weight,\n            )\n            if self.verbose:\n                print(\"Initialization complete\")\n\n            # run a k-means once\n            labels, inertia, centers, n_iter_ = kmeans_single(\n                X,\n                sample_weight,\n                centers_init,\n                max_iter=self.max_iter,\n                verbose=self.verbose,\n                tol=self._tol,\n                n_threads=self._n_threads,\n            )\n\n            # determine if these results are the best so far\n            # we chose a new run if it has a better inertia and the clustering is\n            # different from the best so far (it's possible that the inertia is\n            # slightly better even if the clustering is the same with potentially\n            # permuted labels, due to rounding errors)\n            if best_inertia is None or (\n                inertia < best_inertia\n                and not _is_same_clustering(labels, best_labels, self.n_clusters)\n            ):\n                best_labels = labels\n                best_centers = centers\n                best_inertia = inertia\n                best_n_iter = n_iter_\n\n        if not sp.issparse(X):\n            if not self.copy_x:\n                X += X_mean\n            best_centers += X_mean\n\n        distinct_clusters = len(set(best_labels))\n        if distinct_clusters < self.n_clusters:\n            warnings.warn(\n                \"Number of distinct clusters ({}) found smaller than \"\n                \"n_clusters ({}). Possibly due to duplicate points \"\n                \"in X.\".format(distinct_clusters, self.n_clusters),\n                ConvergenceWarning,\n                stacklevel=2,\n            )\n\n        self.cluster_centers_ = best_centers\n        self._n_features_out = self.cluster_centers_.shape[0]\n        self.labels_ = best_labels\n        self.inertia_ = best_inertia\n        self.n_iter_ = best_n_iter\n        return self\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "dataset/_kmeans.py/k_means"
      ],
      "reference_who": [
        "dataset/_kmeans.py/_kmeans_single_elkan",
        "dataset/_kmeans.py/_kmeans_single_lloyd",
        "dataset/_kmeans.py/_BaseKMeans/_check_mkl_vcomp",
        "dataset/_kmeans.py/_BaseKMeans/_validate_center_shape",
        "dataset/_kmeans.py/_BaseKMeans/_init_centroids",
        "dataset/_kmeans.py/KMeans/_check_params_vs_input"
      ],
      "special_reference_type": [
        false,
        false,
        false,
        false,
        false,
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "_mini_batch_step",
      "md_content": [
        "**_mini_batch_step**: The function of _mini_batch_step is to perform an incremental update of the centers for the Minibatch K-Means algorithm.\n\n**parameters**: The parameters of this Function.\n· X: {ndarray, sparse matrix} of shape (n_samples, n_features) - The original data array. If sparse, must be in CSR format.  \n· sample_weight: ndarray of shape (n_samples,) - The weights for each observation in `X`.  \n· centers: ndarray of shape (n_clusters, n_features) - The cluster centers before the current iteration.  \n· centers_new: ndarray of shape (n_clusters, n_features) - The cluster centers after the current iteration. Modified in-place.  \n· weight_sums: ndarray of shape (n_clusters,) - The vector in which we keep track of the numbers of points in a cluster. This array is modified in place.  \n· random_state: RandomState instance - Determines random number generation for low count centers reassignment.  \n· random_reassign: boolean, default=False - If True, centers with very low counts are randomly reassigned to observations.  \n· reassignment_ratio: float, default=0.01 - Control the fraction of the maximum number of counts for a center to be reassigned.  \n· verbose: bool, default=False - Controls the verbosity of the output.  \n· n_threads: int, default=1 - The number of OpenMP threads to use for the computation.\n\n**Code Description**: The _mini_batch_step function is a key component of the Minibatch K-Means clustering algorithm, designed to efficiently update cluster centers based on a mini-batch of data. The function begins by assigning labels to the input samples (X) based on their proximity to the current cluster centers using the _labels_inertia function. This function computes both the labels and the inertia, which is the sum of squared distances from each sample to its nearest cluster center.\n\nAfter label assignment, the function updates the cluster centers. Depending on whether the input data is sparse or dense, it calls either _minibatch_update_sparse or _minibatch_update_dense to perform the update. This ensures that the algorithm can handle different data formats efficiently.\n\nThe function also includes a mechanism for reassignment of cluster centers that have very low counts. If the random_reassign parameter is set to True, centers with counts below a certain threshold (defined by the reassignment_ratio) are randomly reassigned to new observations from the dataset. This is intended to improve the clustering quality by preventing centers from becoming stagnant.\n\nThe _mini_batch_step function is called by the fit and partial_fit methods of the MiniBatchKMeans class. In these methods, it is used to iteratively update the cluster centers as new mini-batches of data are processed. This iterative approach allows the algorithm to converge more quickly and efficiently on the optimal cluster centers.\n\n**Note**: It is important to ensure that the input data X is in the correct format (ndarray or CSR sparse matrix) and that the sample weights are appropriately defined. The function is optimized for performance with the option to utilize multiple threads for computation.\n\n**Output Example**: A possible return value of the function when called with a dataset of 5 samples and 3 clusters might look like this:  \nInertia: 12.34"
      ],
      "code_start_line": 1583,
      "code_end_line": 1701,
      "params": [
        "X",
        "sample_weight",
        "centers",
        "centers_new",
        "weight_sums",
        "random_state",
        "random_reassign",
        "reassignment_ratio",
        "verbose",
        "n_threads"
      ],
      "have_return": true,
      "code_content": "def _mini_batch_step(\n    X,\n    sample_weight,\n    centers,\n    centers_new,\n    weight_sums,\n    random_state,\n    random_reassign=False,\n    reassignment_ratio=0.01,\n    verbose=False,\n    n_threads=1,\n):\n    \"\"\"Incremental update of the centers for the Minibatch K-Means algorithm.\n\n    Parameters\n    ----------\n\n    X : {ndarray, sparse matrix} of shape (n_samples, n_features)\n        The original data array. If sparse, must be in CSR format.\n\n    x_squared_norms : ndarray of shape (n_samples,)\n        Squared euclidean norm of each data point.\n\n    sample_weight : ndarray of shape (n_samples,)\n        The weights for each observation in `X`.\n\n    centers : ndarray of shape (n_clusters, n_features)\n        The cluster centers before the current iteration\n\n    centers_new : ndarray of shape (n_clusters, n_features)\n        The cluster centers after the current iteration. Modified in-place.\n\n    weight_sums : ndarray of shape (n_clusters,)\n        The vector in which we keep track of the numbers of points in a\n        cluster. This array is modified in place.\n\n    random_state : RandomState instance\n        Determines random number generation for low count centers reassignment.\n        See :term:`Glossary <random_state>`.\n\n    random_reassign : boolean, default=False\n        If True, centers with very low counts are randomly reassigned\n        to observations.\n\n    reassignment_ratio : float, default=0.01\n        Control the fraction of the maximum number of counts for a\n        center to be reassigned. A higher value means that low count\n        centers are more likely to be reassigned, which means that the\n        model will take longer to converge, but should converge in a\n        better clustering.\n\n    verbose : bool, default=False\n        Controls the verbosity.\n\n    n_threads : int, default=1\n        The number of OpenMP threads to use for the computation.\n\n    Returns\n    -------\n    inertia : float\n        Sum of squared distances of samples to their closest cluster center.\n        The inertia is computed after finding the labels and before updating\n        the centers.\n    \"\"\"\n    # Perform label assignment to nearest centers\n    # For better efficiency, it's better to run _mini_batch_step in a\n    # threadpool_limit context than using _labels_inertia_threadpool_limit here\n    labels, inertia = _labels_inertia(X, sample_weight, centers, n_threads=n_threads)\n\n    # Update centers according to the labels\n    if sp.issparse(X):\n        _minibatch_update_sparse(\n            X, sample_weight, centers, centers_new, weight_sums, labels, n_threads\n        )\n    else:\n        _minibatch_update_dense(\n            X,\n            sample_weight,\n            centers,\n            centers_new,\n            weight_sums,\n            labels,\n            n_threads,\n        )\n\n    # Reassign clusters that have very low weight\n    if random_reassign and reassignment_ratio > 0:\n        to_reassign = weight_sums < reassignment_ratio * weight_sums.max()\n\n        # pick at most .5 * batch_size samples as new centers\n        if to_reassign.sum() > 0.5 * X.shape[0]:\n            indices_dont_reassign = np.argsort(weight_sums)[int(0.5 * X.shape[0]) :]\n            to_reassign[indices_dont_reassign] = False\n        n_reassigns = to_reassign.sum()\n\n        if n_reassigns:\n            # Pick new clusters amongst observations with uniform probability\n            new_centers = random_state.choice(\n                X.shape[0], replace=False, size=n_reassigns\n            )\n            if verbose:\n                print(f\"[MiniBatchKMeans] Reassigning {n_reassigns} cluster centers.\")\n\n            if sp.issparse(X):\n                assign_rows_csr(\n                    X,\n                    new_centers.astype(np.intp, copy=False),\n                    np.where(to_reassign)[0].astype(np.intp, copy=False),\n                    centers_new,\n                )\n            else:\n                centers_new[to_reassign] = X[new_centers]\n\n        # reset counts of reassigned centers, but don't reset them too small\n        # to avoid instant reassignment. This is a pretty dirty hack as it\n        # also modifies the learning rates.\n        weight_sums[to_reassign] = np.min(weight_sums[~to_reassign])\n\n    return inertia\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "dataset/_kmeans.py/MiniBatchKMeans/fit",
        "dataset/_kmeans.py/MiniBatchKMeans/partial_fit"
      ],
      "reference_who": [
        "dataset/_kmeans.py/_labels_inertia"
      ],
      "special_reference_type": [
        false
      ]
    },
    {
      "type": "ClassDef",
      "name": "MiniBatchKMeans",
      "md_content": [
        "**MiniBatchKMeans**: The function of MiniBatchKMeans is to perform clustering on large datasets using a mini-batch approach to optimize the K-Means algorithm.\n\n**attributes**: The attributes of this Class.\n· n_clusters: The number of clusters to form as well as the number of centroids to generate.\n· init: Method for initialization of cluster centers, which can be 'k-means++', 'random', or a user-defined callable or array.\n· max_iter: Maximum number of iterations over the complete dataset before stopping.\n· batch_size: Size of the mini batches used in the optimization process.\n· verbose: Verbosity mode to control the level of output during the fitting process.\n· compute_labels: A boolean indicating whether to compute label assignments and inertia for the complete dataset after convergence.\n· random_state: Determines random number generation for centroid initialization and random reassignment.\n· tol: Control early stopping based on the relative center changes.\n· max_no_improvement: Control early stopping based on the consecutive number of mini batches that do not yield an improvement on the inertia.\n· init_size: Number of samples to randomly sample for speeding up the initialization.\n· n_init: Number of random initializations that are tried.\n· reassignment_ratio: Control the fraction of the maximum number of counts for a center to be reassigned.\n\n**Code Description**: The MiniBatchKMeans class is an implementation of the K-Means clustering algorithm optimized for large datasets by processing data in smaller batches. It inherits from the _BaseKMeans class, which provides foundational functionality and parameter validation. The class is designed to handle clustering tasks efficiently by minimizing memory usage and computational time.\n\nThe constructor initializes several parameters, including the number of clusters, initialization method, maximum iterations, batch size, and others. The class includes methods for fitting the model to the data, predicting cluster assignments, and updating the model incrementally with new data through the `partial_fit` method.\n\nThe `_check_params_vs_input` method validates the input parameters against the provided data, ensuring that the number of clusters does not exceed the number of samples and that the initialization method is appropriate. The `_mini_batch_convergence` method implements early stopping logic based on the convergence of the clustering process, allowing the algorithm to terminate when no significant improvements are observed.\n\nThe `fit` method computes the centroids on the input data by chunking it into mini-batches, while the `partial_fit` method allows for updating the K-Means estimate on a single mini-batch. The class also provides attributes to store the resulting cluster centers, labels, inertia, and the number of iterations and steps processed during fitting.\n\nOverall, MiniBatchKMeans is particularly useful for applications involving large datasets where traditional K-Means may be computationally expensive and memory-intensive.\n\n**Note**: When using the MiniBatchKMeans class, it is essential to ensure that the input data meets the specified constraints, particularly regarding the number of samples and clusters. Additionally, users should be aware of the initialization methods and their implications on the clustering results.\n\n**Output Example**: A possible return value from the `fit` method could be an object containing the fitted model, with attributes such as:\n```\ncluster_centers_: array([[3.55102041, 2.48979592],\n                          [1.06896552, 1.        ]])\nlabels_: array([1, 0, 0, 1, 0, 1], dtype=int32)\ninertia_: 5.123456789\nn_iter_: 5\nn_steps_: 10\n``` \nThis output indicates the cluster centers, the labels assigned to each sample, the inertia value, and the number of iterations and steps taken during the fitting process."
      ],
      "code_start_line": 1704,
      "code_end_line": 2318,
      "params": [],
      "have_return": true,
      "code_content": "class MiniBatchKMeans(_BaseKMeans):\n    \"\"\"\n    Mini-Batch K-Means clustering.\n\n    Read more in the :ref:`User Guide <mini_batch_kmeans>`.\n\n    Parameters\n    ----------\n\n    n_clusters : int, default=8\n        The number of clusters to form as well as the number of\n        centroids to generate.\n\n    init : {'k-means++', 'random'}, callable or array-like of shape \\\n            (n_clusters, n_features), default='k-means++'\n        Method for initialization:\n\n        'k-means++' : selects initial cluster centroids using sampling based on\n        an empirical probability distribution of the points' contribution to the\n        overall inertia. This technique speeds up convergence. The algorithm\n        implemented is \"greedy k-means++\". It differs from the vanilla k-means++\n        by making several trials at each sampling step and choosing the best centroid\n        among them.\n\n        'random': choose `n_clusters` observations (rows) at random from data\n        for the initial centroids.\n\n        If an array is passed, it should be of shape (n_clusters, n_features)\n        and gives the initial centers.\n\n        If a callable is passed, it should take arguments X, n_clusters and a\n        random state and return an initialization.\n\n    max_iter : int, default=100\n        Maximum number of iterations over the complete dataset before\n        stopping independently of any early stopping criterion heuristics.\n\n    batch_size : int, default=1024\n        Size of the mini batches.\n        For faster computations, you can set the ``batch_size`` greater than\n        256 * number of cores to enable parallelism on all cores.\n\n        .. versionchanged:: 1.0\n           `batch_size` default changed from 100 to 1024.\n\n    verbose : int, default=0\n        Verbosity mode.\n\n    compute_labels : bool, default=True\n        Compute label assignment and inertia for the complete dataset\n        once the minibatch optimization has converged in fit.\n\n    random_state : int, RandomState instance or None, default=None\n        Determines random number generation for centroid initialization and\n        random reassignment. Use an int to make the randomness deterministic.\n        See :term:`Glossary <random_state>`.\n\n    tol : float, default=0.0\n        Control early stopping based on the relative center changes as\n        measured by a smoothed, variance-normalized of the mean center\n        squared position changes. This early stopping heuristics is\n        closer to the one used for the batch variant of the algorithms\n        but induces a slight computational and memory overhead over the\n        inertia heuristic.\n\n        To disable convergence detection based on normalized center\n        change, set tol to 0.0 (default).\n\n    max_no_improvement : int, default=10\n        Control early stopping based on the consecutive number of mini\n        batches that does not yield an improvement on the smoothed inertia.\n\n        To disable convergence detection based on inertia, set\n        max_no_improvement to None.\n\n    init_size : int, default=None\n        Number of samples to randomly sample for speeding up the\n        initialization (sometimes at the expense of accuracy): the\n        only algorithm is initialized by running a batch KMeans on a\n        random subset of the data. This needs to be larger than n_clusters.\n\n        If `None`, the heuristic is `init_size = 3 * batch_size` if\n        `3 * batch_size < n_clusters`, else `init_size = 3 * n_clusters`.\n\n    n_init : 'auto' or int, default=\"auto\"\n        Number of random initializations that are tried.\n        In contrast to KMeans, the algorithm is only run once, using the best of\n        the `n_init` initializations as measured by inertia. Several runs are\n        recommended for sparse high-dimensional problems (see\n        :ref:`kmeans_sparse_high_dim`).\n\n        When `n_init='auto'`, the number of runs depends on the value of init:\n        3 if using `init='random'` or `init` is a callable;\n        1 if using `init='k-means++'` or `init` is an array-like.\n\n        .. versionadded:: 1.2\n           Added 'auto' option for `n_init`.\n\n        .. versionchanged:: 1.4\n           Default value for `n_init` changed to `'auto'` in version.\n\n    reassignment_ratio : float, default=0.01\n        Control the fraction of the maximum number of counts for a center to\n        be reassigned. A higher value means that low count centers are more\n        easily reassigned, which means that the model will take longer to\n        converge, but should converge in a better clustering. However, too high\n        a value may cause convergence issues, especially with a small batch\n        size.\n\n    Attributes\n    ----------\n\n    cluster_centers_ : ndarray of shape (n_clusters, n_features)\n        Coordinates of cluster centers.\n\n    labels_ : ndarray of shape (n_samples,)\n        Labels of each point (if compute_labels is set to True).\n\n    inertia_ : float\n        The value of the inertia criterion associated with the chosen\n        partition if compute_labels is set to True. If compute_labels is set to\n        False, it's an approximation of the inertia based on an exponentially\n        weighted average of the batch inertiae.\n        The inertia is defined as the sum of square distances of samples to\n        their cluster center, weighted by the sample weights if provided.\n\n    n_iter_ : int\n        Number of iterations over the full dataset.\n\n    n_steps_ : int\n        Number of minibatches processed.\n\n        .. versionadded:: 1.0\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    KMeans : The classic implementation of the clustering method based on the\n        Lloyd's algorithm. It consumes the whole set of input data at each\n        iteration.\n\n    Notes\n    -----\n    See https://www.eecs.tufts.edu/~dsculley/papers/fastkmeans.pdf\n\n    When there are too few points in the dataset, some centers may be\n    duplicated, which means that a proper clustering in terms of the number\n    of requesting clusters and the number of returned clusters will not\n    always match. One solution is to set `reassignment_ratio=0`, which\n    prevents reassignments of clusters that are too small.\n\n    Examples\n    --------\n    >>> from sklearn.cluster import MiniBatchKMeans\n    >>> import numpy as np\n    >>> X = np.array([[1, 2], [1, 4], [1, 0],\n    ...               [4, 2], [4, 0], [4, 4],\n    ...               [4, 5], [0, 1], [2, 2],\n    ...               [3, 2], [5, 5], [1, -1]])\n    >>> # manually fit on batches\n    >>> kmeans = MiniBatchKMeans(n_clusters=2,\n    ...                          random_state=0,\n    ...                          batch_size=6,\n    ...                          n_init=\"auto\")\n    >>> kmeans = kmeans.partial_fit(X[0:6,:])\n    >>> kmeans = kmeans.partial_fit(X[6:12,:])\n    >>> kmeans.cluster_centers_\n    array([[3.375, 3.  ],\n           [0.75 , 0.5 ]])\n    >>> kmeans.predict([[0, 0], [4, 4]])\n    array([1, 0], dtype=int32)\n    >>> # fit on the whole data\n    >>> kmeans = MiniBatchKMeans(n_clusters=2,\n    ...                          random_state=0,\n    ...                          batch_size=6,\n    ...                          max_iter=10,\n    ...                          n_init=\"auto\").fit(X)\n    >>> kmeans.cluster_centers_\n    array([[3.55102041, 2.48979592],\n           [1.06896552, 1.        ]])\n    >>> kmeans.predict([[0, 0], [4, 4]])\n    array([1, 0], dtype=int32)\n    \"\"\"\n\n    _parameter_constraints: dict = {\n        **_BaseKMeans._parameter_constraints,\n        \"batch_size\": [Interval(Integral, 1, None, closed=\"left\")],\n        \"compute_labels\": [\"boolean\"],\n        \"max_no_improvement\": [Interval(Integral, 0, None, closed=\"left\"), None],\n        \"init_size\": [Interval(Integral, 1, None, closed=\"left\"), None],\n        \"reassignment_ratio\": [Interval(Real, 0, None, closed=\"left\")],\n    }\n\n    def __init__(\n        self,\n        n_clusters=8,\n        *,\n        init=\"k-means++\",\n        max_iter=100,\n        batch_size=1024,\n        verbose=0,\n        compute_labels=True,\n        random_state=None,\n        tol=0.0,\n        max_no_improvement=10,\n        init_size=None,\n        n_init=\"auto\",\n        reassignment_ratio=0.01,\n    ):\n        super().__init__(\n            n_clusters=n_clusters,\n            init=init,\n            max_iter=max_iter,\n            verbose=verbose,\n            random_state=random_state,\n            tol=tol,\n            n_init=n_init,\n        )\n\n        self.max_no_improvement = max_no_improvement\n        self.batch_size = batch_size\n        self.compute_labels = compute_labels\n        self.init_size = init_size\n        self.reassignment_ratio = reassignment_ratio\n\n    def _check_params_vs_input(self, X):\n        super()._check_params_vs_input(X, default_n_init=3)\n\n        self._batch_size = min(self.batch_size, X.shape[0])\n\n        # init_size\n        self._init_size = self.init_size\n        if self._init_size is None:\n            self._init_size = 3 * self._batch_size\n            if self._init_size < self.n_clusters:\n                self._init_size = 3 * self.n_clusters\n        elif self._init_size < self.n_clusters:\n            warnings.warn(\n                (\n                    f\"init_size={self._init_size} should be larger than \"\n                    f\"n_clusters={self.n_clusters}. Setting it to \"\n                    \"min(3*n_clusters, n_samples)\"\n                ),\n                RuntimeWarning,\n                stacklevel=2,\n            )\n            self._init_size = 3 * self.n_clusters\n        self._init_size = min(self._init_size, X.shape[0])\n\n        # reassignment_ratio\n        if self.reassignment_ratio < 0:\n            raise ValueError(\n                \"reassignment_ratio should be >= 0, got \"\n                f\"{self.reassignment_ratio} instead.\"\n            )\n\n    def _warn_mkl_vcomp(self, n_active_threads):\n        \"\"\"Warn when vcomp and mkl are both present\"\"\"\n        warnings.warn(\n            \"MiniBatchKMeans is known to have a memory leak on \"\n            \"Windows with MKL, when there are less chunks than \"\n            \"available threads. You can prevent it by setting \"\n            f\"batch_size >= {self._n_threads * CHUNK_SIZE} or by \"\n            \"setting the environment variable \"\n            f\"OMP_NUM_THREADS={n_active_threads}\"\n        )\n\n    def _mini_batch_convergence(\n        self, step, n_steps, n_samples, centers_squared_diff, batch_inertia\n    ):\n        \"\"\"Helper function to encapsulate the early stopping logic\"\"\"\n        # Normalize inertia to be able to compare values when\n        # batch_size changes\n        batch_inertia /= self._batch_size\n\n        # count steps starting from 1 for user friendly verbose mode.\n        step = step + 1\n\n        # Ignore first iteration because it's inertia from initialization.\n        if step == 1:\n            if self.verbose:\n                print(\n                    f\"Minibatch step {step}/{n_steps}: mean batch \"\n                    f\"inertia: {batch_inertia}\"\n                )\n            return False\n\n        # Compute an Exponentially Weighted Average of the inertia to\n        # monitor the convergence while discarding minibatch-local stochastic\n        # variability: https://en.wikipedia.org/wiki/Moving_average\n        if self._ewa_inertia is None:\n            self._ewa_inertia = batch_inertia\n        else:\n            alpha = self._batch_size * 2.0 / (n_samples + 1)\n            alpha = min(alpha, 1)\n            self._ewa_inertia = self._ewa_inertia * (1 - alpha) + batch_inertia * alpha\n\n        # Log progress to be able to monitor convergence\n        if self.verbose:\n            print(\n                f\"Minibatch step {step}/{n_steps}: mean batch inertia: \"\n                f\"{batch_inertia}, ewa inertia: {self._ewa_inertia}\"\n            )\n\n        # Early stopping based on absolute tolerance on squared change of\n        # centers position\n        if self._tol > 0.0 and centers_squared_diff <= self._tol:\n            if self.verbose:\n                print(f\"Converged (small centers change) at step {step}/{n_steps}\")\n            return True\n\n        # Early stopping heuristic due to lack of improvement on smoothed\n        # inertia\n        if self._ewa_inertia_min is None or self._ewa_inertia < self._ewa_inertia_min:\n            self._no_improvement = 0\n            self._ewa_inertia_min = self._ewa_inertia\n        else:\n            self._no_improvement += 1\n\n        if (\n            self.max_no_improvement is not None\n            and self._no_improvement >= self.max_no_improvement\n        ):\n            if self.verbose:\n                print(\n                    \"Converged (lack of improvement in inertia) at step \"\n                    f\"{step}/{n_steps}\"\n                )\n            return True\n\n        return False\n\n    def _random_reassign(self):\n        \"\"\"Check if a random reassignment needs to be done.\n\n        Do random reassignments each time 10 * n_clusters samples have been\n        processed.\n\n        If there are empty clusters we always want to reassign.\n        \"\"\"\n        self._n_since_last_reassign += self._batch_size\n        if (self._counts == 0).any() or self._n_since_last_reassign >= (\n            10 * self.n_clusters\n        ):\n            self._n_since_last_reassign = 0\n            return True\n        return False\n\n    @_fit_context(prefer_skip_nested_validation=True)\n    def fit(self, X, y=None, sample_weight=None):\n        \"\"\"Compute the centroids on X by chunking it into mini-batches.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Training instances to cluster. It must be noted that the data\n            will be converted to C ordering, which will cause a memory copy\n            if the given data is not C-contiguous.\n            If a sparse matrix is passed, a copy will be made if it's not in\n            CSR format.\n\n        y : Ignored\n            Not used, present here for API consistency by convention.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            The weights for each observation in X. If None, all observations\n            are assigned equal weight. `sample_weight` is not used during\n            initialization if `init` is a callable or a user provided array.\n\n            .. versionadded:: 0.20\n\n        Returns\n        -------\n        self : object\n            Fitted estimator.\n        \"\"\"\n        X = self._validate_data(\n            X,\n            accept_sparse=\"csr\",\n            dtype=[np.float64, np.float32],\n            order=\"C\",\n            accept_large_sparse=False,\n        )\n\n        self._check_params_vs_input(X)\n        random_state = check_random_state(self.random_state)\n        sample_weight = _check_sample_weight(sample_weight, X, dtype=X.dtype)\n        self._n_threads = _openmp_effective_n_threads()\n        n_samples, n_features = X.shape\n\n        # Validate init array\n        init = self.init\n        if _is_arraylike_not_scalar(init):\n            init = check_array(init, dtype=X.dtype, copy=True, order=\"C\")\n            self._validate_center_shape(X, init)\n\n        self._check_mkl_vcomp(X, self._batch_size)\n\n        # precompute squared norms of data points\n        x_squared_norms = row_norms(X, squared=True)\n\n        # Validation set for the init\n        validation_indices = random_state.randint(0, n_samples, self._init_size)\n        X_valid = X[validation_indices]\n        sample_weight_valid = sample_weight[validation_indices]\n\n        # perform several inits with random subsets\n        best_inertia = None\n        for init_idx in range(self._n_init):\n            if self.verbose:\n                print(f\"Init {init_idx + 1}/{self._n_init} with method {init}\")\n\n            # Initialize the centers using only a fraction of the data as we\n            # expect n_samples to be very large when using MiniBatchKMeans.\n            cluster_centers = self._init_centroids(\n                X,\n                x_squared_norms=x_squared_norms,\n                init=init,\n                random_state=random_state,\n                init_size=self._init_size,\n                sample_weight=sample_weight,\n            )\n\n            # Compute inertia on a validation set.\n            _, inertia = _labels_inertia_threadpool_limit(\n                X_valid,\n                sample_weight_valid,\n                cluster_centers,\n                n_threads=self._n_threads,\n            )\n\n            if self.verbose:\n                print(f\"Inertia for init {init_idx + 1}/{self._n_init}: {inertia}\")\n            if best_inertia is None or inertia < best_inertia:\n                init_centers = cluster_centers\n                best_inertia = inertia\n\n        centers = init_centers\n        centers_new = np.empty_like(centers)\n\n        # Initialize counts\n        self._counts = np.zeros(self.n_clusters, dtype=X.dtype)\n\n        # Attributes to monitor the convergence\n        self._ewa_inertia = None\n        self._ewa_inertia_min = None\n        self._no_improvement = 0\n\n        # Initialize number of samples seen since last reassignment\n        self._n_since_last_reassign = 0\n\n        n_steps = (self.max_iter * n_samples) // self._batch_size\n\n        with threadpool_limits(limits=1, user_api=\"blas\"):\n            # Perform the iterative optimization until convergence\n            for i in range(n_steps):\n                # Sample a minibatch from the full dataset\n                minibatch_indices = random_state.randint(0, n_samples, self._batch_size)\n\n                # Perform the actual update step on the minibatch data\n                batch_inertia = _mini_batch_step(\n                    X=X[minibatch_indices],\n                    sample_weight=sample_weight[minibatch_indices],\n                    centers=centers,\n                    centers_new=centers_new,\n                    weight_sums=self._counts,\n                    random_state=random_state,\n                    random_reassign=self._random_reassign(),\n                    reassignment_ratio=self.reassignment_ratio,\n                    verbose=self.verbose,\n                    n_threads=self._n_threads,\n                )\n\n                if self._tol > 0.0:\n                    centers_squared_diff = np.sum((centers_new - centers) ** 2)\n                else:\n                    centers_squared_diff = 0\n\n                centers, centers_new = centers_new, centers\n\n                # Monitor convergence and do early stopping if necessary\n                if self._mini_batch_convergence(\n                    i, n_steps, n_samples, centers_squared_diff, batch_inertia\n                ):\n                    break\n\n        self.cluster_centers_ = centers\n        self._n_features_out = self.cluster_centers_.shape[0]\n\n        self.n_steps_ = i + 1\n        self.n_iter_ = int(np.ceil(((i + 1) * self._batch_size) / n_samples))\n\n        if self.compute_labels:\n            self.labels_, self.inertia_ = _labels_inertia_threadpool_limit(\n                X,\n                sample_weight,\n                self.cluster_centers_,\n                n_threads=self._n_threads,\n            )\n        else:\n            self.inertia_ = self._ewa_inertia * n_samples\n\n        return self\n\n    @_fit_context(prefer_skip_nested_validation=True)\n    def partial_fit(self, X, y=None, sample_weight=None):\n        \"\"\"Update k means estimate on a single mini-batch X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Training instances to cluster. It must be noted that the data\n            will be converted to C ordering, which will cause a memory copy\n            if the given data is not C-contiguous.\n            If a sparse matrix is passed, a copy will be made if it's not in\n            CSR format.\n\n        y : Ignored\n            Not used, present here for API consistency by convention.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            The weights for each observation in X. If None, all observations\n            are assigned equal weight. `sample_weight` is not used during\n            initialization if `init` is a callable or a user provided array.\n\n        Returns\n        -------\n        self : object\n            Return updated estimator.\n        \"\"\"\n        has_centers = hasattr(self, \"cluster_centers_\")\n\n        X = self._validate_data(\n            X,\n            accept_sparse=\"csr\",\n            dtype=[np.float64, np.float32],\n            order=\"C\",\n            accept_large_sparse=False,\n            reset=not has_centers,\n        )\n\n        self._random_state = getattr(\n            self, \"_random_state\", check_random_state(self.random_state)\n        )\n        sample_weight = _check_sample_weight(sample_weight, X, dtype=X.dtype)\n        self.n_steps_ = getattr(self, \"n_steps_\", 0)\n\n        # precompute squared norms of data points\n        x_squared_norms = row_norms(X, squared=True)\n\n        if not has_centers:\n            # this instance has not been fitted yet (fit or partial_fit)\n            self._check_params_vs_input(X)\n            self._n_threads = _openmp_effective_n_threads()\n\n            # Validate init array\n            init = self.init\n            if _is_arraylike_not_scalar(init):\n                init = check_array(init, dtype=X.dtype, copy=True, order=\"C\")\n                self._validate_center_shape(X, init)\n\n            self._check_mkl_vcomp(X, X.shape[0])\n\n            # initialize the cluster centers\n            self.cluster_centers_ = self._init_centroids(\n                X,\n                x_squared_norms=x_squared_norms,\n                init=init,\n                random_state=self._random_state,\n                init_size=self._init_size,\n                sample_weight=sample_weight,\n            )\n\n            # Initialize counts\n            self._counts = np.zeros(self.n_clusters, dtype=X.dtype)\n\n            # Initialize number of samples seen since last reassignment\n            self._n_since_last_reassign = 0\n\n        with threadpool_limits(limits=1, user_api=\"blas\"):\n            _mini_batch_step(\n                X,\n                sample_weight=sample_weight,\n                centers=self.cluster_centers_,\n                centers_new=self.cluster_centers_,\n                weight_sums=self._counts,\n                random_state=self._random_state,\n                random_reassign=self._random_reassign(),\n                reassignment_ratio=self.reassignment_ratio,\n                verbose=self.verbose,\n                n_threads=self._n_threads,\n            )\n\n        if self.compute_labels:\n            self.labels_, self.inertia_ = _labels_inertia_threadpool_limit(\n                X,\n                sample_weight,\n                self.cluster_centers_,\n                n_threads=self._n_threads,\n            )\n\n        self.n_steps_ += 1\n        self._n_features_out = self.cluster_centers_.shape[0]\n\n        return self\n",
      "name_column": 6,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "dataset/_kmeans.py/_BaseKMeans"
      ],
      "special_reference_type": [
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [
        "**__init__**: The function of __init__ is to initialize an instance of the MiniBatchKMeans class with specified parameters for clustering.\n\n**parameters**: The parameters of this Function.\n· n_clusters: The number of clusters to form, default is 8.  \n· init: Method for initialization, default is \"k-means++\".  \n· max_iter: Maximum number of iterations for a single run, default is 100.  \n· batch_size: Size of the mini-batches, default is 1024.  \n· verbose: Verbosity mode, default is 0 (no output).  \n· compute_labels: Whether to compute labels for the clusters, default is True.  \n· random_state: Seed for random number generation, default is None.  \n· tol: Tolerance for convergence, default is 0.0.  \n· max_no_improvement: Maximum number of iterations with no improvement before stopping, default is 10.  \n· init_size: Size of the initialization set, default is None.  \n· n_init: Number of time the k-means algorithm will be run with different centroid seeds, default is \"auto\".  \n· reassignment_ratio: The ratio of reassignment for the clusters, default is 0.01.  \n\n**Code Description**: The __init__ function is a constructor for the MiniBatchKMeans class, which is a variant of the KMeans clustering algorithm designed to handle large datasets efficiently by using mini-batches. The function accepts several parameters that allow users to customize the clustering process. The n_clusters parameter specifies how many clusters the algorithm should find. The init parameter determines the method used to initialize the cluster centers, with \"k-means++\" being a popular choice for better convergence. The max_iter parameter sets the upper limit on the number of iterations for the algorithm to run, ensuring that it does not run indefinitely. The batch_size parameter controls how many samples are processed in each iteration, which can significantly affect performance and memory usage.\n\nThe verbose parameter allows users to control the amount of information printed during the execution of the algorithm, which can be useful for debugging or monitoring progress. The compute_labels parameter indicates whether the algorithm should compute and return the labels of the clusters for each data point. The random_state parameter is used to seed the random number generator for reproducibility of results. The tol parameter sets the tolerance level for convergence, while max_no_improvement defines how many iterations without improvement are allowed before the algorithm stops. The init_size parameter can be specified to control the size of the initial sample used for initializing the cluster centers, and n_init determines how many times the algorithm will be run with different initializations to ensure a good solution. Finally, the reassignment_ratio parameter specifies the fraction of points that can be reassigned to different clusters during the clustering process.\n\n**Note**: It is important to choose the parameters wisely based on the dataset and the specific requirements of the clustering task. The default values are provided for convenience, but they may need to be adjusted for optimal performance in different scenarios."
      ],
      "code_start_line": 1907,
      "code_end_line": 1937,
      "params": [
        "self",
        "n_clusters"
      ],
      "have_return": false,
      "code_content": "    def __init__(\n        self,\n        n_clusters=8,\n        *,\n        init=\"k-means++\",\n        max_iter=100,\n        batch_size=1024,\n        verbose=0,\n        compute_labels=True,\n        random_state=None,\n        tol=0.0,\n        max_no_improvement=10,\n        init_size=None,\n        n_init=\"auto\",\n        reassignment_ratio=0.01,\n    ):\n        super().__init__(\n            n_clusters=n_clusters,\n            init=init,\n            max_iter=max_iter,\n            verbose=verbose,\n            random_state=random_state,\n            tol=tol,\n            n_init=n_init,\n        )\n\n        self.max_no_improvement = max_no_improvement\n        self.batch_size = batch_size\n        self.compute_labels = compute_labels\n        self.init_size = init_size\n        self.reassignment_ratio = reassignment_ratio\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "_check_params_vs_input",
      "md_content": [
        "**_check_params_vs_input**: The function of _check_params_vs_input is to validate and adjust parameters based on the input data provided for the MiniBatchKMeans clustering algorithm.\n\n**parameters**: The parameters of this Function.\n· X: {array-like, sparse matrix} of shape (n_samples, n_features) - The input data to be clustered, which must be validated for compatibility with the clustering algorithm.\n\n**Code Description**: The _check_params_vs_input function performs several critical checks and adjustments related to the parameters used in the MiniBatchKMeans algorithm. Initially, it calls the superclass method to ensure that the input data X meets the expected criteria, while also setting a default value for n_init if not specified. \n\nNext, the function determines the batch size by taking the minimum of the specified batch_size and the number of samples in X. This ensures that the batch size does not exceed the available data points. \n\nThe function then checks and sets the _init_size parameter, which dictates how many samples will be used to initialize the cluster centers. If _init_size is not provided, it defaults to three times the batch size, with a further check to ensure it is at least three times the number of clusters. If _init_size is less than the number of clusters, a warning is issued, and _init_size is adjusted accordingly. The function also ensures that _init_size does not exceed the number of samples in X.\n\nAdditionally, the function validates the reassignment_ratio parameter, raising a ValueError if it is negative, as this would be invalid for the clustering process.\n\nThis function is called within the fit and partial_fit methods of the MiniBatchKMeans class. In the fit method, it is invoked after validating the input data to ensure that all parameters are correctly set before proceeding with the clustering process. In the partial_fit method, it is called when initializing the cluster centers for the first time, ensuring that the parameters are appropriately configured for subsequent updates to the clustering model.\n\n**Note**: It is important to ensure that the input data X is in the correct format and that all parameters are set appropriately before calling the fit or partial_fit methods to avoid runtime errors and ensure optimal clustering performance."
      ],
      "code_start_line": 1939,
      "code_end_line": 1968,
      "params": [
        "self",
        "X"
      ],
      "have_return": false,
      "code_content": "    def _check_params_vs_input(self, X):\n        super()._check_params_vs_input(X, default_n_init=3)\n\n        self._batch_size = min(self.batch_size, X.shape[0])\n\n        # init_size\n        self._init_size = self.init_size\n        if self._init_size is None:\n            self._init_size = 3 * self._batch_size\n            if self._init_size < self.n_clusters:\n                self._init_size = 3 * self.n_clusters\n        elif self._init_size < self.n_clusters:\n            warnings.warn(\n                (\n                    f\"init_size={self._init_size} should be larger than \"\n                    f\"n_clusters={self.n_clusters}. Setting it to \"\n                    \"min(3*n_clusters, n_samples)\"\n                ),\n                RuntimeWarning,\n                stacklevel=2,\n            )\n            self._init_size = 3 * self.n_clusters\n        self._init_size = min(self._init_size, X.shape[0])\n\n        # reassignment_ratio\n        if self.reassignment_ratio < 0:\n            raise ValueError(\n                \"reassignment_ratio should be >= 0, got \"\n                f\"{self.reassignment_ratio} instead.\"\n            )\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "dataset/_kmeans.py/MiniBatchKMeans/fit",
        "dataset/_kmeans.py/MiniBatchKMeans/partial_fit"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "_warn_mkl_vcomp",
      "md_content": [
        "**_warn_mkl_vcomp**: The function of _warn_mkl_vcomp is to issue a warning regarding potential memory leaks when using MiniBatchKMeans with MKL on Windows.\n\n**parameters**: The parameters of this Function.\n· n_active_threads: An integer representing the number of active threads that are being utilized.\n\n**Code Description**: The _warn_mkl_vcomp function is designed to alert users when both the Intel Math Kernel Library (MKL) and the vcomp library are present in the environment. This situation can lead to a memory leak issue specifically on Windows systems when the number of chunks processed is less than the number of available threads. The function takes one parameter, n_active_threads, which indicates how many threads are actively being used during the execution of the MiniBatchKMeans algorithm. \n\nWhen the function is called, it triggers a warning message that informs the user about the potential memory leak. The warning suggests two possible solutions to mitigate the issue: either increase the batch size to be greater than or equal to the product of the number of threads and a predefined chunk size (self._n_threads * CHUNK_SIZE), or set the environment variable OMP_NUM_THREADS to the value of n_active_threads. This guidance helps users adjust their configurations to avoid performance degradation due to memory management issues.\n\n**Note**: It is important for users to heed this warning when using MiniBatchKMeans in environments where both MKL and vcomp are present, especially on Windows, to ensure optimal performance and prevent memory leaks."
      ],
      "code_start_line": 1970,
      "code_end_line": 1979,
      "params": [
        "self",
        "n_active_threads"
      ],
      "have_return": false,
      "code_content": "    def _warn_mkl_vcomp(self, n_active_threads):\n        \"\"\"Warn when vcomp and mkl are both present\"\"\"\n        warnings.warn(\n            \"MiniBatchKMeans is known to have a memory leak on \"\n            \"Windows with MKL, when there are less chunks than \"\n            \"available threads. You can prevent it by setting \"\n            f\"batch_size >= {self._n_threads * CHUNK_SIZE} or by \"\n            \"setting the environment variable \"\n            f\"OMP_NUM_THREADS={n_active_threads}\"\n        )\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "_mini_batch_convergence",
      "md_content": [
        "**_mini_batch_convergence**: The function of _mini_batch_convergence is to implement early stopping logic for the MiniBatchKMeans clustering algorithm based on convergence criteria.\n\n**parameters**: The parameters of this Function.\n· step: An integer representing the current iteration step in the minibatch process, starting from 0.\n· n_steps: An integer indicating the total number of steps to be performed during the fitting process.\n· n_samples: An integer representing the total number of samples in the dataset.\n· centers_squared_diff: A float representing the squared difference in the positions of the cluster centers between iterations.\n· batch_inertia: A float representing the inertia (or within-cluster sum of squares) calculated for the current minibatch.\n\n**Code Description**: The _mini_batch_convergence function is a helper function designed to monitor the convergence of the MiniBatchKMeans algorithm during its iterative fitting process. It normalizes the batch inertia by dividing it by the batch size to ensure comparability across different batch sizes. The function begins by incrementing the step count for user-friendly output. It ignores the first iteration since it only contains inertia from initialization.\n\nThe function computes an Exponentially Weighted Average (EWA) of the batch inertia to smooth out the stochastic variability inherent in minibatch processing. This is done using a formula that incorporates a decay factor (alpha), which is derived from the batch size and the total number of samples. The EWA inertia is then logged if verbosity is enabled.\n\nThe function checks for convergence based on two criteria: \n1. If the absolute change in the position of the cluster centers (centers_squared_diff) is less than a predefined tolerance (_tol), it indicates convergence due to small changes in the centers.\n2. If there is no improvement in the smoothed inertia over a specified number of iterations (max_no_improvement), it also indicates convergence.\n\nIf either of these conditions is met, the function returns True, signaling that the algorithm can stop early. If neither condition is satisfied, it returns False, allowing the fitting process to continue.\n\nThis function is called within the fit method of the MiniBatchKMeans class, specifically during the iterative optimization loop. After each minibatch update, the fit method invokes _mini_batch_convergence to assess whether the algorithm has converged based on the current step's inertia and the change in cluster centers. This integration ensures that the fitting process is efficient and can terminate early when appropriate, enhancing performance and reducing unnecessary computations.\n\n**Note**: It is important to ensure that the parameters passed to this function are correctly calculated and represent the current state of the fitting process to avoid incorrect convergence signals.\n\n**Output Example**: The function may return a boolean value, such as True or False, indicating whether the algorithm has converged. For instance, if the function detects convergence due to small changes in cluster centers, it would return True."
      ],
      "code_start_line": 1981,
      "code_end_line": 2044,
      "params": [
        "self",
        "step",
        "n_steps",
        "n_samples",
        "centers_squared_diff",
        "batch_inertia"
      ],
      "have_return": true,
      "code_content": "    def _mini_batch_convergence(\n        self, step, n_steps, n_samples, centers_squared_diff, batch_inertia\n    ):\n        \"\"\"Helper function to encapsulate the early stopping logic\"\"\"\n        # Normalize inertia to be able to compare values when\n        # batch_size changes\n        batch_inertia /= self._batch_size\n\n        # count steps starting from 1 for user friendly verbose mode.\n        step = step + 1\n\n        # Ignore first iteration because it's inertia from initialization.\n        if step == 1:\n            if self.verbose:\n                print(\n                    f\"Minibatch step {step}/{n_steps}: mean batch \"\n                    f\"inertia: {batch_inertia}\"\n                )\n            return False\n\n        # Compute an Exponentially Weighted Average of the inertia to\n        # monitor the convergence while discarding minibatch-local stochastic\n        # variability: https://en.wikipedia.org/wiki/Moving_average\n        if self._ewa_inertia is None:\n            self._ewa_inertia = batch_inertia\n        else:\n            alpha = self._batch_size * 2.0 / (n_samples + 1)\n            alpha = min(alpha, 1)\n            self._ewa_inertia = self._ewa_inertia * (1 - alpha) + batch_inertia * alpha\n\n        # Log progress to be able to monitor convergence\n        if self.verbose:\n            print(\n                f\"Minibatch step {step}/{n_steps}: mean batch inertia: \"\n                f\"{batch_inertia}, ewa inertia: {self._ewa_inertia}\"\n            )\n\n        # Early stopping based on absolute tolerance on squared change of\n        # centers position\n        if self._tol > 0.0 and centers_squared_diff <= self._tol:\n            if self.verbose:\n                print(f\"Converged (small centers change) at step {step}/{n_steps}\")\n            return True\n\n        # Early stopping heuristic due to lack of improvement on smoothed\n        # inertia\n        if self._ewa_inertia_min is None or self._ewa_inertia < self._ewa_inertia_min:\n            self._no_improvement = 0\n            self._ewa_inertia_min = self._ewa_inertia\n        else:\n            self._no_improvement += 1\n\n        if (\n            self.max_no_improvement is not None\n            and self._no_improvement >= self.max_no_improvement\n        ):\n            if self.verbose:\n                print(\n                    \"Converged (lack of improvement in inertia) at step \"\n                    f\"{step}/{n_steps}\"\n                )\n            return True\n\n        return False\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "dataset/_kmeans.py/MiniBatchKMeans/fit"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "_random_reassign",
      "md_content": [
        "_random_reassign: The function of _random_reassign is to determine whether a random reassignment of clusters should occur based on the number of processed samples and the presence of empty clusters.\n\nparameters: The parameters of this Function.\n· None\n\nCode Description: The _random_reassign function is a private method within the MiniBatchKMeans class that checks if a random reassignment of data points to clusters is necessary. This function is crucial for maintaining the effectiveness of the clustering algorithm, particularly in scenarios where some clusters may become empty or when a significant number of samples have been processed since the last reassignment.\n\nThe function operates by first incrementing the _n_since_last_reassign attribute by the size of the current batch (_batch_size). It then evaluates two conditions to decide if a reassignment should take place:\n1. It checks if any clusters are empty by evaluating if the _counts array (which tracks the number of samples assigned to each cluster) contains any zeros. If any cluster is empty, a reassignment is warranted.\n2. It also checks if the number of samples processed since the last reassignment (_n_since_last_reassign) has reached or exceeded ten times the number of clusters (10 * n_clusters). This ensures that reassignments occur at regular intervals, promoting better convergence of the algorithm.\n\nIf either condition is met, the function resets _n_since_last_reassign to zero and returns True, indicating that a reassignment should occur. If neither condition is satisfied, it returns False.\n\nThe _random_reassign function is called within the fit and partial_fit methods of the MiniBatchKMeans class. In the fit method, it is invoked during the iterative optimization process, where mini-batches of data are processed. The result of _random_reassign informs the _mini_batch_step function whether to perform a random reassignment of data points to clusters. Similarly, in the partial_fit method, it is used to determine if a reassignment is needed when updating the clustering model with a new mini-batch of data. This integration ensures that the clustering algorithm remains adaptive and responsive to the distribution of data points across clusters.\n\nNote: It is important to ensure that the MiniBatchKMeans instance has been initialized correctly before invoking the fit or partial_fit methods, as the behavior of _random_reassign relies on the state of the clustering model.\n\nOutput Example: The function does not return a value in the traditional sense but returns a boolean indicating whether a reassignment is needed. For example, it may return True if a reassignment is warranted or False if it is not."
      ],
      "code_start_line": 2046,
      "code_end_line": 2060,
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def _random_reassign(self):\n        \"\"\"Check if a random reassignment needs to be done.\n\n        Do random reassignments each time 10 * n_clusters samples have been\n        processed.\n\n        If there are empty clusters we always want to reassign.\n        \"\"\"\n        self._n_since_last_reassign += self._batch_size\n        if (self._counts == 0).any() or self._n_since_last_reassign >= (\n            10 * self.n_clusters\n        ):\n            self._n_since_last_reassign = 0\n            return True\n        return False\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "dataset/_kmeans.py/MiniBatchKMeans/fit",
        "dataset/_kmeans.py/MiniBatchKMeans/partial_fit"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "fit",
      "md_content": [
        "**fit**: The function of fit is to compute the centroids on the input data X by chunking it into mini-batches.\n\n**parameters**: The parameters of this Function.\n· X: {array-like, sparse matrix} of shape (n_samples, n_features) - Training instances to cluster. The data will be converted to C ordering, which may cause a memory copy if the data is not C-contiguous. If a sparse matrix is passed, a copy will be made if it's not in CSR format.  \n· y: Ignored - Not used, present here for API consistency by convention.  \n· sample_weight: array-like of shape (n_samples,), default=None - The weights for each observation in X. If None, all observations are assigned equal weight. `sample_weight` is not used during initialization if `init` is a callable or a user-provided array.\n\n**Code Description**: The fit function is a core method of the MiniBatchKMeans class, responsible for performing the clustering operation on the provided dataset X. It begins by validating the input data through the _validate_data method, ensuring that the data is in an acceptable format and meets the requirements for clustering. The function checks for the compatibility of the input parameters with the data, including the initialization of cluster centers and the handling of sample weights.\n\nThe function then initializes several variables, including the number of samples and features in the dataset. It validates the initial cluster centers if provided, ensuring they conform to the expected shape relative to the number of clusters and features in the data. The function also precomputes the squared norms of the data points to optimize subsequent calculations.\n\nThe main clustering process is executed in an iterative loop, where mini-batches of data are sampled and processed. For each mini-batch, the function updates the cluster centers based on the assigned labels using the _mini_batch_step method. This method performs the actual update of the centers and computes the inertia, which quantifies the compactness of the clusters.\n\nThe function monitors convergence through the _mini_batch_convergence method, which checks if the algorithm has reached a satisfactory solution based on the change in cluster centers and the inertia values. If convergence criteria are met, the fitting process is terminated early to enhance efficiency.\n\nFinally, the function returns the fitted estimator, which includes the computed cluster centers and other relevant attributes, such as labels and inertia, if requested. The fit method is integral to the MiniBatchKMeans algorithm, allowing it to efficiently handle large datasets by processing them in smaller, manageable chunks.\n\n**Note**: It is crucial to ensure that the input data X is in the correct format and that all parameters are appropriately set before invoking the fit method to avoid runtime errors and ensure optimal clustering performance.\n\n**Output Example**: A possible return value of the function when called with a dataset of 5 samples and 3 clusters might look like this:  \nCluster Centers: [[1.5, 2.0], [3.0, 4.5], [5.0, 6.0]]  \nLabels: [0, 1, 0, 2, 1]  \nInertia: 10.56"
      ],
      "code_start_line": 2063,
      "code_end_line": 2216,
      "params": [
        "self",
        "X",
        "y",
        "sample_weight"
      ],
      "have_return": true,
      "code_content": "    def fit(self, X, y=None, sample_weight=None):\n        \"\"\"Compute the centroids on X by chunking it into mini-batches.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Training instances to cluster. It must be noted that the data\n            will be converted to C ordering, which will cause a memory copy\n            if the given data is not C-contiguous.\n            If a sparse matrix is passed, a copy will be made if it's not in\n            CSR format.\n\n        y : Ignored\n            Not used, present here for API consistency by convention.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            The weights for each observation in X. If None, all observations\n            are assigned equal weight. `sample_weight` is not used during\n            initialization if `init` is a callable or a user provided array.\n\n            .. versionadded:: 0.20\n\n        Returns\n        -------\n        self : object\n            Fitted estimator.\n        \"\"\"\n        X = self._validate_data(\n            X,\n            accept_sparse=\"csr\",\n            dtype=[np.float64, np.float32],\n            order=\"C\",\n            accept_large_sparse=False,\n        )\n\n        self._check_params_vs_input(X)\n        random_state = check_random_state(self.random_state)\n        sample_weight = _check_sample_weight(sample_weight, X, dtype=X.dtype)\n        self._n_threads = _openmp_effective_n_threads()\n        n_samples, n_features = X.shape\n\n        # Validate init array\n        init = self.init\n        if _is_arraylike_not_scalar(init):\n            init = check_array(init, dtype=X.dtype, copy=True, order=\"C\")\n            self._validate_center_shape(X, init)\n\n        self._check_mkl_vcomp(X, self._batch_size)\n\n        # precompute squared norms of data points\n        x_squared_norms = row_norms(X, squared=True)\n\n        # Validation set for the init\n        validation_indices = random_state.randint(0, n_samples, self._init_size)\n        X_valid = X[validation_indices]\n        sample_weight_valid = sample_weight[validation_indices]\n\n        # perform several inits with random subsets\n        best_inertia = None\n        for init_idx in range(self._n_init):\n            if self.verbose:\n                print(f\"Init {init_idx + 1}/{self._n_init} with method {init}\")\n\n            # Initialize the centers using only a fraction of the data as we\n            # expect n_samples to be very large when using MiniBatchKMeans.\n            cluster_centers = self._init_centroids(\n                X,\n                x_squared_norms=x_squared_norms,\n                init=init,\n                random_state=random_state,\n                init_size=self._init_size,\n                sample_weight=sample_weight,\n            )\n\n            # Compute inertia on a validation set.\n            _, inertia = _labels_inertia_threadpool_limit(\n                X_valid,\n                sample_weight_valid,\n                cluster_centers,\n                n_threads=self._n_threads,\n            )\n\n            if self.verbose:\n                print(f\"Inertia for init {init_idx + 1}/{self._n_init}: {inertia}\")\n            if best_inertia is None or inertia < best_inertia:\n                init_centers = cluster_centers\n                best_inertia = inertia\n\n        centers = init_centers\n        centers_new = np.empty_like(centers)\n\n        # Initialize counts\n        self._counts = np.zeros(self.n_clusters, dtype=X.dtype)\n\n        # Attributes to monitor the convergence\n        self._ewa_inertia = None\n        self._ewa_inertia_min = None\n        self._no_improvement = 0\n\n        # Initialize number of samples seen since last reassignment\n        self._n_since_last_reassign = 0\n\n        n_steps = (self.max_iter * n_samples) // self._batch_size\n\n        with threadpool_limits(limits=1, user_api=\"blas\"):\n            # Perform the iterative optimization until convergence\n            for i in range(n_steps):\n                # Sample a minibatch from the full dataset\n                minibatch_indices = random_state.randint(0, n_samples, self._batch_size)\n\n                # Perform the actual update step on the minibatch data\n                batch_inertia = _mini_batch_step(\n                    X=X[minibatch_indices],\n                    sample_weight=sample_weight[minibatch_indices],\n                    centers=centers,\n                    centers_new=centers_new,\n                    weight_sums=self._counts,\n                    random_state=random_state,\n                    random_reassign=self._random_reassign(),\n                    reassignment_ratio=self.reassignment_ratio,\n                    verbose=self.verbose,\n                    n_threads=self._n_threads,\n                )\n\n                if self._tol > 0.0:\n                    centers_squared_diff = np.sum((centers_new - centers) ** 2)\n                else:\n                    centers_squared_diff = 0\n\n                centers, centers_new = centers_new, centers\n\n                # Monitor convergence and do early stopping if necessary\n                if self._mini_batch_convergence(\n                    i, n_steps, n_samples, centers_squared_diff, batch_inertia\n                ):\n                    break\n\n        self.cluster_centers_ = centers\n        self._n_features_out = self.cluster_centers_.shape[0]\n\n        self.n_steps_ = i + 1\n        self.n_iter_ = int(np.ceil(((i + 1) * self._batch_size) / n_samples))\n\n        if self.compute_labels:\n            self.labels_, self.inertia_ = _labels_inertia_threadpool_limit(\n                X,\n                sample_weight,\n                self.cluster_centers_,\n                n_threads=self._n_threads,\n            )\n        else:\n            self.inertia_ = self._ewa_inertia * n_samples\n\n        return self\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "dataset/_kmeans.py/_labels_inertia_threadpool_limit",
        "dataset/_kmeans.py/_BaseKMeans/_check_mkl_vcomp",
        "dataset/_kmeans.py/_BaseKMeans/_validate_center_shape",
        "dataset/_kmeans.py/_BaseKMeans/_init_centroids",
        "dataset/_kmeans.py/_mini_batch_step",
        "dataset/_kmeans.py/MiniBatchKMeans/_check_params_vs_input",
        "dataset/_kmeans.py/MiniBatchKMeans/_mini_batch_convergence",
        "dataset/_kmeans.py/MiniBatchKMeans/_random_reassign"
      ],
      "special_reference_type": [
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "partial_fit",
      "md_content": [
        "**partial_fit**: The function of partial_fit is to update the k-means estimate on a single mini-batch of data.\n\n**parameters**: The parameters of this Function.\n· X: {array-like, sparse matrix} of shape (n_samples, n_features) - Training instances to cluster. The data will be converted to C ordering, which may cause a memory copy if the data is not C-contiguous. If a sparse matrix is passed, a copy will be made if it's not in CSR format.  \n· y: Ignored - Not used, present here for API consistency by convention.  \n· sample_weight: array-like of shape (n_samples,), default=None - The weights for each observation in X. If None, all observations are assigned equal weight. `sample_weight` is not used during initialization if `init` is a callable or a user-provided array.\n\n**Code Description**: The partial_fit function is a core method of the MiniBatchKMeans class, designed to incrementally update the clustering model with new data. It begins by checking if the model has already been fitted by examining the presence of cluster centers. If the model has not been fitted, it initializes the cluster centers based on the provided data and specified initialization method.\n\nThe function first validates the input data X, ensuring it is in an acceptable format (either dense or sparse) and conforms to the expected data types. It also checks the sample weights to ensure they are appropriately defined. The squared norms of the data points are precomputed to facilitate efficient distance calculations during the clustering process.\n\nIf the model is being fitted for the first time, the function validates the initialization parameters and computes the initial cluster centers using the _init_centroids method. It also initializes the counts of samples assigned to each cluster and tracks the number of samples seen since the last reassignment.\n\nThe core of the function involves performing a mini-batch step, where the _mini_batch_step function is called to update the cluster centers based on the current mini-batch of data. This function handles the assignment of samples to the nearest cluster centers and updates the centers accordingly.\n\nAdditionally, if the compute_labels attribute is set to True, the function computes the labels and inertia for the current mini-batch using the _labels_inertia_threadpool_limit function. This provides insights into the clustering quality and helps in monitoring the convergence of the algorithm.\n\nThe function concludes by incrementing the number of steps taken and returning the updated estimator, allowing for further incremental updates with additional data.\n\nThe partial_fit method is integral to the MiniBatchKMeans algorithm, enabling it to process large datasets in smaller, manageable chunks while continuously refining the clustering model.\n\n**Note**: It is essential to ensure that the input data X is in the correct format and that all parameters are set appropriately before calling the partial_fit method to avoid runtime errors and ensure optimal clustering performance.\n\n**Output Example**: A possible return value of the function could be the updated MiniBatchKMeans instance itself, reflecting the changes made during the fitting process. For example:  \nMiniBatchKMeans(n_clusters=3, n_steps=1, cluster_centers_=array([[1.5, 2.5], [3.0, 4.0], [5.0, 6.0]]))"
      ],
      "code_start_line": 2219,
      "code_end_line": 2318,
      "params": [
        "self",
        "X",
        "y",
        "sample_weight"
      ],
      "have_return": true,
      "code_content": "    def partial_fit(self, X, y=None, sample_weight=None):\n        \"\"\"Update k means estimate on a single mini-batch X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Training instances to cluster. It must be noted that the data\n            will be converted to C ordering, which will cause a memory copy\n            if the given data is not C-contiguous.\n            If a sparse matrix is passed, a copy will be made if it's not in\n            CSR format.\n\n        y : Ignored\n            Not used, present here for API consistency by convention.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            The weights for each observation in X. If None, all observations\n            are assigned equal weight. `sample_weight` is not used during\n            initialization if `init` is a callable or a user provided array.\n\n        Returns\n        -------\n        self : object\n            Return updated estimator.\n        \"\"\"\n        has_centers = hasattr(self, \"cluster_centers_\")\n\n        X = self._validate_data(\n            X,\n            accept_sparse=\"csr\",\n            dtype=[np.float64, np.float32],\n            order=\"C\",\n            accept_large_sparse=False,\n            reset=not has_centers,\n        )\n\n        self._random_state = getattr(\n            self, \"_random_state\", check_random_state(self.random_state)\n        )\n        sample_weight = _check_sample_weight(sample_weight, X, dtype=X.dtype)\n        self.n_steps_ = getattr(self, \"n_steps_\", 0)\n\n        # precompute squared norms of data points\n        x_squared_norms = row_norms(X, squared=True)\n\n        if not has_centers:\n            # this instance has not been fitted yet (fit or partial_fit)\n            self._check_params_vs_input(X)\n            self._n_threads = _openmp_effective_n_threads()\n\n            # Validate init array\n            init = self.init\n            if _is_arraylike_not_scalar(init):\n                init = check_array(init, dtype=X.dtype, copy=True, order=\"C\")\n                self._validate_center_shape(X, init)\n\n            self._check_mkl_vcomp(X, X.shape[0])\n\n            # initialize the cluster centers\n            self.cluster_centers_ = self._init_centroids(\n                X,\n                x_squared_norms=x_squared_norms,\n                init=init,\n                random_state=self._random_state,\n                init_size=self._init_size,\n                sample_weight=sample_weight,\n            )\n\n            # Initialize counts\n            self._counts = np.zeros(self.n_clusters, dtype=X.dtype)\n\n            # Initialize number of samples seen since last reassignment\n            self._n_since_last_reassign = 0\n\n        with threadpool_limits(limits=1, user_api=\"blas\"):\n            _mini_batch_step(\n                X,\n                sample_weight=sample_weight,\n                centers=self.cluster_centers_,\n                centers_new=self.cluster_centers_,\n                weight_sums=self._counts,\n                random_state=self._random_state,\n                random_reassign=self._random_reassign(),\n                reassignment_ratio=self.reassignment_ratio,\n                verbose=self.verbose,\n                n_threads=self._n_threads,\n            )\n\n        if self.compute_labels:\n            self.labels_, self.inertia_ = _labels_inertia_threadpool_limit(\n                X,\n                sample_weight,\n                self.cluster_centers_,\n                n_threads=self._n_threads,\n            )\n\n        self.n_steps_ += 1\n        self._n_features_out = self.cluster_centers_.shape[0]\n\n        return self\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "dataset/_kmeans.py/_labels_inertia_threadpool_limit",
        "dataset/_kmeans.py/_BaseKMeans/_check_mkl_vcomp",
        "dataset/_kmeans.py/_BaseKMeans/_validate_center_shape",
        "dataset/_kmeans.py/_BaseKMeans/_init_centroids",
        "dataset/_kmeans.py/_mini_batch_step",
        "dataset/_kmeans.py/MiniBatchKMeans/_check_params_vs_input",
        "dataset/_kmeans.py/MiniBatchKMeans/_random_reassign"
      ],
      "special_reference_type": [
        false,
        false,
        false,
        false,
        false,
        false,
        false
      ]
    }
  ],
  "dataset/bench_glmnet.py": [
    {
      "type": "FunctionDef",
      "name": "rmse",
      "md_content": [
        "**rmse**: The function of rmse is to calculate the Root Mean Squared Error (RMSE) between two arrays.\n\n**parameters**: The parameters of this Function.\n· a: The first array, typically representing the actual values.\n· b: The second array, typically representing the predicted values.\n\n**Code Description**: The `rmse` function computes the Root Mean Squared Error (RMSE) between two arrays, `a` and `b`. RMSE is a commonly used metric to measure the differences between values predicted by a model and the actual observed values. The function first calculates the squared differences between corresponding elements of the two arrays using `(a - b) ** 2`. It then computes the mean of these squared differences using `np.mean()`. Finally, it takes the square root of the mean squared differences using `np.sqrt()` to obtain the RMSE value. This value is returned as the output of the function.\n\nIn the context of the project, the `rmse` function is called within the `bench` function to evaluate the performance of a machine learning model. Specifically, it is used to calculate the RMSE between the actual test values (`Y_test`) and the predicted values (`clf.predict(X_test)`). This helps in assessing how well the model is performing in terms of prediction accuracy.\n\n**Note**: Ensure that the input arrays `a` and `b` have the same shape, as the function performs element-wise operations. If the arrays have different shapes, it will result in an error.\n\n**Output Example**: The function returns a single floating-point value representing the RMSE. For example, if the actual values are `[3, 5, 7]` and the predicted values are `[2.5, 5.5, 7.5]`, the function might return `0.7071` as the RMSE."
      ],
      "code_start_line": 28,
      "code_end_line": 29,
      "params": [
        "a",
        "b"
      ],
      "have_return": true,
      "code_content": "def rmse(a, b):\n    return np.sqrt(np.mean((a - b) ** 2))\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "dataset/bench_glmnet.py/bench"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "bench",
      "md_content": [
        "**bench**: The function of bench is to evaluate the performance of a machine learning model by measuring the training duration, prediction accuracy (RMSE), and the difference between the model's coefficients and reference coefficients.\n\n**parameters**: The parameters of this Function.\n· factory: A factory function that creates and returns a machine learning model instance. The model is expected to have an `alpha` parameter and methods `fit` and `predict`.\n· X: The feature matrix used for training the model.\n· Y: The target values (labels) corresponding to the training data.\n· X_test: The feature matrix used for testing the model.\n· Y_test: The target values (labels) corresponding to the test data.\n· ref_coef: A reference coefficient array used to compare with the model's learned coefficients.\n\n**Code Description**: The `bench` function performs the following steps to evaluate a machine learning model:\n1. It starts by invoking garbage collection (`gc.collect()`) to free up memory and ensure a clean environment for timing the model training.\n2. The function records the start time using `time()`.\n3. It creates a model instance by calling the `factory` function with the `alpha` parameter and fits the model to the training data (`X`, `Y`) using the `fit` method.\n4. The training duration is calculated by subtracting the start time from the current time.\n5. The function prints the training duration in seconds.\n6. It calculates the Root Mean Squared Error (RMSE) between the actual test values (`Y_test`) and the predicted values (`clf.predict(X_test)`) using the `rmse` function. The RMSE is printed as a measure of prediction accuracy.\n7. The function computes the mean absolute difference between the reference coefficients (`ref_coef`) and the model's learned coefficients (`clf.coef_.ravel()`). This difference is printed as a measure of how closely the model's coefficients match the reference.\n8. Finally, the function returns the training duration as a floating-point value.\n\nThe `bench` function relies on the `rmse` function to calculate the prediction accuracy. The `rmse` function computes the Root Mean Squared Error between two arrays, which is a standard metric for evaluating regression models. The relationship between `bench` and `rmse` is functional, as `bench` uses `rmse` to quantify the model's performance on the test data.\n\n**Note**: Ensure that the `factory` function returns a model instance that supports the `fit` and `predict` methods, and that the `alpha` parameter is correctly set. Additionally, the input arrays (`X`, `Y`, `X_test`, `Y_test`, `ref_coef`) should be properly formatted and compatible with the model's requirements.\n\n**Output Example**: The function returns the training duration as a floating-point value. For example, if the training process takes 1.234 seconds, the function will return `1.234`. The printed output might look like this:\n```\nduration: 1.234s\nrmse: 0.567890\nmean coef abs diff: 0.012345\n```"
      ],
      "code_start_line": 32,
      "code_end_line": 44,
      "params": [
        "factory",
        "X",
        "Y",
        "X_test",
        "Y_test",
        "ref_coef"
      ],
      "have_return": true,
      "code_content": "def bench(factory, X, Y, X_test, Y_test, ref_coef):\n    gc.collect()\n\n    # start time\n    tstart = time()\n    clf = factory(alpha=alpha).fit(X, Y)\n    delta = (time() - tstart)\n    # stop time\n\n    print(\"duration: %0.3fs\" % delta)\n    print(\"rmse: %f\" % rmse(Y_test, clf.predict(X_test)))\n    print(\"mean coef abs diff: %f\" % abs(ref_coef - clf.coef_.ravel()).mean())\n    return delta\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "dataset/bench_glmnet.py/rmse"
      ],
      "special_reference_type": [
        false
      ]
    }
  ],
  "dataset/verify.py": [
    {
      "type": "ClassDef",
      "name": "ReturnCode",
      "md_content": [
        "**ReturnCode**: The function of ReturnCode is to define a set of constants representing the various return statuses for file verification operations.\n\n**attributes**: The attributes of this Class.\n· SUCCESS: Indicates that the operation completed successfully.  \n· INTEGRITY_FAILURE: Indicates that a file's integrity check has failed.  \n· FILE_GET_FAILED: Indicates that the attempt to retrieve a file was unsuccessful.  \n· FILE_MISSING_FROM_ONE_HOST: Indicates that a file was not found on one of the specified hosts.  \n· FILES_NOT_EQUAL: Indicates that the files being compared are not identical.  \n· NO_BINARIES_MATCH: Indicates that no binaries matched the specified criteria.  \n· NOT_ENOUGH_GOOD_SIGS: Indicates that there are not enough trusted signatures to meet the required threshold.  \n· BINARY_DOWNLOAD_FAILED: Indicates that the download of a binary file has failed.  \n· BAD_VERSION: Indicates that the version provided is not acceptable or is incorrectly formatted.  \n\n**Code Description**: The ReturnCode class is an enumeration that extends the functionality of the built-in IntEnum class from the enum module. It provides a clear and organized way to represent various return codes that can be used throughout the dataset verification process. Each attribute corresponds to a specific outcome of operations related to file retrieval, integrity checks, and signature verification.\n\nThis class is utilized in several functions within the dataset/verify.py module, including get_files_from_hosts_and_compare, verify_shasums_signature, verify_binary_hashes, verify_published_handler, and verify_binaries_handler. Each of these functions returns a ReturnCode value to indicate the result of their operations. For instance, if a file cannot be retrieved from a host, the function will return ReturnCode.FILE_GET_FAILED. Similarly, if the integrity of a file fails, ReturnCode.INTEGRITY_FAILURE will be returned.\n\nThe use of ReturnCode enhances code readability and maintainability by providing meaningful names for return values instead of using arbitrary integers. This allows developers to quickly understand the outcome of operations without needing to reference documentation or comments extensively.\n\n**Note**: When using the ReturnCode class, it is essential to handle each return value appropriately in the calling functions to ensure that errors are logged and managed correctly. This practice helps maintain the robustness of the verification process and provides clear feedback to users regarding the status of their operations."
      ],
      "code_start_line": 56,
      "code_end_line": 65,
      "params": [],
      "have_return": false,
      "code_content": "class ReturnCode(enum.IntEnum):\n    SUCCESS = 0\n    INTEGRITY_FAILURE = 1\n    FILE_GET_FAILED = 4\n    FILE_MISSING_FROM_ONE_HOST = 5\n    FILES_NOT_EQUAL = 6\n    NO_BINARIES_MATCH = 7\n    NOT_ENOUGH_GOOD_SIGS = 9\n    BINARY_DOWNLOAD_FAILED = 10\n    BAD_VERSION = 11\n",
      "name_column": 6,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "dataset/verify.py/get_files_from_hosts_and_compare",
        "dataset/verify.py/verify_shasums_signature",
        "dataset/verify.py/verify_binary_hashes",
        "dataset/verify.py/verify_published_handler",
        "dataset/verify.py/verify_binaries_handler"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "set_up_logger",
      "md_content": [
        "**set_up_logger**: The function of set_up_logger is to configure a logger that outputs log messages to standard error (stderr).\n\n**parameters**: The parameters of this Function.\n· is_verbose: A boolean value that determines the logging level. If set to True, the logger will log informational messages; if set to False, it will log warnings and above.\n\n**Code Description**: The set_up_logger function initializes a logger using Python's logging module. It first retrieves a logger instance associated with the current module using `logging.getLogger(__name__)`. The logging level is set based on the is_verbose parameter: if is_verbose is True, the logging level is set to INFO, allowing informational messages to be logged; if False, the level is set to WARNING, which restricts logging to warning messages and errors only. \n\nNext, a StreamHandler is created to direct log messages to standard error (stderr). This handler is set to DEBUG level, meaning it will process all messages at this level and above. A formatter is then defined to structure the log messages, which will display the log level and the message in the format '[LEVEL] message'. The formatter is applied to the console handler, and the handler is added to the logger. Finally, the configured logger instance is returned for use in other parts of the application.\n\n**Note**: It is important to ensure that the logging configuration does not conflict with other logging setups in the application. The is_verbose parameter allows for easy toggling between detailed and concise logging output.\n\n**Output Example**: When is_verbose is set to True and a log message is generated, the output might appear as:\n```\n[INFO] This is an informational message.\n```\nIf is_verbose is set to False, the output for a warning message would be:\n```\n[WARNING] This is a warning message.\n```"
      ],
      "code_start_line": 68,
      "code_end_line": 77,
      "params": [
        "is_verbose"
      ],
      "have_return": true,
      "code_content": "def set_up_logger(is_verbose: bool = True) -> logging.Logger:\n    \"\"\"Set up a logger that writes to stderr.\"\"\"\n    log = logging.getLogger(__name__)\n    log.setLevel(logging.INFO if is_verbose else logging.WARNING)\n    console = logging.StreamHandler(sys.stderr)  # log to stderr\n    console.setLevel(logging.DEBUG)\n    formatter = logging.Formatter('[%(levelname)s] %(message)s')\n    console.setFormatter(formatter)\n    log.addHandler(console)\n    return log\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "indent",
      "md_content": [
        "**indent**: The function of indent is to add indentation to a given string output.\n\n**parameters**: The parameters of this Function.\n· output: A string that represents the text to which indentation will be added.\n\n**Code Description**: The indent function utilizes the textwrap module's indent method to prepend a specified string (in this case, two spaces) to each line of the provided output string. This function is particularly useful for formatting text output, making it more readable by visually distinguishing it from other text. \n\nThe indent function is called in several other functions within the dataset/verify.py module. For instance, in the files_are_equal function, it is used to format the output of the diff between two files when they are found to be unequal. This enhances the clarity of the log messages by ensuring that the differences are easily identifiable.\n\nSimilarly, in the get_files_from_hosts_and_compare function, the indent function is employed to format the output from the wget command when a file download fails. This ensures that error messages are presented in a structured manner, making it easier for developers to diagnose issues.\n\nIn the check_multisig function, the indent function is used to format the output from the GPG verification process when verbose logging is enabled. This allows users to see the GPG output in a more organized way, improving the readability of the logs.\n\nLastly, in the verify_shasums_signature function, the indent function formats the GPG output when an integrity failure occurs. This consistent use of indentation across various functions helps maintain a uniform logging style throughout the module.\n\n**Note**: When using the indent function, ensure that the output string is properly formatted to avoid unexpected results. The function assumes that the input is a string and does not handle cases where the input may be of a different type.\n\n**Output Example**: If the input to the indent function is:\n```\n\"Line 1\\nLine 2\\nLine 3\"\n```\nThe output will be:\n```\n\"  Line 1\\n  Line 2\\n  Line 3\"\n```"
      ],
      "code_start_line": 83,
      "code_end_line": 84,
      "params": [
        "output"
      ],
      "have_return": true,
      "code_content": "def indent(output: str) -> str:\n    return textwrap.indent(output, '  ')\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "dataset/verify.py/files_are_equal",
        "dataset/verify.py/get_files_from_hosts_and_compare",
        "dataset/verify.py/check_multisig",
        "dataset/verify.py/verify_shasums_signature",
        "dataset/verify.py/verify_published_handler"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "bool_from_env",
      "md_content": [
        "**bool_from_env**: The function of bool_from_env is to retrieve a boolean value from the environment variables based on a specified key.\n\n**parameters**: The parameters of this Function.\n· parameter1: key - A string representing the name of the environment variable to check.\n· parameter2: default - A boolean value that serves as the fallback if the specified key is not found in the environment variables. The default value is set to False.\n\n**Code Description**: The bool_from_env function checks if a specified key exists in the environment variables. If the key is not present, it returns the default value provided as a parameter. If the key is found, it retrieves the corresponding value and converts it to lowercase for comparison. The function recognizes the strings '1' and 'true' as True, while '0' and 'false' are interpreted as False. If the value does not match any of these recognized strings, the function raises a ValueError, indicating that the environment variable contains an unrecognized value.\n\nThis function is utilized within the main function of the dataset/verify.py module. It is called multiple times to set default values for various command-line arguments based on the corresponding environment variables. For instance, the verbosity level, quiet mode, and JSON output options are all determined by the values of environment variables such as 'BINVERIFY_VERBOSE', 'BINVERIFY_QUIET', and 'BINVERIFY_JSON', respectively. This design allows for flexible configuration of the program's behavior through environment variables, enhancing usability and adaptability in different execution contexts.\n\n**Note**: It is important to ensure that the environment variable values are strictly '1', 'true', '0', or 'false' to avoid triggering the ValueError. Users should be aware of the expected formats when setting environment variables to prevent runtime errors.\n\n**Output Example**: If the environment variable 'BINVERIFY_VERBOSE' is set to 'true', the function call bool_from_env('BINVERIFY_VERBOSE') will return True. If the variable is not set, it will return False as the default value."
      ],
      "code_start_line": 87,
      "code_end_line": 96,
      "params": [
        "key",
        "default"
      ],
      "have_return": true,
      "code_content": "def bool_from_env(key, default=False) -> bool:\n    if key not in os.environ:\n        return default\n    raw = os.environ[key]\n\n    if raw.lower() in ('1', 'true'):\n        return True\n    elif raw.lower() in ('0', 'false'):\n        return False\n    raise ValueError(f\"Unrecognized environment value {key}={raw!r}\")\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "dataset/verify.py/main"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "parse_version_string",
      "md_content": [
        "**parse_version_string**: The function of parse_version_string is to parse a version string into its base version, release candidate (if applicable), and operating system/platform information.\n\n**parameters**: The parameters of this Function.\n· version_str: A string representing the version, which may include a release candidate suffix and/or platform information.\n\n**Code Description**: The parse_version_string function takes a version string formatted in a specific way and splits it into three components: the base version, the release candidate (if present), and the operating system or platform information. The function first splits the input string using the hyphen ('-') as a delimiter. The first part of the split string is always considered the base version. Depending on the number of parts obtained from the split, the function determines whether there is a release candidate or platform information. \n\nIf the input string contains two parts, it checks if the second part includes \"rc\" to identify it as a release candidate; otherwise, it is treated as platform information. If there are three parts, the second part is assigned to the release candidate and the third part to the platform information. The function then returns a tuple containing the base version, release candidate, and platform information.\n\nThis function is called within the verify_published_handler function, which is responsible for verifying published binaries based on the provided version string. The parse_version_string function is crucial in this context as it extracts the necessary components from the version string to determine the appropriate remote directory for fetching binaries and signatures. The successful parsing of the version string is essential for the subsequent operations in verify_published_handler, such as constructing the remote directory path and managing the verification process.\n\n**Note**: It is important to ensure that the version string provided to the parse_version_string function adheres to the expected format, as deviations may lead to exceptions being raised during parsing.\n\n**Output Example**: For an input string \"1.0.0-rc1-linux\", the function would return the tuple: (\"1.0.0\", \"rc1\", \"linux\")."
      ],
      "code_start_line": 102,
      "code_end_line": 116,
      "params": [
        "version_str"
      ],
      "have_return": true,
      "code_content": "def parse_version_string(version_str):\n    parts = version_str.split('-')\n    version_base = parts[0]\n    version_rc = \"\"\n    version_os = \"\"\n    if len(parts) == 2:  # \"<version>-rcN\" or \"version-platform\"\n        if \"rc\" in parts[1]:\n            version_rc = parts[1]\n        else:\n            version_os = parts[1]\n    elif len(parts) == 3:  # \"<version>-rcN-platform\"\n        version_rc = parts[1]\n        version_os = parts[2]\n\n    return version_base, version_rc, version_os\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "dataset/verify.py/verify_published_handler"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "download_with_wget",
      "md_content": [
        "**download_with_wget**: The function of download_with_wget is to download a file from a specified remote location and save it to a local file path using the wget command-line utility.\n\n**parameters**: The parameters of this Function.\n· remote_file: A string representing the URL of the file to be downloaded.\n· local_file: A string representing the path where the downloaded file will be saved locally.\n\n**Code Description**: The download_with_wget function utilizes the subprocess module to execute the wget command, which is a widely used utility for downloading files from the web. The function constructs a command that includes the remote file URL and the local file path where the downloaded content should be stored. It runs this command and captures both the standard output and standard error. The function returns a tuple containing a boolean indicating the success of the download operation and the decoded output from the wget command, stripped of any trailing whitespace.\n\nThis function is called by other functions in the project, specifically get_files_from_hosts_and_compare and verify_published_handler. In get_files_from_hosts_and_compare, download_with_wget is used to retrieve files from multiple hosts, ensuring that the files are identical across these sources. The success of the download is critical for the subsequent comparison of file contents. In verify_published_handler, download_with_wget is employed to download binary files after verifying their signatures and checksums, ensuring that the correct files are obtained for further verification processes. The successful execution of download_with_wget is essential for the overall integrity and reliability of the file verification workflow in the project.\n\n**Note**: It is important to ensure that the wget utility is installed and accessible in the environment where this function is executed. Additionally, the remote URL must be valid and reachable to avoid download failures.\n\n**Output Example**: A possible return value of the function could be (True, \"Downloaded 1234 bytes in 0.5 seconds\"), indicating that the download was successful and providing information about the download size and time."
      ],
      "code_start_line": 119,
      "code_end_line": 122,
      "params": [
        "remote_file",
        "local_file"
      ],
      "have_return": true,
      "code_content": "def download_with_wget(remote_file, local_file):\n    result = subprocess.run(['wget', '-O', local_file, remote_file],\n                            stderr=subprocess.STDOUT, stdout=subprocess.PIPE)\n    return result.returncode == 0, result.stdout.decode().rstrip()\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "dataset/verify.py/get_files_from_hosts_and_compare",
        "dataset/verify.py/verify_published_handler"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "download_lines_with_urllib",
      "md_content": [
        "**download_lines_with_urllib**: The function of download_lines_with_urllib is to retrieve text lines from a specified URL over HTTP.\n\n**parameters**: The parameters of this Function.\n· url: A string representing the URL from which to download the text lines.\n\n**Code Description**: The download_lines_with_urllib function attempts to open a specified URL and read its content line by line. It utilizes the urllib library to perform an HTTP request. The function returns a tuple containing a boolean value and a list of strings. If the request is successful, the boolean value is True, and the list contains the stripped lines of text retrieved from the URL. Each line is decoded from bytes to a string format. If an HTTP error occurs, such as a 404 or 500 status code, the function catches the urllib.error.HTTPError exception and logs a warning message indicating the failure. Additionally, if any other exception occurs during the request, it is caught, and a warning is logged as well. In both cases of failure, the function returns a tuple with the boolean value set to False and an empty list.\n\n**Note**: It is important to ensure that the URL provided is valid and accessible. The function handles exceptions gracefully, logging warnings for any errors encountered during the HTTP request.\n\n**Output Example**: A successful call to download_lines_with_urllib(\"http://example.com/file.txt\") might return:\n(True, ['First line of text', 'Second line of text', 'Third line of text']) \n\nConversely, if the URL is invalid or an error occurs, it might return:\n(False, [])"
      ],
      "code_start_line": 125,
      "code_end_line": 134,
      "params": [
        "url"
      ],
      "have_return": true,
      "code_content": "def download_lines_with_urllib(url) -> t.Tuple[bool, t.List[str]]:\n    \"\"\"Get (success, text lines of a file) over HTTP.\"\"\"\n    try:\n        return (True, [\n            line.strip().decode() for line in urllib.request.urlopen(url).readlines()])\n    except urllib.error.HTTPError as e:\n        log.warning(f\"HTTP request to {url} failed (HTTPError): {e}\")\n    except Exception as e:\n        log.warning(f\"HTTP request to {url} failed ({e})\")\n    return (False, [])\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "verify_with_gpg",
      "md_content": [
        "**verify_with_gpg**: The function of verify_with_gpg is to verify the authenticity of a file using GPG signatures.\n\n**parameters**: The parameters of this Function.\n· filename: The path to the file that needs to be verified.\n· signature_filename: The path to the GPG signature file associated with the file being verified.\n· output_filename: An optional parameter specifying the path where the output should be written. If not provided, the output will not be written to a file.\n\n**Code Description**: The verify_with_gpg function utilizes the GPG (GNU Privacy Guard) command-line tool to verify the signature of a specified file. It constructs a command with the necessary arguments to invoke GPG, including options to handle the verification process and specify the output format. The function creates a temporary file to capture the status output from GPG during the verification process.\n\nThe function begins by creating a temporary file using `tempfile.NamedTemporaryFile()` to store the status messages generated by GPG. It then constructs the command-line arguments for the GPG verification, including options to display only the primary UID and to specify the output file for the verification results. The environment variable `LANGUAGE` is set to 'en' to ensure that the output is in English.\n\nThe subprocess module is used to execute the GPG command, capturing both standard output and error output. After the command execution, the status file is read to obtain the GPG output, which is then decoded and stripped of any trailing whitespace.\n\nThe function logs the return code and the output from GPG for debugging purposes and returns a tuple containing the GPG return code and the status output. A return code of '0' typically indicates a successful verification, while other codes indicate various error states.\n\nThis function is called by the check_multisig function, which is responsible for checking the signatures of multiple files. In check_multisig, verify_with_gpg is invoked to validate the signature of a sums file against a provided signature file. The output from verify_with_gpg is then parsed to determine the status of the signatures (good, unknown, or bad). If there are unknown signatures and the user opts to import keys, the function attempts to retrieve the necessary keys before re-verifying the signatures.\n\n**Note**: It is important to ensure that GPG is installed and properly configured on the system where this function is executed. Additionally, the output_filename parameter should be used with caution, as providing an invalid path may lead to errors during execution.\n\n**Output Example**: A possible return value from the function could be (0, \"gpg: Good signature from 'John Doe <john@example.com>'\"), indicating a successful verification with a message confirming the good signature."
      ],
      "code_start_line": 137,
      "code_end_line": 154,
      "params": [
        "filename",
        "signature_filename",
        "output_filename"
      ],
      "have_return": true,
      "code_content": "def verify_with_gpg(\n    filename,\n    signature_filename,\n    output_filename: t.Optional[str] = None\n) -> t.Tuple[int, str]:\n    with tempfile.NamedTemporaryFile() as status_file:\n        args = [\n            'gpg', '--yes', '--verify', '--verify-options', 'show-primary-uid-only', \"--status-file\", status_file.name,\n            '--output', output_filename if output_filename else '', signature_filename, filename]\n\n        env = dict(os.environ, LANGUAGE='en')\n        result = subprocess.run(args, stderr=subprocess.STDOUT, stdout=subprocess.PIPE, env=env)\n\n        gpg_data = status_file.read().decode().rstrip()\n\n    log.debug(f'Result from GPG ({result.returncode}): {result.stdout.decode()}')\n    log.debug(f\"{gpg_data}\")\n    return result.returncode, gpg_data\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "dataset/verify.py/check_multisig"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "remove_files",
      "md_content": [
        "**remove_files**: The function of remove_files is to delete files specified in a list of filenames.\n\n**parameters**: The parameters of this Function.\n· parameter1: filenames - A list of strings, where each string represents the name of a file to be removed from the filesystem.\n\n**Code Description**: The remove_files function iterates over a list of filenames provided as an argument. For each filename in the list, it calls the os.remove() function to delete the corresponding file from the filesystem. This function assumes that the filenames provided are valid and that the files exist; otherwise, an exception will be raised if a file cannot be found or accessed.\n\nThe function does not return any value. It performs the operation of file deletion directly. It is important to ensure that the list of filenames does not contain any unintended files, as this operation is irreversible and will permanently remove the specified files from the system.\n\n**Note**: Points to note about the use of the code\n- Ensure that the filenames provided are correct and that the files exist to avoid exceptions.\n- Consider implementing error handling to manage cases where a file cannot be deleted, such as using try-except blocks around the os.remove() call.\n- Be cautious when using this function, as it will permanently delete files without any confirmation or recovery option."
      ],
      "code_start_line": 157,
      "code_end_line": 159,
      "params": [
        "filenames"
      ],
      "have_return": false,
      "code_content": "def remove_files(filenames):\n    for filename in filenames:\n        os.remove(filename)\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "ClassDef",
      "name": "SigData",
      "md_content": [
        "**SigData**: The function of SigData is to represent GPG signature data parsed from GPG stdout.\n\n**attributes**: The attributes of this Class.\n· key: Represents the unique identifier of the GPG key associated with the signature. It can be None if not set.\n· name: A string that holds the name of the entity associated with the signature.\n· trusted: A boolean indicating whether the signature is trusted or not.\n· status: A string that describes the status of the signature, such as \"expired\" or \"revoked\".\n\n**Code Description**: The SigData class is designed to encapsulate the details of a GPG signature, including the key, name, trust status, and any relevant status messages. The constructor initializes the attributes to default values, with `key` set to None, `name` as an empty string, `trusted` as False, and `status` as an empty string. \n\nThe class includes a `__bool__` method that allows instances of SigData to be evaluated in a boolean context. This method returns True if the `key` attribute is not None, indicating that the signature data is valid. The `__repr__` method provides a string representation of the SigData instance, which includes the values of its attributes, formatted for clarity.\n\nThe SigData class is utilized within the context of signature verification processes in the project. It is primarily called by functions such as `parse_gpg_result`, `check_multisig`, and `verify_shasums_signature`. The `parse_gpg_result` function processes the output from GPG, creating instances of SigData for good, unknown, and bad signatures. These instances are then returned as lists to the calling functions, which further handle the verification logic, including checking the trustworthiness of the signatures and logging the results.\n\n**Note**: When using the SigData class, it is important to ensure that the `key` attribute is set appropriately to reflect the actual GPG key being represented. The trust status and signature status should also be updated based on the results of the GPG verification process.\n\n**Output Example**: An instance of SigData might be represented as follows:\nSigData('A1B2C3D4', 'John Doe', trusted=True, status='')"
      ],
      "code_start_line": 162,
      "code_end_line": 176,
      "params": [],
      "have_return": true,
      "code_content": "class SigData:\n    \"\"\"GPG signature data as parsed from GPG stdout.\"\"\"\n    def __init__(self):\n        self.key = None\n        self.name = \"\"\n        self.trusted = False\n        self.status = \"\"\n\n    def __bool__(self):\n        return self.key is not None\n\n    def __repr__(self):\n        return (\n            \"SigData(%r, %r, trusted=%s, status=%r)\" %\n            (self.key, self.name, self.trusted, self.status))\n",
      "name_column": 6,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "dataset/verify.py/parse_gpg_result",
        "dataset/verify.py/check_multisig",
        "dataset/verify.py/verify_shasums_signature"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [
        "**__init__**: The function of __init__ is to initialize an instance of the SigData class with default attribute values.\n\n**parameters**: The parameters of this Function.\n· There are no parameters for this function.\n\n**Code Description**: The __init__ function is a constructor method that is automatically called when an instance of the SigData class is created. This function initializes four attributes of the class: \n- `self.key`: This attribute is set to None, indicating that it does not hold any value upon initialization. It is likely intended to store a key value that may be assigned later in the object's lifecycle.\n- `self.name`: This attribute is initialized as an empty string. It is intended to hold the name associated with the SigData instance, which can be populated with a meaningful string later.\n- `self.trusted`: This attribute is set to False, indicating that the instance is not trusted by default. This boolean value may be used to determine the trustworthiness of the data represented by the instance.\n- `self.status`: This attribute is also initialized as an empty string. It is meant to represent the current status of the SigData instance, which can be updated as needed.\n\nOverall, the __init__ function establishes a clean state for the SigData object, ensuring that all attributes are defined and ready for use.\n\n**Note**: It is important to remember that this constructor does not take any parameters, meaning that any specific values for the attributes must be set after the object is instantiated. Users of this class should ensure that they assign appropriate values to these attributes to avoid operating with default states that may not be suitable for their use case."
      ],
      "code_start_line": 164,
      "code_end_line": 168,
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def __init__(self):\n        self.key = None\n        self.name = \"\"\n        self.trusted = False\n        self.status = \"\"\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "__bool__",
      "md_content": [
        "**__bool__**: The function of __bool__ is to determine the truthiness of an instance of the SigData class.\n\n**parameters**: The parameters of this Function.\n· There are no parameters for this function.\n\n**Code Description**: The __bool__ function is a special method in Python that is used to define the truth value of an object. In this implementation, the function checks if the attribute `key` of the instance is not `None`. If `self.key` is not `None`, the function returns `True`, indicating that the instance is considered \"truthy\". Conversely, if `self.key` is `None`, the function returns `False`, indicating that the instance is considered \"falsy\". This behavior allows instances of the SigData class to be used in conditional statements and boolean contexts, providing a clear and intuitive way to evaluate the state of the object based on the presence of the `key` attribute.\n\n**Note**: It is important to ensure that the `key` attribute is properly initialized before using this method, as its value directly affects the truthiness of the instance. If `key` is expected to be `None` at certain times, the behavior of the instance in boolean contexts should be understood accordingly.\n\n**Output Example**: \n- If an instance of SigData has `key` set to a valid value (e.g., `key = \"some_value\"`), calling `bool(instance)` will return `True`.\n- If `key` is set to `None` (e.g., `key = None`), calling `bool(instance)` will return `False`."
      ],
      "code_start_line": 170,
      "code_end_line": 171,
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def __bool__(self):\n        return self.key is not None\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "__repr__",
      "md_content": [
        "**__repr__**: The function of __repr__ is to provide a string representation of the SigData object.\n\n**parameters**: The parameters of this Function.\n· There are no parameters for this function.\n\n**Code Description**: The __repr__ method is a special method in Python that is used to define a string representation for an instance of a class. In this implementation, the method returns a formatted string that includes the key attributes of the SigData object. Specifically, it returns a string that contains the class name \"SigData\" followed by the values of the object's attributes: `key`, `name`, `trusted`, and `status`. The attributes are formatted using the `%r` format specifier, which calls the `repr()` function on the attributes, ensuring that they are represented in a way that is unambiguous and suitable for debugging. The `trusted` attribute is formatted as a boolean value, while the others are represented in their respective formats.\n\n**Note**: It is important to ensure that the attributes `key`, `name`, `trusted`, and `status` are defined within the SigData class for this method to function correctly. This method is particularly useful for debugging and logging, as it provides a clear and concise representation of the object.\n\n**Output Example**: An example of the output from this method could be:\n\"SigData('12345', 'Sample Data', trusted=True, status='active')\""
      ],
      "code_start_line": 173,
      "code_end_line": 176,
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def __repr__(self):\n        return (\n            \"SigData(%r, %r, trusted=%s, status=%r)\" %\n            (self.key, self.name, self.trusted, self.status))\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "parse_gpg_result",
      "md_content": [
        "**parse_gpg_result**: The function of parse_gpg_result is to parse the output from GPG and return categorized lists of good, unknown, and bad signatures.\n\n**parameters**: The parameters of this Function.\n· output: A list of strings representing the lines of output from GPG.\n\n**Code Description**: The parse_gpg_result function processes the output from GPG, which is expected to contain information about various signatures associated with a file. It categorizes these signatures into three distinct lists: good signatures, unknown signatures, and bad signatures. The function begins by initializing three lists to hold instances of the SigData class, which encapsulates the details of each signature.\n\nThe function defines a nested helper function, line_begins_with, which checks if a given line starts with a specific pattern. This is crucial for ensuring that the parser only processes lines that are relevant and prevents malicious input from affecting the parsing logic.\n\nAs the function iterates through each line of the GPG output, it uses regular expressions to identify and categorize the signatures based on specific prefixes such as \"NEWSIG\", \"GOODSIG\", \"BADSIG\", and others. For each identified signature, it populates the attributes of a SigData instance, including the key, name, trust status, and any relevant status messages (e.g., \"expired\" or \"revoked\"). \n\nAt the end of the parsing process, the function checks that the total number of resolved signatures matches the number of signatures found in the output. If there is a discrepancy, it raises a RuntimeError to alert the caller of the issue.\n\nThe parse_gpg_result function is called by the check_multisig function, which is responsible for verifying signatures against a given file. After obtaining the output from GPG, check_multisig calls parse_gpg_result to categorize the signatures, allowing it to handle unknown signatures appropriately, such as prompting the user to retrieve missing keys.\n\n**Note**: When using parse_gpg_result, it is essential to ensure that the output provided is correctly formatted as expected by the function. Any deviations in the output format may lead to incorrect parsing or runtime errors.\n\n**Output Example**: The return value of parse_gpg_result could look like this:\n(\n    [SigData('A1B2C3D4', 'John Doe', trusted=True, status='')],\n    [SigData('E5F6G7H8', 'Unknown Entity', trusted=False, status='')],\n    [SigData('I9J0K1L2', 'Malicious Entity', trusted=False, status='revoked')]\n)"
      ],
      "code_start_line": 179,
      "code_end_line": 244,
      "params": [
        "output"
      ],
      "have_return": true,
      "code_content": "def parse_gpg_result(\n    output: t.List[str]\n) -> t.Tuple[t.List[SigData], t.List[SigData], t.List[SigData]]:\n    \"\"\"Returns good, unknown, and bad signatures from GPG stdout.\"\"\"\n    good_sigs: t.List[SigData] = []\n    unknown_sigs: t.List[SigData] = []\n    bad_sigs: t.List[SigData] = []\n    total_resolved_sigs = 0\n\n    # Ensure that all lines we match on include a prefix that prevents malicious input\n    # from fooling the parser.\n    def line_begins_with(patt: str, line: str) -> t.Optional[re.Match]:\n        return re.match(r'^(\\[GNUPG:\\])\\s+' + patt, line)\n\n    curr_sigs = unknown_sigs\n    curr_sigdata = SigData()\n\n    for line in output:\n        if line_begins_with(r\"NEWSIG(?:\\s|$)\", line):\n            total_resolved_sigs += 1\n            if curr_sigdata:\n                curr_sigs.append(curr_sigdata)\n                curr_sigdata = SigData()\n            newsig_split = line.split()\n            if len(newsig_split) == 3:\n                curr_sigdata.name = newsig_split[2]\n\n        elif line_begins_with(r\"GOODSIG(?:\\s|$)\", line):\n            curr_sigdata.key, curr_sigdata.name = line.split(maxsplit=3)[2:4]\n            curr_sigs = good_sigs\n\n        elif line_begins_with(r\"EXPKEYSIG(?:\\s|$)\", line):\n            curr_sigdata.key, curr_sigdata.name = line.split(maxsplit=3)[2:4]\n            curr_sigs = good_sigs\n            curr_sigdata.status = \"expired\"\n\n        elif line_begins_with(r\"REVKEYSIG(?:\\s|$)\", line):\n            curr_sigdata.key, curr_sigdata.name = line.split(maxsplit=3)[2:4]\n            curr_sigs = good_sigs\n            curr_sigdata.status = \"revoked\"\n\n        elif line_begins_with(r\"BADSIG(?:\\s|$)\", line):\n            curr_sigdata.key, curr_sigdata.name = line.split(maxsplit=3)[2:4]\n            curr_sigs = bad_sigs\n\n        elif line_begins_with(r\"ERRSIG(?:\\s|$)\", line):\n            curr_sigdata.key, _, _, _, _, _ = line.split()[2:8]\n            curr_sigs = unknown_sigs\n\n        elif line_begins_with(r\"TRUST_(UNDEFINED|NEVER)(?:\\s|$)\", line):\n            curr_sigdata.trusted = False\n\n        elif line_begins_with(r\"TRUST_(MARGINAL|FULLY|ULTIMATE)(?:\\s|$)\", line):\n            curr_sigdata.trusted = True\n\n    # The last one won't have been added, so add it now\n    assert curr_sigdata\n    curr_sigs.append(curr_sigdata)\n\n    all_found = len(good_sigs + bad_sigs + unknown_sigs)\n    if all_found != total_resolved_sigs:\n        raise RuntimeError(\n            f\"failed to evaluate all signatures: found {all_found} \"\n            f\"but expected {total_resolved_sigs}\")\n\n    return (good_sigs, unknown_sigs, bad_sigs)\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "dataset/verify.py/check_multisig"
      ],
      "reference_who": [
        "dataset/verify.py/SigData"
      ],
      "special_reference_type": [
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "line_begins_with",
      "md_content": [
        "**line_begins_with**: The function of line_begins_with is to determine if a given line starts with a specific pattern prefixed by the string \"[GNUPG:]\".\n\n**parameters**: The parameters of this Function.\n· parameter1: patt - A string representing the pattern that the line should match after the \"[GNUPG:]\" prefix.  \n· parameter2: line - A string that represents the line to be checked against the specified pattern.\n\n**Code Description**: The line_begins_with function utilizes the re.match method from the regular expression module (re) to check if the input line begins with the specified pattern. The function constructs a regular expression that looks for the exact string \"[GNUPG:]\" followed by one or more whitespace characters and then the provided pattern (patt). The caret (^) in the regular expression signifies that the match must occur at the start of the line. If the line matches this constructed pattern, re.match returns a match object; otherwise, it returns None. This function is particularly useful for parsing lines of text that are expected to follow a specific format, such as log entries from GnuPG.\n\n**Note**: It is important to ensure that the pattern provided in the patt parameter is a valid regular expression. Additionally, the function only checks for matches at the beginning of the line, so any content before \"[GNUPG:]\" will result in no match.\n\n**Output Example**: If the line is \"[GNUPG:] key 12345\" and the pattern is \"key\", the function will return a match object. If the line is \"key 12345\" (without the \"[GNUPG:]\" prefix), the function will return None."
      ],
      "code_start_line": 190,
      "code_end_line": 191,
      "params": [
        "patt",
        "line"
      ],
      "have_return": true,
      "code_content": "    def line_begins_with(patt: str, line: str) -> t.Optional[re.Match]:\n        return re.match(r'^(\\[GNUPG:\\])\\s+' + patt, line)\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "files_are_equal",
      "md_content": [
        "**files_are_equal**: The function of files_are_equal is to compare the contents of two files and determine if they are identical.\n\n**parameters**: The parameters of this Function.\n· filename1: A string representing the path to the first file to be compared.\n· filename2: A string representing the path to the second file to be compared.\n\n**Code Description**: The files_are_equal function opens two files in binary mode and reads their contents to compare them for equality. If the contents are identical, the function returns True. If the contents differ, it proceeds to open both files in text mode, reads their lines, and generates a unified diff of the differences using the difflib module. This diff is then indented for better readability and logged as a warning message, indicating the files that were found to be different. The function ultimately returns False in this case.\n\nThis function is called by the get_files_from_hosts_and_compare function, which is responsible for retrieving files from multiple hosts and ensuring that they are identical. After downloading files from the specified hosts, get_files_from_hosts_and_compare invokes files_are_equal to compare each downloaded file with the next one in the list. If any pair of files is found to be unequal, an error is logged, and the function returns a specific return code indicating that the files are not equal.\n\nThe use of files_are_equal is crucial in maintaining data integrity when files are fetched from different sources. By ensuring that the contents of the files match, it helps prevent issues that may arise from discrepancies in the data.\n\n**Note**: When using the files_are_equal function, it is important to ensure that the file paths provided are correct and that the files are accessible. The function assumes that the files can be opened without any permission issues. Additionally, the function handles files in both binary and text modes, which is essential for accurately comparing different types of files.\n\n**Output Example**: If the two files being compared are identical, the function will return:\n```\nTrue\n```\nIf the files differ, the function will log a warning and return:\n```\nFalse\n```"
      ],
      "code_start_line": 247,
      "code_end_line": 264,
      "params": [
        "filename1",
        "filename2"
      ],
      "have_return": true,
      "code_content": "def files_are_equal(filename1, filename2):\n    with open(filename1, 'rb') as file1:\n        contents1 = file1.read()\n    with open(filename2, 'rb') as file2:\n        contents2 = file2.read()\n    eq = contents1 == contents2\n\n    if not eq:\n        with open(filename1, 'r', encoding='utf-8') as f1, \\\n                open(filename2, 'r', encoding='utf-8') as f2:\n            f1lines = f1.readlines()\n            f2lines = f2.readlines()\n\n            diff = indent(\n                ''.join(difflib.unified_diff(f1lines, f2lines)))\n            log.warning(f\"found diff in files ({filename1}, {filename2}):\\n{diff}\\n\")\n\n    return eq\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "dataset/verify.py/get_files_from_hosts_and_compare"
      ],
      "reference_who": [
        "dataset/verify.py/indent"
      ],
      "special_reference_type": [
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "get_files_from_hosts_and_compare",
      "md_content": [
        "**get_files_from_hosts_and_compare**: The function of get_files_from_hosts_and_compare is to retrieve the same file from multiple hosts and ensure that they have identical contents.\n\n**parameters**: The parameters of this Function.\n· hosts: A list of strings representing the host addresses from which the file will be downloaded. The first host is treated as the primary host.\n· path: A string representing the path to the file on the remote hosts.\n· filename: A string representing the name of the file to be saved locally.\n· require_all: A boolean indicating whether all hosts must successfully provide the file for the operation to be considered successful. Defaults to False.\n\n**Code Description**: The get_files_from_hosts_and_compare function is designed to facilitate the retrieval of a specified file from multiple remote hosts and to verify that the contents of the downloaded files are identical. The function begins by asserting that more than one host is provided, as it requires at least one primary host and one or more additional hosts for comparison.\n\nThe primary host is defined as the first entry in the hosts list, and the function constructs a URL for the file to be downloaded from this host. It utilizes the download_with_wget function to attempt to download the file. If the download from the primary host fails, an error is logged, and the function returns a specific ReturnCode indicating the failure.\n\nSubsequently, the function iterates over the remaining hosts, downloading the same file from each. If the require_all parameter is set to True and any of the additional hosts fail to provide the file, the function logs an error and returns a ReturnCode indicating that a file is missing from one host. If a host fails to provide the file but require_all is False, a warning is logged, and the function continues with the files obtained from the primary host.\n\nOnce all files have been downloaded, the function compares the contents of the downloaded files using the files_are_equal function. If any pair of files is found to be different, an error is logged, and the function returns a ReturnCode indicating that the files are not equal. If all files are confirmed to be identical, the function returns a ReturnCode indicating success.\n\nThis function is called by the verify_published_handler function, which is responsible for orchestrating the verification of published binaries. In this context, get_files_from_hosts_and_compare is used to retrieve and compare signature files and checksum files from multiple hosts, ensuring that the verification process is based on consistent and reliable data.\n\n**Note**: When using the get_files_from_hosts_and_compare function, it is essential to ensure that the provided hosts are valid and reachable. Additionally, the path and filename parameters must be correctly specified to avoid download failures. The function assumes that the files can be accessed without permission issues on the remote hosts.\n\n**Output Example**: A possible return value of the function could be ReturnCode.SUCCESS, indicating that all files were successfully retrieved and verified as identical. If there was a failure in downloading from the primary host, the return value could be ReturnCode.FILE_GET_FAILED."
      ],
      "code_start_line": 267,
      "code_end_line": 326,
      "params": [
        "hosts",
        "path",
        "filename",
        "require_all"
      ],
      "have_return": true,
      "code_content": "def get_files_from_hosts_and_compare(\n    hosts: t.List[str], path: str, filename: str, require_all: bool = False\n) -> ReturnCode:\n    \"\"\"\n    Retrieve the same file from a number of hosts and ensure they have the same contents.\n    The first host given will be treated as the \"primary\" host, and is required to succeed.\n\n    Args:\n        filename: for writing the file locally.\n    \"\"\"\n    assert len(hosts) > 1\n    primary_host = hosts[0]\n    other_hosts = hosts[1:]\n    got_files = []\n\n    def join_url(host: str) -> str:\n        return host.rstrip('/') + '/' + path.lstrip('/')\n\n    url = join_url(primary_host)\n    success, output = download_with_wget(url, filename)\n    if not success:\n        log.error(\n            f\"couldn't fetch file ({url}). \"\n            \"Have you specified the version number in the following format?\\n\"\n            f\"{VERSION_FORMAT} \"\n            f\"(example: {VERSION_EXAMPLE})\\n\"\n            f\"wget output:\\n{indent(output)}\")\n        return ReturnCode.FILE_GET_FAILED\n    else:\n        log.info(f\"got file {url} as {filename}\")\n        got_files.append(filename)\n\n    for i, host in enumerate(other_hosts):\n        url = join_url(host)\n        fname = filename + f'.{i + 2}'\n        success, output = download_with_wget(url, fname)\n\n        if require_all and not success:\n            log.error(\n                f\"{host} failed to provide file ({url}), but {primary_host} did?\\n\"\n                f\"wget output:\\n{indent(output)}\")\n            return ReturnCode.FILE_MISSING_FROM_ONE_HOST\n        elif not success:\n            log.warning(\n                f\"{host} failed to provide file ({url}). \"\n                f\"Continuing based solely upon {primary_host}.\")\n        else:\n            log.info(f\"got file {url} as {fname}\")\n            got_files.append(fname)\n\n    for i, got_file in enumerate(got_files):\n        if got_file == got_files[-1]:\n            break  # break on last file, nothing after it to compare to\n\n        compare_to = got_files[i + 1]\n        if not files_are_equal(got_file, compare_to):\n            log.error(f\"files not equal: {got_file} and {compare_to}\")\n            return ReturnCode.FILES_NOT_EQUAL\n\n    return ReturnCode.SUCCESS\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "dataset/verify.py/verify_published_handler"
      ],
      "reference_who": [
        "dataset/verify.py/ReturnCode",
        "dataset/verify.py/indent",
        "dataset/verify.py/download_with_wget",
        "dataset/verify.py/files_are_equal"
      ],
      "special_reference_type": [
        false,
        false,
        false,
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "join_url",
      "md_content": [
        "**join_url**: The function of join_url is to concatenate a host URL with a specified path, ensuring proper formatting by removing any trailing slashes from the host and leading slashes from the path.\n\n**parameters**: The parameters of this Function.\n· host: A string representing the base URL or host to which the path will be appended.\n\n**Code Description**: The join_url function takes a single parameter, host, which is expected to be a string. The function performs two main operations to ensure that the resulting URL is correctly formatted. First, it uses the rstrip('/') method on the host string to remove any trailing slashes. This is important because having a trailing slash can lead to double slashes when concatenating with the path. Next, it concatenates the cleaned host with a forward slash ('/') and the path, which is processed using lstrip('/') to remove any leading slashes. This ensures that the path does not start with a slash, preventing the formation of an incorrect URL structure. The final result is a well-formed URL that combines the host and the path.\n\n**Note**: It is essential to ensure that the host parameter is a valid URL format. Additionally, the path variable should be defined in the scope where the join_url function is called, as it is not passed as a parameter. Users should be cautious about the input values to avoid unexpected URL formats.\n\n**Output Example**: If the host is \"http://example.com/\" and the path is \"/api/v1/resource\", the function will return \"http://example.com/api/v1/resource\"."
      ],
      "code_start_line": 282,
      "code_end_line": 283,
      "params": [
        "host"
      ],
      "have_return": true,
      "code_content": "    def join_url(host: str) -> str:\n        return host.rstrip('/') + '/' + path.lstrip('/')\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "check_multisig",
      "md_content": [
        "**check_multisig**: The function of check_multisig is to verify the signatures of a specified sums file against a provided GPG signature file and return the results of the verification process.\n\n**parameters**: The parameters of this Function.\n· sums_file: A string representing the path to the file containing the checksums that need to be verified.\n· sigfilename: A string representing the path to the GPG signature file associated with the sums file.\n· args: An instance of argparse.Namespace containing command-line arguments that influence the behavior of the function.\n\n**Code Description**: The check_multisig function is responsible for verifying the authenticity of signatures associated with a sums file using GPG (GNU Privacy Guard). The function begins by calling the verify_with_gpg function, which executes the GPG command to check the signatures and returns a status code along with the output from GPG. The output is then processed to categorize the signatures into three lists: good, unknown, and bad, using the parse_gpg_result function.\n\nIf there are any unknown signatures and the user has specified the option to import keys, the function prompts the user for confirmation to retrieve the unknown keys from a keyserver. If the user agrees, it attempts to retrieve the keys using a subprocess call to GPG. After retrieving the keys, the function re-verifies the signatures by calling verify_with_gpg again and updates the lists of good, unknown, and bad signatures.\n\nThe function returns a tuple containing the GPG return code, the output from GPG, and the categorized lists of good, unknown, and bad signatures. This structured output allows the calling function, such as verify_shasums_signature, to make informed decisions based on the verification results. In verify_shasums_signature, the results from check_multisig are used to determine if there are enough trusted signatures to meet a specified threshold, logging appropriate messages based on the verification outcome.\n\n**Note**: It is essential to ensure that the GPG environment is properly set up and that the necessary keys are accessible for the verification process to succeed. Additionally, the function assumes that the input files are correctly formatted and accessible.\n\n**Output Example**: A possible return value from the function could be:\n(0, \"gpg: Good signature from 'John Doe <john@example.com>'\", \n [SigData('A1B2C3D4', 'John Doe', trusted=True, status='')], \n [SigData('E5F6G7H8', 'Unknown Entity', trusted=False, status='')], \n [SigData('I9J0K1L2', 'Malicious Entity', trusted=False, status='revoked')])"
      ],
      "code_start_line": 329,
      "code_end_line": 356,
      "params": [
        "sums_file",
        "sigfilename",
        "args"
      ],
      "have_return": true,
      "code_content": "def check_multisig(sums_file: str, sigfilename: str, args: argparse.Namespace) -> t.Tuple[int, str, t.List[SigData], t.List[SigData], t.List[SigData]]:\n    # check signature\n    #\n    # We don't write output to a file because this command will almost certainly\n    # fail with GPG exit code '2' (and so not writing to --output) because of the\n    # likely presence of multiple untrusted signatures.\n    retval, output = verify_with_gpg(sums_file, sigfilename)\n\n    if args.verbose:\n        log.info(f\"gpg output:\\n{indent(output)}\")\n\n    good, unknown, bad = parse_gpg_result(output.splitlines())\n\n    if unknown and args.import_keys:\n        # Retrieve unknown keys and then try GPG again.\n        for unsig in unknown:\n            if prompt_yn(f\" ? Retrieve key {unsig.key} ({unsig.name})? (y/N) \"):\n                ran = subprocess.run(\n                    [\"gpg\", \"--keyserver\", args.keyserver, \"--recv-keys\", unsig.key])\n\n                if ran.returncode != 0:\n                    log.warning(f\"failed to retrieve key {unsig.key}\")\n\n        # Reparse the GPG output now that we have more keys\n        retval, output = verify_with_gpg(sums_file, sigfilename)\n        good, unknown, bad = parse_gpg_result(output.splitlines())\n\n    return retval, output, good, unknown, bad\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "dataset/verify.py/verify_shasums_signature"
      ],
      "reference_who": [
        "dataset/verify.py/indent",
        "dataset/verify.py/verify_with_gpg",
        "dataset/verify.py/SigData",
        "dataset/verify.py/parse_gpg_result",
        "dataset/verify.py/prompt_yn"
      ],
      "special_reference_type": [
        false,
        false,
        true,
        false,
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "prompt_yn",
      "md_content": [
        "**prompt_yn**: The function of prompt_yn is to prompt the user for a yes or no input and return a boolean value based on the user's response.\n\n**parameters**: The parameters of this Function.\n· prompt: A string that represents the message displayed to the user when asking for input.\n\n**Code Description**: The prompt_yn function is designed to solicit a yes ('y') or no ('n') response from the user. It begins by initializing a variable `got` to an empty string. The function then enters a while loop that continues until the user provides a valid input, which must be either 'y' or 'n'. Inside the loop, the function calls the input function with the provided prompt, converting the user's response to lowercase to ensure case insensitivity. Once a valid response is received, the function returns True if the response is 'y' and False if it is 'n'.\n\nThis function is utilized within the check_multisig function, which is responsible for verifying signatures associated with a file. Specifically, prompt_yn is called when the program encounters an unknown key during the verification process and the user has opted to import keys. The function prompts the user to confirm whether they wish to retrieve the unknown key from a keyserver. The boolean result from prompt_yn determines whether the program will attempt to retrieve the key using a subprocess call to GPG. This interaction illustrates how prompt_yn facilitates user decision-making in the signature verification workflow.\n\n**Note**: It is important to ensure that the prompt provided to the user is clear and concise, as this will directly affect the user's ability to respond correctly. Additionally, the function only accepts 'y' or 'n' as valid inputs, and any other input will result in the prompt being displayed again.\n\n**Output Example**: If the user inputs 'y', the function will return True. If the user inputs 'n', the function will return False. For any other input, the prompt will be displayed again until a valid response is received."
      ],
      "code_start_line": 359,
      "code_end_line": 364,
      "params": [
        "prompt"
      ],
      "have_return": true,
      "code_content": "def prompt_yn(prompt) -> bool:\n    \"\"\"Return true if the user inputs 'y'.\"\"\"\n    got = ''\n    while got not in ['y', 'n']:\n        got = input(prompt).lower()\n    return got == 'y'\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "dataset/verify.py/check_multisig"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "verify_shasums_signature",
      "md_content": [
        "**verify_shasums_signature**: The function of verify_shasums_signature is to verify the signatures of a SHA256SUMS file against a provided GPG signature file and ensure that the number of trusted signatures meets a specified threshold.\n\n**parameters**: The parameters of this Function.\n· signature_file_path: A string representing the path to the GPG signature file associated with the SHA256SUMS file.  \n· sums_file_path: A string representing the path to the file containing the checksums that need to be verified.  \n· args: An instance of argparse.Namespace containing command-line arguments that influence the behavior of the function, including the minimum number of good signatures required for verification.\n\n**Code Description**: The verify_shasums_signature function is responsible for validating the integrity of a SHA256SUMS file by checking its signatures using GPG (GNU Privacy Guard). It begins by defining the minimum number of good signatures required and the allowed GPG return codes for successful verification. The function then calls check_multisig, which executes the GPG command to verify the signatures and returns a status code along with categorized lists of good, unknown, and bad signatures.\n\nUpon receiving the results from check_multisig, the function checks the GPG return code against the allowed codes. If the return code indicates an integrity failure or an unexpected error, it logs the appropriate error messages and returns a corresponding ReturnCode indicating the failure.\n\nThe function then assesses which signatures are trusted based on the command-line arguments provided. It tallies the good signatures, distinguishing between trusted and untrusted signatures. If the number of trusted signatures is below the specified threshold, it logs a warning and returns a ReturnCode indicating that there are not enough good signatures.\n\nFor each signature, the function logs its status, including good signatures, expired keys, bad signatures, and unknown signatures. Finally, it returns a tuple containing the overall status of the verification process, along with lists of trusted good signatures, untrusted good signatures, unknown signatures, and bad signatures.\n\nThis function is called by other functions such as verify_published_handler and verify_binaries_handler, which handle the overall verification process for published binaries and specific binary files, respectively. These calling functions rely on verify_shasums_signature to ensure that the SHA256SUMS file is properly verified before proceeding with further operations, such as downloading binaries or verifying their hashes.\n\n**Note**: When using the verify_shasums_signature function, it is essential to ensure that the GPG environment is correctly configured and that the necessary keys are available for the verification process to succeed. Additionally, the function assumes that the input files are correctly formatted and accessible.\n\n**Output Example**: A possible return value from the function could be:\n(ReturnCode.SUCCESS, \n [SigData('A1B2C3D4', 'John Doe', trusted=True, status='')], \n [SigData('E5F6G7H8', 'Unknown Entity', trusted=False, status='')], \n [SigData('I9J0K1L2', 'Malicious Entity', trusted=False, status='revoked')])"
      ],
      "code_start_line": 366,
      "code_end_line": 429,
      "params": [
        "signature_file_path",
        "sums_file_path",
        "args"
      ],
      "have_return": true,
      "code_content": "def verify_shasums_signature(\n    signature_file_path: str, sums_file_path: str, args: argparse.Namespace\n) -> t.Tuple[\n   ReturnCode, t.List[SigData], t.List[SigData], t.List[SigData], t.List[SigData]\n]:\n    min_good_sigs = args.min_good_sigs\n    gpg_allowed_codes = [0, 2]  # 2 is returned when untrusted signatures are present.\n\n    gpg_retval, gpg_output, good, unknown, bad = check_multisig(sums_file_path, signature_file_path, args)\n\n    if gpg_retval not in gpg_allowed_codes:\n        if gpg_retval == 1:\n            log.critical(f\"Bad signature (code: {gpg_retval}).\")\n        else:\n            log.critical(f\"unexpected GPG exit code ({gpg_retval})\")\n\n        log.error(f\"gpg output:\\n{indent(gpg_output)}\")\n        return (ReturnCode.INTEGRITY_FAILURE, [], [], [], [])\n\n    # Decide which keys we trust, though not \"trust\" in the GPG sense, but rather\n    # which pubkeys convince us that this sums file is legitimate. In other words,\n    # which pubkeys within the Bitcoin community do we trust for the purposes of\n    # binary verification?\n    trusted_keys = set()\n    if args.trusted_keys:\n        trusted_keys |= set(args.trusted_keys.split(','))\n\n    # Tally signatures and make sure we have enough goods to fulfill\n    # our threshold.\n    good_trusted = [sig for sig in good if sig.trusted or sig.key in trusted_keys]\n    good_untrusted = [sig for sig in good if sig not in good_trusted]\n    num_trusted = len(good_trusted) + len(good_untrusted)\n    log.info(f\"got {num_trusted} good signatures\")\n\n    if num_trusted < min_good_sigs:\n        log.info(\"Maybe you need to import \"\n                  f\"(`gpg --keyserver {args.keyserver} --recv-keys <key-id>`) \"\n                  \"some of the following keys: \")\n        log.info('')\n        for sig in unknown:\n            log.info(f\"    {sig.key} ({sig.name})\")\n        log.info('')\n        log.error(\n            \"not enough trusted sigs to meet threshold \"\n            f\"({num_trusted} vs. {min_good_sigs})\")\n\n        return (ReturnCode.NOT_ENOUGH_GOOD_SIGS, [], [], [], [])\n\n    for sig in good_trusted:\n        log.info(f\"GOOD SIGNATURE: {sig}\")\n\n    for sig in good_untrusted:\n        log.info(f\"GOOD SIGNATURE (untrusted): {sig}\")\n\n    for sig in [sig for sig in good if sig.status == 'expired']:\n        log.warning(f\"key {sig.key} for {sig.name} is expired\")\n\n    for sig in bad:\n        log.warning(f\"BAD SIGNATURE: {sig}\")\n\n    for sig in unknown:\n        log.warning(f\"UNKNOWN SIGNATURE: {sig}\")\n\n    return (ReturnCode.SUCCESS, good_trusted, good_untrusted, unknown, bad)\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "dataset/verify.py/verify_published_handler",
        "dataset/verify.py/verify_binaries_handler"
      ],
      "reference_who": [
        "dataset/verify.py/ReturnCode",
        "dataset/verify.py/indent",
        "dataset/verify.py/SigData",
        "dataset/verify.py/check_multisig"
      ],
      "special_reference_type": [
        false,
        false,
        false,
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "parse_sums_file",
      "md_content": [
        "**parse_sums_file**: The function of parse_sums_file is to extract hashes and filenames of binaries to verify from a specified hash file.\n\n**parameters**: The parameters of this Function.\n· sums_file_path: A string representing the path to the hash file that contains the hashes and binary filenames.\n· filename_filter: A list of strings used to filter the filenames extracted from the hash file. If empty, all entries will be returned.\n\n**Code Description**: The parse_sums_file function reads a hash file specified by the sums_file_path parameter. Each line in this file is expected to contain a hash followed by a binary filename, formatted as \"<hash> <binary_filename>\". The function processes the file line by line, splitting each line into its constituent parts. It checks if the filename_filter is empty or if any of the filters are present in the line. If either condition is met, the hash and filename are included in the returned list. The function ultimately returns a list of lists, where each inner list contains a hash and its corresponding binary filename.\n\nThis function is called by two different handlers within the dataset/verify.py module: verify_published_handler and verify_binaries_handler. In verify_published_handler, it is used to extract hashes and filenames after verifying the signature of the SHA256SUMS file. The extracted data is then filtered to remove binaries that are not hosted by the specified source. In verify_binaries_handler, parse_sums_file is utilized to extract hashes and filenames from a user-specified sums file, which are then verified against the provided binaries. Both handlers rely on the output of parse_sums_file to ensure that the correct binaries are being processed for verification.\n\n**Note**: It is important to ensure that the sums_file_path provided points to a valid file with the expected format. If the filename_filter is not used, the function will return all entries from the hash file, which may include unwanted binaries.\n\n**Output Example**: An example of the return value from parse_sums_file could be:\n```\n[\n    ['abc123hash', 'binary_file_1'],\n    ['def456hash', 'binary_file_2']\n]\n``` \nThis output indicates that two binaries, 'binary_file_1' and 'binary_file_2', have been extracted along with their corresponding hashes from the specified hash file."
      ],
      "code_start_line": 432,
      "code_end_line": 436,
      "params": [
        "sums_file_path",
        "filename_filter"
      ],
      "have_return": true,
      "code_content": "def parse_sums_file(sums_file_path: str, filename_filter: t.List[str]) -> t.List[t.List[str]]:\n    # extract hashes/filenames of binaries to verify from hash file;\n    # each line has the following format: \"<hash> <binary_filename>\"\n    with open(sums_file_path, 'r', encoding='utf8') as hash_file:\n        return [line.split()[:2] for line in hash_file if len(filename_filter) == 0 or any(f in line for f in filename_filter)]\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "dataset/verify.py/verify_published_handler",
        "dataset/verify.py/verify_binaries_handler"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "verify_binary_hashes",
      "md_content": [
        "**verify_binary_hashes**: The function of verify_binary_hashes is to verify the integrity of binary files by comparing their calculated SHA256 hashes against expected values.\n\n**parameters**: The parameters of this Function.\n· hashes_to_verify: A list of lists, where each inner list contains an expected hash and the corresponding binary filename to verify.\n\n**Code Description**: The verify_binary_hashes function is designed to ensure the integrity of binary files by calculating their SHA256 hashes and comparing them to expected values provided in the hashes_to_verify parameter. The function begins by initializing two lists: offending_files, which will store the names of files that fail the hash check, and files_to_hashes, which will map binary filenames to their calculated hashes.\n\nThe function iterates over each pair of expected hash and binary filename from the hashes_to_verify list. For each binary file, it opens the file in binary read mode and computes its SHA256 hash using the sha256 function. If the calculated hash does not match the expected hash, the filename is added to the offending_files list. If the hashes match, the filename and its calculated hash are stored in the files_to_hashes dictionary.\n\nAfter processing all files, if there are any offending files, the function logs a critical error message detailing which files failed the integrity check and returns a tuple containing ReturnCode.INTEGRITY_FAILURE and the files_to_hashes dictionary. If all files pass the integrity check, the function returns a tuple with ReturnCode.SUCCESS and the files_to_hashes dictionary.\n\nThis function is called by verify_published_handler and verify_binaries_handler functions within the same module. In verify_published_handler, it is invoked after downloading binaries and verifying their signatures, ensuring that the downloaded files are intact and have not been tampered with. In verify_binaries_handler, it is called after verifying the signature of the SHA256SUMS file and extracting the hashes to verify, ensuring that the specified binaries match their expected hashes.\n\n**Note**: It is important to handle the return values of this function appropriately in the calling functions to ensure that any integrity failures are logged and managed correctly. This practice is crucial for maintaining the robustness of the verification process.\n\n**Output Example**: A possible return value of the function could be:\n- If all hashes match: (ReturnCode.SUCCESS, {'binary1': 'calculated_hash1', 'binary2': 'calculated_hash2'})\n- If there are integrity failures: (ReturnCode.INTEGRITY_FAILURE, {'binary1': 'calculated_hash1'})"
      ],
      "code_start_line": 439,
      "code_end_line": 458,
      "params": [
        "hashes_to_verify"
      ],
      "have_return": true,
      "code_content": "def verify_binary_hashes(hashes_to_verify: t.List[t.List[str]]) -> t.Tuple[ReturnCode, t.Dict[str, str]]:\n    offending_files = []\n    files_to_hashes = {}\n\n    for hash_expected, binary_filename in hashes_to_verify:\n        with open(binary_filename, 'rb') as binary_file:\n            hash_calculated = sha256(binary_file.read()).hexdigest()\n        if hash_calculated != hash_expected:\n            offending_files.append(binary_filename)\n        else:\n            files_to_hashes[binary_filename] = hash_calculated\n\n    if offending_files:\n        joined_files = '\\n'.join(offending_files)\n        log.critical(\n            \"Hashes don't match.\\n\"\n            f\"Offending files:\\n{joined_files}\")\n        return (ReturnCode.INTEGRITY_FAILURE, files_to_hashes)\n\n    return (ReturnCode.SUCCESS, files_to_hashes)\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "dataset/verify.py/verify_published_handler",
        "dataset/verify.py/verify_binaries_handler"
      ],
      "reference_who": [
        "dataset/verify.py/ReturnCode"
      ],
      "special_reference_type": [
        true
      ]
    },
    {
      "type": "FunctionDef",
      "name": "verify_published_handler",
      "md_content": [
        "**verify_published_handler**: The function of verify_published_handler is to verify the integrity and authenticity of published Bitcoin binaries based on a specified version.\n\n**parameters**: The parameters of this Function.\n· args: An instance of argparse.Namespace that contains command-line arguments, including the version of the Bitcoin release to verify, cleanup options, and host requirements for signature verification.\n\n**Code Description**: The verify_published_handler function orchestrates the process of verifying published Bitcoin binaries by performing several key operations. It begins by establishing a working directory in the system's temporary directory, specifically named according to the version provided in the arguments.\n\nThe function first attempts to parse the version string using the parse_version_string function, which extracts the base version, release candidate (if applicable), and operating system information. If parsing fails, it logs an error and returns a ReturnCode indicating a bad version.\n\nNext, the function constructs the remote directory path for the binaries and their associated signatures based on the parsed version information. It then creates the working directory and changes the current working directory to this new location.\n\nThe function retrieves signature files from specified hosts using the get_files_from_hosts_and_compare function, which ensures that the signatures are identical across the hosts. If the retrieval is unsuccessful, it returns the corresponding error code.\n\nFollowing this, the function checks if the version is suitable for multi-signature verification. If the version is below 22.0, it logs an error indicating that single signature verification is not supported and returns a bad version code.\n\nThe function then retrieves checksum files in a similar manner and verifies the integrity of the SHA256SUMS file against its signature using the verify_shasums_signature function. If this verification fails, it cleans up the working directory if necessary and returns the appropriate error code.\n\nOnce the signatures and checksums are verified, the function extracts the hashes and filenames of the binaries to be verified using the parse_sums_file function. It filters out any binaries that are not hosted by the specified source.\n\nThe function proceeds to download the binaries using the download_with_wget function, logging the status of each download. If any downloads fail, it returns an error code indicating the failure.\n\nFinally, the function verifies the integrity of the downloaded binaries by comparing their hashes against the expected values using the verify_binary_hashes function. If all checks pass, it outputs the results, either in JSON format or as a simple list of verified binaries, and returns a success code.\n\nThis function is called by the main function of the module, which sets up the command-line interface for the script. The main function defines a subcommand \"pub\" that links directly to verify_published_handler, allowing users to invoke this verification process through command-line arguments.\n\n**Note**: It is essential to ensure that the version string provided adheres to the expected format, and that the necessary network access is available to retrieve files from the specified hosts. Additionally, users should be aware of the cleanup option, which determines whether temporary files are removed after the verification process.\n\n**Output Example**: A possible return value of the function could be ReturnCode.SUCCESS, indicating that all binaries have been successfully verified, along with a printed list of verified binaries or a JSON output containing detailed verification results."
      ],
      "code_start_line": 461,
      "code_end_line": 567,
      "params": [
        "args"
      ],
      "have_return": true,
      "code_content": "def verify_published_handler(args: argparse.Namespace) -> ReturnCode:\n    WORKINGDIR = Path(tempfile.gettempdir()) / f\"bitcoin_verify_binaries.{args.version}\"\n\n    def cleanup():\n        log.info(\"cleaning up files\")\n        os.chdir(Path.home())\n        shutil.rmtree(WORKINGDIR)\n\n    # determine remote dir dependent on provided version string\n    try:\n        version_base, version_rc, os_filter = parse_version_string(args.version)\n        version_tuple = [int(i) for i in version_base.split('.')]\n    except Exception as e:\n        log.debug(e)\n        log.error(f\"unable to parse version; expected format is {VERSION_FORMAT}\")\n        log.error(f\"  e.g. {VERSION_EXAMPLE}\")\n        return ReturnCode.BAD_VERSION\n\n    remote_dir = f\"/bin/{VERSIONPREFIX}{version_base}/\"\n    if version_rc:\n        remote_dir += f\"test.{version_rc}/\"\n    remote_sigs_path = remote_dir + SIGNATUREFILENAME\n    remote_sums_path = remote_dir + SUMS_FILENAME\n\n    # create working directory\n    os.makedirs(WORKINGDIR, exist_ok=True)\n    os.chdir(WORKINGDIR)\n\n    hosts = [HOST1, HOST2]\n\n    got_sig_status = get_files_from_hosts_and_compare(\n        hosts, remote_sigs_path, SIGNATUREFILENAME, args.require_all_hosts)\n    if got_sig_status != ReturnCode.SUCCESS:\n        return got_sig_status\n\n    # Multi-sig verification is available after 22.0.\n    if version_tuple[0] < 22:\n        log.error(\"Version too old - single sig not supported. Use a previous \"\n                  \"version of this script from the repo.\")\n        return ReturnCode.BAD_VERSION\n\n    got_sums_status = get_files_from_hosts_and_compare(\n        hosts, remote_sums_path, SUMS_FILENAME, args.require_all_hosts)\n    if got_sums_status != ReturnCode.SUCCESS:\n        return got_sums_status\n\n    # Verify the signature on the SHA256SUMS file\n    sigs_status, good_trusted, good_untrusted, unknown, bad = verify_shasums_signature(SIGNATUREFILENAME, SUMS_FILENAME, args)\n    if sigs_status != ReturnCode.SUCCESS:\n        if sigs_status == ReturnCode.INTEGRITY_FAILURE:\n            cleanup()\n        return sigs_status\n\n    # Extract hashes and filenames\n    hashes_to_verify = parse_sums_file(SUMS_FILENAME, [os_filter])\n    if not hashes_to_verify:\n        log.error(\"no files matched the platform specified\")\n        return ReturnCode.NO_BINARIES_MATCH\n\n    # remove binaries that are known not to be hosted by bitcoincore.org\n    fragments_to_remove = ['-unsigned', '-debug', '-codesignatures']\n    for fragment in fragments_to_remove:\n        nobinaries = [i for i in hashes_to_verify if fragment in i[1]]\n        if nobinaries:\n            remove_str = ', '.join(i[1] for i in nobinaries)\n            log.info(\n                f\"removing *{fragment} binaries ({remove_str}) from verification \"\n                f\"since {HOST1} does not host *{fragment} binaries\")\n            hashes_to_verify = [i for i in hashes_to_verify if fragment not in i[1]]\n\n    # download binaries\n    for _, binary_filename in hashes_to_verify:\n        log.info(f\"downloading {binary_filename} to {WORKINGDIR}\")\n        success, output = download_with_wget(\n            HOST1 + remote_dir + binary_filename, binary_filename)\n\n        if not success:\n            log.error(\n                f\"failed to download {binary_filename}\\n\"\n                f\"wget output:\\n{indent(output)}\")\n            return ReturnCode.BINARY_DOWNLOAD_FAILED\n\n    # verify hashes\n    hashes_status, files_to_hashes = verify_binary_hashes(hashes_to_verify)\n    if hashes_status != ReturnCode.SUCCESS:\n        return hashes_status\n\n\n    if args.cleanup:\n        cleanup()\n    else:\n        log.info(f\"did not clean up {WORKINGDIR}\")\n\n    if args.json:\n        output = {\n            'good_trusted_sigs': [str(s) for s in good_trusted],\n            'good_untrusted_sigs': [str(s) for s in good_untrusted],\n            'unknown_sigs': [str(s) for s in unknown],\n            'bad_sigs': [str(s) for s in bad],\n            'verified_binaries': files_to_hashes,\n        }\n        print(json.dumps(output, indent=2))\n    else:\n        for filename in files_to_hashes:\n            print(f\"VERIFIED: {filename}\")\n\n    return ReturnCode.SUCCESS\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "dataset/verify.py/main"
      ],
      "reference_who": [
        "dataset/verify.py/ReturnCode",
        "dataset/verify.py/indent",
        "dataset/verify.py/parse_version_string",
        "dataset/verify.py/download_with_wget",
        "dataset/verify.py/get_files_from_hosts_and_compare",
        "dataset/verify.py/verify_shasums_signature",
        "dataset/verify.py/parse_sums_file",
        "dataset/verify.py/verify_binary_hashes"
      ],
      "special_reference_type": [
        true,
        false,
        false,
        false,
        false,
        false,
        false,
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "cleanup",
      "md_content": [
        "**cleanup**: The function of cleanup is to remove temporary files and reset the working directory.\n\n**parameters**: The parameters of this Function.\n· There are no parameters for this function.\n\n**Code Description**: The cleanup function is designed to perform a cleanup operation by removing temporary files and resetting the working environment. The function begins by logging an informational message indicating that the cleanup process is starting. This is done using the `log.info` method, which helps in tracking the execution flow and debugging if necessary.\n\nNext, the function changes the current working directory to the user's home directory using `os.chdir(Path.home())`. This step is crucial as it ensures that any subsequent file operations are performed in a known and safe location, preventing accidental modifications to files in other directories.\n\nFinally, the function deletes the directory specified by the `WORKINGDIR` variable and all its contents using `shutil.rmtree(WORKINGDIR)`. This method is powerful as it recursively removes a directory and all its files and subdirectories, effectively cleaning up any temporary files that may have been created during the execution of the program.\n\n**Note**: It is important to ensure that the `WORKINGDIR` variable is correctly defined and points to the intended directory before calling this function. Additionally, users should be cautious when using `shutil.rmtree` as it permanently deletes files and directories without recovery options."
      ],
      "code_start_line": 464,
      "code_end_line": 467,
      "params": [],
      "have_return": false,
      "code_content": "    def cleanup():\n        log.info(\"cleaning up files\")\n        os.chdir(Path.home())\n        shutil.rmtree(WORKINGDIR)\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "verify_binaries_handler",
      "md_content": [
        "**verify_binaries_handler**: The function of verify_binaries_handler is to verify the integrity and authenticity of specified binary files against a SHA256SUMS file and its associated GPG signature.\n\n**parameters**: The parameters of this Function.\n· args: An instance of argparse.Namespace containing command-line arguments that influence the behavior of the function, including paths to binary files, SHA256SUMS file, and signature file.\n\n**Code Description**: The verify_binaries_handler function is designed to handle the verification process of binary files by checking their signatures and hashes against a provided SHA256SUMS file. The function begins by creating a mapping of binary file names to their respective paths based on the input arguments. If a signature file is specified, it uses that; otherwise, it defaults to assuming the signature file is the SHA256SUMS file with an \".asc\" suffix.\n\nThe function then calls verify_shasums_signature to validate the signature of the SHA256SUMS file. If the signature verification fails, the function returns the corresponding ReturnCode. Following successful signature verification, it extracts the hashes and filenames from the SHA256SUMS file using the parse_sums_file function. If no matching hashes are found for the specified binaries, it logs an error and returns a ReturnCode indicating that no binaries match.\n\nNext, the function ensures that all specified binaries are accounted for by comparing the extracted hashes with the provided binaries. If any binaries are missing, it logs an error. The function then proceeds to verify the integrity of the binaries by calling verify_binary_hashes, which checks the calculated SHA256 hashes against the expected values. If any integrity checks fail, it returns the appropriate ReturnCode.\n\nDepending on the command-line arguments, the function outputs the results in either a human-readable format or as JSON. It concludes by returning ReturnCode.SUCCESS if all operations are successful.\n\nThis function is called by the main function within the same module, specifically when the \"bin\" command is invoked. It serves as a critical component in the verification process, ensuring that the binaries are both authentic and intact before they are used.\n\n**Note**: When using the verify_binaries_handler function, it is essential to provide valid paths for the SHA256SUMS file and the binaries to be verified. Additionally, the GPG environment must be correctly configured for signature verification to succeed.\n\n**Output Example**: A possible return value from the function could be:\nReturnCode.SUCCESS"
      ],
      "code_start_line": 570,
      "code_end_line": 634,
      "params": [
        "args"
      ],
      "have_return": true,
      "code_content": "def verify_binaries_handler(args: argparse.Namespace) -> ReturnCode:\n    binary_to_basename = {}\n    for file in args.binary:\n        binary_to_basename[PurePath(file).name] = file\n\n    sums_sig_path = None\n    if args.sums_sig_file:\n        sums_sig_path = Path(args.sums_sig_file)\n    else:\n        log.info(f\"No signature file specified, assuming it is {args.sums_file}.asc\")\n        sums_sig_path = Path(args.sums_file).with_suffix(\".asc\")\n\n    # Verify the signature on the SHA256SUMS file\n    sigs_status, good_trusted, good_untrusted, unknown, bad = verify_shasums_signature(str(sums_sig_path), args.sums_file, args)\n    if sigs_status != ReturnCode.SUCCESS:\n        return sigs_status\n\n    # Extract hashes and filenames\n    hashes_to_verify = parse_sums_file(args.sums_file, [k for k, n in binary_to_basename.items()])\n    if not hashes_to_verify:\n        log.error(f\"No files in {args.sums_file} match the specified binaries\")\n        return ReturnCode.NO_BINARIES_MATCH\n\n    # Make sure all files are accounted for\n    sums_file_path = Path(args.sums_file)\n    missing_files = []\n    files_to_hash = []\n    if len(binary_to_basename) > 0:\n        for file_hash, file in hashes_to_verify:\n            files_to_hash.append([file_hash, binary_to_basename[file]])\n            del binary_to_basename[file]\n        if len(binary_to_basename) > 0:\n            log.error(f\"Not all specified binaries are in {args.sums_file}\")\n            return ReturnCode.NO_BINARIES_MATCH\n    else:\n        log.info(f\"No binaries specified, assuming all files specified in {args.sums_file} are located relatively\")\n        for file_hash, file in hashes_to_verify:\n            file_path = Path(sums_file_path.parent.joinpath(file))\n            if file_path.exists():\n                files_to_hash.append([file_hash, str(file_path)])\n            else:\n                missing_files.append(file)\n\n    # verify hashes\n    hashes_status, files_to_hashes = verify_binary_hashes(files_to_hash)\n    if hashes_status != ReturnCode.SUCCESS:\n        return hashes_status\n\n    if args.json:\n        output = {\n            'good_trusted_sigs': [str(s) for s in good_trusted],\n            'good_untrusted_sigs': [str(s) for s in good_untrusted],\n            'unknown_sigs': [str(s) for s in unknown],\n            'bad_sigs': [str(s) for s in bad],\n            'verified_binaries': files_to_hashes,\n            \"missing_binaries\": missing_files,\n        }\n        print(json.dumps(output, indent=2))\n    else:\n        for filename in files_to_hashes:\n            print(f\"VERIFIED: {filename}\")\n        for filename in missing_files:\n            print(f\"MISSING: {filename}\")\n\n    return ReturnCode.SUCCESS\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "dataset/verify.py/main"
      ],
      "reference_who": [
        "dataset/verify.py/ReturnCode",
        "dataset/verify.py/verify_shasums_signature",
        "dataset/verify.py/parse_sums_file",
        "dataset/verify.py/verify_binary_hashes"
      ],
      "special_reference_type": [
        true,
        false,
        false,
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "main",
      "md_content": [
        "**main**: The function of main is to serve as the entry point for the command-line interface of the verification tool, allowing users to specify various options and commands for verifying Bitcoin binaries and published releases.\n\n**parameters**: The parameters of this Function.\n· None (The function does not take any parameters directly; it utilizes argparse to handle command-line arguments.)\n\n**Code Description**: The main function initializes a command-line interface using the argparse library, which facilitates user interaction with the verification tool. It begins by creating an ArgumentParser instance, which is configured with a description derived from the module's docstring. The function then defines several command-line arguments that users can specify when executing the script.\n\nKey arguments include:\n- `-v` or `--verbose`: Enables verbose output if specified.\n- `-q` or `--quiet`: Suppresses output messages if specified.\n- `--import-keys`: Prompts the user to import unknown builder keys if specified.\n- `--min-good-sigs`: Sets the minimum number of good signatures required for successful verification, defaulting to 3 unless overridden by an environment variable.\n- `--keyserver`: Specifies the keyserver to use for key retrieval, with a default value set to 'hkps://keys.openpgp.org'.\n- `--trusted-keys`: Accepts a comma-separated list of trusted signer GPG keys.\n- `--json`: Outputs the results in JSON format if specified.\n\nThe function also establishes subcommands for the verification process:\n1. `pub`: This subcommand is linked to the `verify_published_handler` function, which handles the verification of published Bitcoin releases based on a specified version.\n2. `bin`: This subcommand is associated with the `verify_binaries_handler` function, which verifies local binary files against a SHA256SUMS file and its signature.\n\nAfter parsing the command-line arguments, the function checks the verbosity level and adjusts the logging settings accordingly. It then invokes the appropriate handler function (either `verify_published_handler` or `verify_binaries_handler`) based on the user's command, passing the parsed arguments to it.\n\nThe main function plays a crucial role in orchestrating the verification process by setting up the necessary configurations and delegating tasks to the respective handler functions. It ensures that users can easily interact with the tool and customize their verification experience through command-line options.\n\n**Note**: Users should ensure that they provide valid command-line arguments and that the necessary environment variables are set for optimal functionality. It is also important to follow the expected formats for version strings and file paths to avoid errors during execution.\n\n**Output Example**: The function does not return a value directly; instead, it triggers the execution of the specified verification process, which may result in output such as verification results printed to the console or in JSON format, depending on the user's command-line options."
      ],
      "code_start_line": 637,
      "code_end_line": 709,
      "params": [],
      "have_return": true,
      "code_content": "def main():\n    parser = argparse.ArgumentParser(description=__doc__)\n    parser.add_argument(\n        '-v', '--verbose', action='store_true',\n        default=bool_from_env('BINVERIFY_VERBOSE'),\n    )\n    parser.add_argument(\n        '-q', '--quiet', action='store_true',\n        default=bool_from_env('BINVERIFY_QUIET'),\n    )\n    parser.add_argument(\n        '--import-keys', action='store_true',\n        default=bool_from_env('BINVERIFY_IMPORTKEYS'),\n        help='if specified, ask to import each unknown builder key'\n    )\n    parser.add_argument(\n        '--min-good-sigs', type=int, action='store', nargs='?',\n        default=int(os.environ.get('BINVERIFY_MIN_GOOD_SIGS', 3)),\n        help=(\n            'The minimum number of good signatures to require successful termination.'),\n    )\n    parser.add_argument(\n        '--keyserver', action='store', nargs='?',\n        default=os.environ.get('BINVERIFY_KEYSERVER', 'hkps://keys.openpgp.org'),\n        help='which keyserver to use',\n    )\n    parser.add_argument(\n        '--trusted-keys', action='store', nargs='?',\n        default=os.environ.get('BINVERIFY_TRUSTED_KEYS', ''),\n        help='A list of trusted signer GPG keys, separated by commas. Not \"trusted keys\" in the GPG sense.',\n    )\n    parser.add_argument(\n        '--json', action='store_true',\n        default=bool_from_env('BINVERIFY_JSON'),\n        help='If set, output the result as JSON',\n    )\n\n    subparsers = parser.add_subparsers(title=\"Commands\", required=True, dest=\"command\")\n\n    pub_parser = subparsers.add_parser(\"pub\", help=\"Verify a published release.\")\n    pub_parser.set_defaults(func=verify_published_handler)\n    pub_parser.add_argument(\n        'version', type=str, help=(\n            f'version of the bitcoin release to download; of the format '\n            f'{VERSION_FORMAT}. Example: {VERSION_EXAMPLE}')\n    )\n    pub_parser.add_argument(\n        '--cleanup', action='store_true',\n        default=bool_from_env('BINVERIFY_CLEANUP'),\n        help='if specified, clean up files afterwards'\n    )\n    pub_parser.add_argument(\n        '--require-all-hosts', action='store_true',\n        default=bool_from_env('BINVERIFY_REQUIRE_ALL_HOSTS'),\n        help=(\n            f'If set, require all hosts ({HOST1}, {HOST2}) to provide signatures. '\n            '(Sometimes bitcoin.org lags behind bitcoincore.org.)')\n    )\n\n    bin_parser = subparsers.add_parser(\"bin\", help=\"Verify local binaries.\")\n    bin_parser.set_defaults(func=verify_binaries_handler)\n    bin_parser.add_argument(\"--sums-sig-file\", \"-s\", help=\"Path to the SHA256SUMS.asc file to verify\")\n    bin_parser.add_argument(\"sums_file\", help=\"Path to the SHA256SUMS file to verify\")\n    bin_parser.add_argument(\n        \"binary\", nargs=\"*\",\n        help=\"Path to a binary distribution file to verify. Can be specified multiple times for multiple files to verify.\"\n    )\n\n    args = parser.parse_args()\n    if args.quiet:\n        log.setLevel(logging.WARNING)\n\n    return args.func(args)\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "dataset/verify.py/bool_from_env",
        "dataset/verify.py/verify_published_handler",
        "dataset/verify.py/verify_binaries_handler"
      ],
      "special_reference_type": [
        false,
        false,
        false
      ]
    }
  ],
  "dataset/compat.py": [
    {
      "type": "FunctionDef",
      "name": "as_bytes",
      "md_content": [
        "**as_bytes**: The function of as_bytes is to convert various Python input types, such as `bytearray`, `bytes`, or unicode strings, into a `bytes` object.\n\n**parameters**: The parameters of this Function.\n· bytes_or_text: A `bytearray`, `bytes`, `str`, or `unicode` object that needs to be converted to `bytes`.\n· encoding: A string indicating the charset for encoding unicode. The default value is 'utf-8'.\n\n**Code Description**: \nThe `as_bytes` function is designed to handle multiple input types and convert them into a `bytes` object. It first validates the provided encoding by using `codecs.lookup(encoding).name`, which ensures that the encoding is valid and raises a `LookupError` if it is not. The function then checks the type of the input `bytes_or_text`:\n- If the input is a `bytearray`, it converts it directly to `bytes` using the `bytes()` constructor.\n- If the input is a unicode string (checked using `_six.text_type`), it encodes the string into `bytes` using the specified encoding.\n- If the input is already a `bytes` object, it returns the input as is.\n- If the input is neither a binary nor a unicode string, the function raises a `TypeError`.\n\nIn the project, `as_bytes` is called by the `path_to_bytes` function, which converts a `PathLike` object or a string to `bytes`. The `path_to_bytes` function first checks if the input has the `__fspath__` method (indicating it is a `PathLike` object) and converts it to a string representation. It then passes this string to `as_bytes` to ensure the final output is in `bytes` format. This is particularly useful when a simplified `bytes` version of a path is required, such as when dealing with file system operations.\n\n**Note**: \n- Ensure that the input to `as_bytes` is either a `bytearray`, `bytes`, or a unicode string. Providing an unsupported type will result in a `TypeError`.\n- The default encoding is 'utf-8', but you can specify a different encoding if needed. However, make sure the encoding is valid to avoid a `LookupError`.\n\n**Output Example**: \nIf the input is the unicode string \"hello\", the function will return `b'hello'`. If the input is a `bytearray` containing `[104, 101, 108, 108, 111]`, the function will return `b'hello'`. If the input is already a `bytes` object like `b'world'`, the function will return `b'world'` unchanged."
      ],
      "code_start_line": 61,
      "code_end_line": 86,
      "params": [
        "bytes_or_text",
        "encoding"
      ],
      "have_return": true,
      "code_content": "def as_bytes(bytes_or_text, encoding='utf-8'):\n  \"\"\"Converts `bytearray`, `bytes`, or unicode python input types to `bytes`.\n\n  Uses utf-8 encoding for text by default.\n\n  Args:\n    bytes_or_text: A `bytearray`, `bytes`, `str`, or `unicode` object.\n    encoding: A string indicating the charset for encoding unicode.\n\n  Returns:\n    A `bytes` object.\n\n  Raises:\n    TypeError: If `bytes_or_text` is not a binary or unicode string.\n  \"\"\"\n  # Validate encoding, a LookupError will be raised if invalid.\n  encoding = codecs.lookup(encoding).name\n  if isinstance(bytes_or_text, bytearray):\n    return bytes(bytes_or_text)\n  elif isinstance(bytes_or_text, _six.text_type):\n    return bytes_or_text.encode(encoding)\n  elif isinstance(bytes_or_text, bytes):\n    return bytes_or_text\n  else:\n    raise TypeError('Expected binary or unicode string, got %r' %\n                    (bytes_or_text,))\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "dataset/compat.py/path_to_bytes"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "as_text",
      "md_content": [
        "**as_text**: The function of as_text is to convert any string-like Python input types to a unicode string.\n\n**parameters**: The parameters of this Function.\n· bytes_or_text: A `bytes`, `str`, or `unicode` object that needs to be converted to a unicode string.\n· encoding: A string indicating the charset for decoding unicode. The default value is 'utf-8'.\n\n**Code Description**: \nThe `as_text` function is designed to handle string-like inputs in Python and ensure they are returned as unicode strings. It first validates the provided encoding by looking it up using the `codecs.lookup` method, which raises a `LookupError` if the encoding is invalid. \n\nThe function then checks the type of the input `bytes_or_text`. If the input is already a unicode string (checked using `_six.text_type`), it is returned as is. If the input is of type `bytes`, it is decoded into a unicode string using the specified encoding. If the input is neither a unicode string nor bytes, a `TypeError` is raised, indicating that the input type is not supported.\n\nThis function is particularly useful in scenarios where consistent unicode string handling is required, such as when processing text data from various sources that may provide data in different formats (e.g., binary or encoded strings).\n\nIn the project, `as_text` is called by the `as_str` function, which simply returns the result of `as_text`. This indicates that `as_str` is a convenience wrapper around `as_text`, likely used to provide a more intuitive function name or to maintain consistency with other parts of the codebase.\n\n**Note**: \n- Ensure that the input `bytes_or_text` is either a `bytes`, `str`, or `unicode` object to avoid raising a `TypeError`.\n- The default encoding is 'utf-8', but you can specify a different encoding if needed. However, make sure the specified encoding is valid to prevent a `LookupError`.\n\n**Output Example**: \nIf the input is `b'hello'` (a bytes object), the function will return `'hello'` as a unicode string. If the input is already a unicode string like `'hello'`, it will return the same string without modification."
      ],
      "code_start_line": 89,
      "code_end_line": 112,
      "params": [
        "bytes_or_text",
        "encoding"
      ],
      "have_return": true,
      "code_content": "def as_text(bytes_or_text, encoding='utf-8'):\n  \"\"\"Converts any string-like python input types to unicode.\n\n  Returns the input as a unicode string. Uses utf-8 encoding for text\n  by default.\n\n  Args:\n    bytes_or_text: A `bytes`, `str`, or `unicode` object.\n    encoding: A string indicating the charset for decoding unicode.\n\n  Returns:\n    A `unicode` (Python 2) or `str` (Python 3) object.\n\n  Raises:\n    TypeError: If `bytes_or_text` is not a binary or unicode string.\n  \"\"\"\n  # Validate encoding, a LookupError will be raised if invalid.\n  encoding = codecs.lookup(encoding).name\n  if isinstance(bytes_or_text, _six.text_type):\n    return bytes_or_text\n  elif isinstance(bytes_or_text, bytes):\n    return bytes_or_text.decode(encoding)\n  else:\n    raise TypeError('Expected binary or unicode string, got %r' % bytes_or_text)\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "dataset/compat.py/as_str"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "as_str",
      "md_content": [
        "**as_str**: The function of as_str is to convert a bytes or text input into a unicode string using the specified encoding.\n\n**parameters**: The parameters of this Function.\n· bytes_or_text: A `bytes`, `str`, or `unicode` object that needs to be converted to a unicode string.\n· encoding: A string indicating the charset for decoding unicode. The default value is 'utf-8'.\n\n**Code Description**: \nThe `as_str` function is a convenience wrapper around the `as_text` function. It takes an input `bytes_or_text` and an optional `encoding` parameter, and directly returns the result of calling `as_text` with the same arguments. This function is designed to simplify the process of converting various string-like inputs (such as `bytes`, `str`, or `unicode`) into a unicode string. \n\nThe `as_text` function, which `as_str` relies on, handles the actual conversion logic. It first validates the provided encoding and then checks the type of the input. If the input is already a unicode string, it is returned as is. If the input is of type `bytes`, it is decoded into a unicode string using the specified encoding. If the input is neither a unicode string nor bytes, a `TypeError` is raised.\n\nIn the project, `as_str` is called by the `as_str_any` function, which is used to convert any input to a `str` type. Specifically, `as_str_any` uses `as_str` to handle `bytes` typed inputs, ensuring consistent unicode string handling across the codebase. This relationship highlights the role of `as_str` as a utility function that ensures text data is consistently processed as unicode strings, regardless of the input format.\n\n**Note**: \n- Ensure that the input `bytes_or_text` is either a `bytes`, `str`, or `unicode` object to avoid raising a `TypeError`.\n- The default encoding is 'utf-8', but you can specify a different encoding if needed. However, make sure the specified encoding is valid to prevent a `LookupError`.\n\n**Output Example**: \nIf the input is `b'hello'` (a bytes object), the function will return `'hello'` as a unicode string. If the input is already a unicode string like `'hello'`, it will return the same string without modification."
      ],
      "code_start_line": 115,
      "code_end_line": 116,
      "params": [
        "bytes_or_text",
        "encoding"
      ],
      "have_return": true,
      "code_content": "def as_str(bytes_or_text, encoding='utf-8'):\n  return as_text(bytes_or_text, encoding)\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "dataset/compat.py/as_str_any"
      ],
      "reference_who": [
        "dataset/compat.py/as_text"
      ],
      "special_reference_type": [
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "as_str_any",
      "md_content": [
        "**as_str_any**: The function of as_str_any is to convert any input object to a `str` type, handling `bytes` inputs with a specific encoding.\n\n**parameters**: The parameters of this Function.\n· value: An object that can be converted to `str`. This can be of any type, including `bytes`, `str`, or other objects that support the `str()` conversion.\n· encoding: A string indicating the charset for decoding `bytes` typed inputs. The default value is 'utf-8'.\n\n**Code Description**: \nThe `as_str_any` function is designed to ensure that any input object is converted to a `str` type. It achieves this by first checking if the input is of type `bytes`. If the input is `bytes`, it calls the `as_str` function to decode the bytes into a `str` using the specified encoding. For all other types of input, it directly applies the `str()` function to convert the object to a string.\n\nThe function relies on `as_str` to handle the conversion of `bytes` inputs. `as_str` is a utility function that ensures `bytes` or text inputs are consistently converted to unicode strings. This relationship ensures that `as_str_any` can handle a wide range of input types while maintaining consistent behavior for `bytes` inputs.\n\nIn the project, `as_str_any` is used by the `path_to_str` function to convert path-like objects to strings. Specifically, `path_to_str` uses `as_str_any` to handle the conversion of path representations that may include `bytes` or other types. This highlights the role of `as_str_any` as a versatile utility function for string conversion, ensuring that path representations are consistently processed as strings.\n\n**Note**: \n- Ensure that the `encoding` parameter is valid for `bytes` inputs to avoid decoding errors.\n- The function is designed to handle a wide range of input types, but non-string-like objects may result in unexpected string representations when using `str()`.\n\n**Output Example**: \nIf the input is `b'hello'` (a bytes object), the function will return `'hello'` as a string. If the input is an integer like `123`, the function will return `'123'` as a string. If the input is already a string like `'world'`, it will return the same string without modification."
      ],
      "code_start_line": 124,
      "code_end_line": 140,
      "params": [
        "value",
        "encoding"
      ],
      "have_return": true,
      "code_content": "def as_str_any(value, encoding='utf-8'):\n  \"\"\"Converts input to `str` type.\n\n     Uses `str(value)`, except for `bytes` typed inputs, which are converted\n     using `as_str`.\n\n  Args:\n    value: A object that can be converted to `str`.\n    encoding: Encoding for `bytes` typed inputs.\n\n  Returns:\n    A `str` object.\n  \"\"\"\n  if isinstance(value, bytes):\n    return as_str(value, encoding=encoding)\n  else:\n    return str(value)\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "dataset/compat.py/path_to_str"
      ],
      "reference_who": [
        "dataset/compat.py/as_str"
      ],
      "special_reference_type": [
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "path_to_str",
      "md_content": [
        "**path_to_str**: The function of path_to_str is to convert a `PathLike` object to a `str` type, ensuring compatibility with string-based path representations.\n\n**parameters**: The parameters of this Function.\n· path: An object that can be converted to a path representation. This can be any object that supports the `__fspath__` method or is already a string-like object.\n\n**Code Description**: The `path_to_str` function is designed to handle path-like objects and convert them into string representations. It first checks if the input object has the `__fspath__` method, which is a standard method for objects that represent file system paths (e.g., `os.PathLike` objects). If the object has this method, the function calls `__fspath__` to retrieve the path as a string and then uses the `as_str_any` function to ensure the result is a `str` type. If the input object does not have the `__fspath__` method, the function simply returns the input as-is, assuming it is already a string or can be treated as one.\n\nThe function relies on `as_str_any` to handle the conversion of the path representation to a string. `as_str_any` is a utility function that ensures consistent string conversion, particularly for `bytes` inputs, by decoding them using a specified encoding (default is 'utf-8'). This ensures that `path_to_str` can handle a wide range of path-like objects, including those that may return `bytes` from their `__fspath__` method.\n\n**Note**: \n- The function is primarily intended for use with `PathLike` objects, such as those from the `pathlib` module or other objects that implement the `__fspath__` method.\n- If the input is not a `PathLike` object, the function will return it unchanged, which may not always be desirable if the input is not already a string.\n- Ensure that the input object's `__fspath__` method returns a valid string or `bytes` representation to avoid unexpected behavior.\n\n**Output Example**: \nIf the input is a `Path` object representing the path `'C:\\XYZ\\tensorflow\\./.././tensorflow'` on a Windows system, the function will return `'C:\\XYZ\\tensorflow\\..\\tensorflow'`. If the input is a string like `'./corpus'`, the function will return the same string `'./corpus'`. If the input is a `Path` object representing `'./.././Corpus'` on a Linux system, the function will return `'../Corpus'`."
      ],
      "code_start_line": 144,
      "code_end_line": 179,
      "params": [
        "path"
      ],
      "have_return": true,
      "code_content": "def path_to_str(path):\n  r\"\"\"Converts input which is a `PathLike` object to `str` type.\n\n  Converts from any python constant representation of a `PathLike` object to\n  a string. If the input is not a `PathLike` object, simply returns the input.\n\n  Args:\n    path: An object that can be converted to path representation.\n\n  Returns:\n    A `str` object.\n\n  Usage:\n    In case a simplified `str` version of the path is needed from an\n    `os.PathLike` object.\n\n  Examples:\n  ```python\n  $ tf.compat.path_to_str('C:\\XYZ\\tensorflow\\./.././tensorflow')\n  'C:\\XYZ\\tensorflow\\./.././tensorflow' # Windows OS\n  $ tf.compat.path_to_str(Path('C:\\XYZ\\tensorflow\\./.././tensorflow'))\n  'C:\\XYZ\\tensorflow\\..\\tensorflow' # Windows OS\n  $ tf.compat.path_to_str(Path('./corpus'))\n  'corpus' # Linux OS\n  $ tf.compat.path_to_str('./.././Corpus')\n  './.././Corpus' # Linux OS\n  $ tf.compat.path_to_str(Path('./.././Corpus'))\n  '../Corpus' # Linux OS\n  $ tf.compat.path_to_str(Path('./..////../'))\n  '../..' # Linux OS\n\n  ```\n  \"\"\"\n  if hasattr(path, '__fspath__'):\n    path = as_str_any(path.__fspath__())\n  return path\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "dataset/compat.py/as_str_any"
      ],
      "special_reference_type": [
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "path_to_bytes",
      "md_content": [
        "**path_to_bytes**: The function of path_to_bytes is to convert a `PathLike` object or a string into a `bytes` object.\n\n**parameters**: The parameters of this Function.\n· path: An object that can be converted to a path representation. This can be any Python constant representation of a `PathLike` object or a `str`.\n\n**Code Description**: The `path_to_bytes` function is designed to handle input that represents a file path, whether it is a `PathLike` object or a string, and convert it into a `bytes` object. The function first checks if the input object has the `__fspath__` method, which is a characteristic of `PathLike` objects. If the input has this method, the function calls it to obtain the string representation of the path. This ensures compatibility with any object that adheres to the `PathLike` protocol. Once the path is in string form, the function passes it to the `as_bytes` function, which converts the string into a `bytes` object using the specified encoding (default is 'utf-8').\n\nThe relationship between `path_to_bytes` and `as_bytes` is functional and sequential. `path_to_bytes` acts as a higher-level function that prepares the input for `as_bytes` by ensuring the path is in a string format. This is particularly useful in scenarios where a simplified `bytes` version of a path is required, such as when interacting with file systems or APIs that expect binary data.\n\n**Note**: \n- Ensure that the input to `path_to_bytes` is either a `PathLike` object or a string. Providing an unsupported type will result in a `TypeError` when `as_bytes` is called.\n- The function relies on the `as_bytes` function for the final conversion, so the encoding used will be the default 'utf-8' unless specified otherwise in `as_bytes`.\n\n**Output Example**: \nIf the input is a `PathLike` object representing the path \"/usr/local/bin\", the function will return `b'/usr/local/bin'`. If the input is a string \"C:\\\\Windows\\\\System32\", the function will return `b'C:\\\\Windows\\\\System32'`."
      ],
      "code_start_line": 182,
      "code_end_line": 200,
      "params": [
        "path"
      ],
      "have_return": true,
      "code_content": "def path_to_bytes(path):\n  r\"\"\"Converts input which is a `PathLike` object to `bytes`.\n\n  Converts from any python constant representation of a `PathLike` object\n  or `str` to bytes.\n\n  Args:\n    path: An object that can be converted to path representation.\n\n  Returns:\n    A `bytes` object.\n\n  Usage:\n    In case a simplified `bytes` version of the path is needed from an\n    `os.PathLike` object.\n  \"\"\"\n  if hasattr(path, '__fspath__'):\n    path = path.__fspath__()\n  return as_bytes(path)\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "dataset/compat.py/as_bytes"
      ],
      "special_reference_type": [
        false
      ]
    }
  ],
  "dataset/test_functions.py": [
    {
      "type": "ClassDef",
      "name": "CompileTest",
      "md_content": [
        "**CompileTest**: The function of CompileTest is to test the compilation of SQL functions and expressions across different SQL dialects, ensuring that they are correctly translated into the appropriate SQL syntax.\n\n**attributes**: The attributes of this Class.\n· __dialect__: Specifies the default SQL dialect to be used for testing. This is set to \"default\" by default.\n· _registry: A deep copy of the function registry used to restore the original state after each test.\n\n**Code Description**: \nThe CompileTest class is designed to validate the correct compilation of SQL functions and expressions across various SQL dialects. It inherits from fixtures.TestBase and AssertsCompiledSQL, which provide the necessary testing framework and assertion methods for SQL compilation. The class includes methods to set up and tear down the test environment, ensuring that the function registry is restored to its original state after each test.\n\nThe primary method, `test_compile`, iterates over all supported SQL dialects and tests the compilation of various SQL functions, such as `current_timestamp`, `localtime`, and custom functions. It also tests the compilation of generic functions and ensures that the correct SQL syntax is generated for each dialect.\n\nThe class also includes tests for specific scenarios, such as the use of labels, underscores in function names, uppercase and mixed-case function names, and special characters in function names. It verifies that functions with custom return types and namespaces are correctly compiled and that function overrides and replacements are handled properly.\n\nAdditionally, the class tests the compilation of aggregate functions, window functions, and functions with filters. It ensures that functions with custom arguments and namespaces are correctly compiled and that the correct SQL syntax is generated for each scenario.\n\n**Note**: \n- The class uses a deep copy of the function registry to ensure that tests do not interfere with each other.\n- The `test_compile` method tests a wide range of SQL functions and expressions, making it a comprehensive test for SQL function compilation.\n- The class includes tests for edge cases, such as functions with special characters and custom namespaces, ensuring robust handling of various SQL function scenarios.\n\n**Output Example**: \nThe output of the `test_compile` method would be a series of assertions that verify the correct SQL syntax for each function and dialect. For example, the method might assert that the function `func.current_timestamp()` compiles to `\"CURRENT_TIMESTAMP\"` for the default dialect. Similarly, it might assert that a custom function `fake_func(\"foo\")` compiles to `\"fake_func(:fake_func_1)\"` for a specific dialect. These assertions ensure that the SQL functions are correctly translated into the appropriate SQL syntax for each dialect."
      ],
      "code_start_line": 67,
      "code_end_line": 1037,
      "params": [],
      "have_return": true,
      "code_content": "class CompileTest(fixtures.TestBase, AssertsCompiledSQL):\n    __dialect__ = \"default\"\n\n    def setup_test(self):\n        self._registry = deepcopy(functions._registry)\n\n    def teardown_test(self):\n        functions._registry = self._registry\n\n    def test_compile(self):\n        for dialect in all_dialects():\n            bindtemplate = BIND_TEMPLATES[dialect.paramstyle]\n            self.assert_compile(\n                func.current_timestamp(), \"CURRENT_TIMESTAMP\", dialect=dialect\n            )\n            self.assert_compile(func.localtime(), \"LOCALTIME\", dialect=dialect)\n            self.assert_compile(\n                func.nosuchfunction(), \"nosuchfunction()\", dialect=dialect\n            )\n\n            # test generic function compile\n            class fake_func(GenericFunction):\n                inherit_cache = True\n                __return_type__ = sqltypes.Integer\n\n                def __init__(self, arg, **kwargs):\n                    GenericFunction.__init__(self, arg, **kwargs)\n\n            self.assert_compile(\n                fake_func(\"foo\"),\n                \"fake_func(%s)\"\n                % bindtemplate\n                % {\"name\": \"fake_func_1\", \"position\": 1},\n                dialect=dialect,\n            )\n\n            functions._registry[\"_default\"].pop(\"fake_func\")\n\n    @testing.combinations(\n        (operators.in_op, [1, 2, 3], \"myfunc() IN (1, 2, 3)\"),\n        (operators.add, 5, \"myfunc() + 5\"),\n        (operators.eq, column(\"q\"), \"myfunc() = q\"),\n        argnames=\"op,other,expected\",\n    )\n    @testing.combinations((True,), (False,), argnames=\"use_custom\")\n    def test_operators_custom(self, op, other, expected, use_custom):\n        if use_custom:\n\n            class MyFunc(FunctionElement):\n                inherit_cache = True\n                name = \"myfunc\"\n                type = Integer()\n\n            @compiles(MyFunc)\n            def visit_myfunc(element, compiler, **kw):\n                return \"myfunc(%s)\" % compiler.process(element.clauses, **kw)\n\n            expr = op(MyFunc(), other)\n        else:\n            expr = op(func.myfunc(type_=Integer), other)\n\n        self.assert_compile(\n            select(1).where(expr),\n            \"SELECT 1 WHERE %s\" % (expected,),\n            literal_binds=True,\n            render_postcompile=True,\n            dialect=\"default_enhanced\",\n        )\n\n    def test_use_labels(self):\n        self.assert_compile(\n            select(func.foo()).set_label_style(LABEL_STYLE_TABLENAME_PLUS_COL),\n            \"SELECT foo() AS foo_1\",\n        )\n\n    def test_use_labels_function_element(self):\n        class max_(FunctionElement):\n            name = \"max\"\n            inherit_cache = True\n\n        @compiles(max_)\n        def visit_max(element, compiler, **kw):\n            return \"max(%s)\" % compiler.process(element.clauses, **kw)\n\n        self.assert_compile(\n            select(max_(5, 6)).set_label_style(LABEL_STYLE_TABLENAME_PLUS_COL),\n            \"SELECT max(:max_2, :max_3) AS max_1\",\n        )\n\n    def test_underscores(self):\n        self.assert_compile(func.if_(), \"if()\")\n\n    def test_underscores_packages(self):\n        self.assert_compile(func.foo_.bar_.if_(), \"foo.bar.if()\")\n\n    def test_uppercase(self):\n        # for now, we need to keep case insensitivity\n        self.assert_compile(func.UNREGISTERED_FN(), \"UNREGISTERED_FN()\")\n\n    def test_uppercase_packages(self):\n        # for now, we need to keep case insensitivity\n        self.assert_compile(func.FOO.BAR.NOW(), \"FOO.BAR.NOW()\")\n\n    def test_mixed_case(self):\n        # for now, we need to keep case insensitivity\n        self.assert_compile(func.SomeFunction(), \"SomeFunction()\")\n\n    def test_mixed_case_packages(self):\n        # for now, we need to keep case insensitivity\n        self.assert_compile(\n            func.Foo.Bar.SomeFunction(), \"Foo.Bar.SomeFunction()\"\n        )\n\n    def test_quote_special_chars(self):\n        # however we need to be quoting any other identifiers\n        self.assert_compile(\n            getattr(func, \"im a function\")(), '\"im a function\"()'\n        )\n\n    def test_quote_special_chars_packages(self):\n        # however we need to be quoting any other identifiers\n        self.assert_compile(\n            getattr(\n                getattr(getattr(func, \"im foo package\"), \"im bar package\"),\n                \"im a function\",\n            )(),\n            '\"im foo package\".\"im bar package\".\"im a function\"()',\n        )\n\n    def test_generic_now(self):\n        assert isinstance(func.now().type, sqltypes.DateTime)\n\n        for ret, dialect in [\n            (\"CURRENT_TIMESTAMP\", sqlite.dialect()),\n            (\"now()\", postgresql.dialect()),\n            (\"now()\", mysql.dialect()),\n            (\"CURRENT_TIMESTAMP\", oracle.dialect()),\n        ]:\n            self.assert_compile(func.now(), ret, dialect=dialect)\n\n    def test_generic_random(self):\n        assert func.random().type == sqltypes.NULLTYPE\n        assert isinstance(func.random(type_=Integer).type, Integer)\n\n        for ret, dialect in [\n            (\"random()\", sqlite.dialect()),\n            (\"random()\", postgresql.dialect()),\n            (\"rand()\", mysql.dialect()),\n            (\"random()\", oracle.dialect()),\n        ]:\n            self.assert_compile(func.random(), ret, dialect=dialect)\n\n    def test_return_type_aggregate_strings(self):\n        t = table(\"t\", column(\"value\", String))\n        expr = func.aggregate_strings(t.c.value, \",\")\n        is_(expr.type._type_affinity, String)\n\n    @testing.combinations(\n        (\n            \"SELECT group_concat(t.value, ?) AS aggregate_strings_1 FROM t\",\n            \"sqlite\",\n        ),\n        (\n            \"SELECT string_agg(t.value, %(aggregate_strings_2)s) AS \"\n            \"aggregate_strings_1 FROM t\",\n            \"postgresql\",\n        ),\n        (\n            \"SELECT string_agg(t.value, \"\n            \"__[POSTCOMPILE_aggregate_strings_2]) AS \"\n            \"aggregate_strings_1 FROM t\",\n            \"mssql\",\n        ),\n        (\n            \"SELECT group_concat(t.value SEPARATOR %s) \"\n            \"AS aggregate_strings_1 FROM t\",\n            \"mysql\",\n        ),\n        (\n            \"SELECT LISTAGG(t.value, :aggregate_strings_2) AS\"\n            \" aggregate_strings_1 FROM t\",\n            \"oracle\",\n        ),\n    )\n    def test_aggregate_strings(self, expected_sql, dialect):\n        t = table(\"t\", column(\"value\", String))\n        stmt = select(func.aggregate_strings(t.c.value, \",\"))\n\n        self.assert_compile(stmt, expected_sql, dialect=dialect)\n\n    def test_cube_operators(self):\n        t = table(\n            \"t\",\n            column(\"value\"),\n            column(\"x\"),\n            column(\"y\"),\n            column(\"z\"),\n            column(\"q\"),\n        )\n\n        stmt = select(func.sum(t.c.value))\n\n        self.assert_compile(\n            stmt.group_by(func.cube(t.c.x, t.c.y)),\n            \"SELECT sum(t.value) AS sum_1 FROM t GROUP BY CUBE(t.x, t.y)\",\n        )\n\n        self.assert_compile(\n            stmt.group_by(func.rollup(t.c.x, t.c.y)),\n            \"SELECT sum(t.value) AS sum_1 FROM t GROUP BY ROLLUP(t.x, t.y)\",\n        )\n\n        self.assert_compile(\n            stmt.group_by(func.grouping_sets(t.c.x, t.c.y)),\n            \"SELECT sum(t.value) AS sum_1 FROM t \"\n            \"GROUP BY GROUPING SETS(t.x, t.y)\",\n        )\n\n        self.assert_compile(\n            stmt.group_by(\n                func.grouping_sets(\n                    sql.tuple_(t.c.x, t.c.y), sql.tuple_(t.c.z, t.c.q)\n                )\n            ),\n            \"SELECT sum(t.value) AS sum_1 FROM t GROUP BY \"\n            \"GROUPING SETS((t.x, t.y), (t.z, t.q))\",\n        )\n\n    def test_generic_annotation(self):\n        fn = func.coalesce(\"x\", \"y\")._annotate({\"foo\": \"bar\"})\n        self.assert_compile(fn, \"coalesce(:coalesce_1, :coalesce_2)\")\n\n    def test_annotation_dialect_specific(self):\n        fn = func.current_date()\n        self.assert_compile(fn, \"CURRENT_DATE\", dialect=\"sqlite\")\n\n        fn = fn._annotate({\"foo\": \"bar\"})\n        self.assert_compile(fn, \"CURRENT_DATE\", dialect=\"sqlite\")\n\n    def test_custom_default_namespace(self):\n        class myfunc(GenericFunction):\n            inherit_cache = True\n\n        assert isinstance(func.myfunc(), myfunc)\n        self.assert_compile(func.myfunc(), \"myfunc()\")\n\n    def test_custom_type(self):\n        class myfunc(GenericFunction):\n            type = DateTime\n            inherit_cache = True\n\n        assert isinstance(func.myfunc().type, DateTime)\n        self.assert_compile(func.myfunc(), \"myfunc()\")\n\n    def test_custom_legacy_type(self):\n        # in case someone was using this system\n        class myfunc(GenericFunction):\n            inherit_cache = True\n            __return_type__ = DateTime\n\n        assert isinstance(func.myfunc().type, DateTime)\n\n    def test_case_sensitive(self):\n        class MYFUNC(GenericFunction):\n            inherit_cache = True\n            type = DateTime\n\n        assert isinstance(func.MYFUNC().type, DateTime)\n        assert isinstance(func.MyFunc().type, DateTime)\n        assert isinstance(func.mYfUnC().type, DateTime)\n        assert isinstance(func.myfunc().type, DateTime)\n\n    def test_replace_function(self):\n        class replaceable_func(GenericFunction):\n            type = Integer\n            identifier = \"replaceable_func\"\n\n        assert isinstance(func.Replaceable_Func().type, Integer)\n        assert isinstance(func.RePlAcEaBlE_fUnC().type, Integer)\n        assert isinstance(func.replaceable_func().type, Integer)\n\n        with expect_warnings(\n            \"The GenericFunction 'replaceable_func' is already registered and \"\n            \"is going to be overridden.\",\n            regex=False,\n        ):\n\n            class replaceable_func_override(GenericFunction):\n                type = DateTime\n                identifier = \"replaceable_func\"\n\n        assert isinstance(func.Replaceable_Func().type, DateTime)\n        assert isinstance(func.RePlAcEaBlE_fUnC().type, DateTime)\n        assert isinstance(func.replaceable_func().type, DateTime)\n\n    def test_replace_function_case_insensitive(self):\n        class replaceable_func(GenericFunction):\n            type = Integer\n            identifier = \"replaceable_func\"\n\n        assert isinstance(func.Replaceable_Func().type, Integer)\n        assert isinstance(func.RePlAcEaBlE_fUnC().type, Integer)\n        assert isinstance(func.replaceable_func().type, Integer)\n\n        with expect_warnings(\n            \"The GenericFunction 'replaceable_func' is already registered and \"\n            \"is going to be overridden.\",\n            regex=False,\n        ):\n\n            class replaceable_func_override(GenericFunction):\n                type = DateTime\n                identifier = \"REPLACEABLE_Func\"\n\n        assert isinstance(func.Replaceable_Func().type, DateTime)\n        assert isinstance(func.RePlAcEaBlE_fUnC().type, DateTime)\n        assert isinstance(func.replaceable_func().type, DateTime)\n\n    def test_custom_w_custom_name(self):\n        class myfunc(GenericFunction):\n            inherit_cache = True\n            name = \"notmyfunc\"\n\n        assert isinstance(func.notmyfunc(), myfunc)\n        assert not isinstance(func.myfunc(), myfunc)\n\n    def test_custom_w_quoted_name(self):\n        class myfunc(GenericFunction):\n            inherit_cache = True\n            name = quoted_name(\"NotMyFunc\", quote=True)\n            identifier = \"myfunc\"\n\n        self.assert_compile(func.myfunc(), '\"NotMyFunc\"()')\n\n    def test_custom_w_quoted_name_no_identifier(self):\n        class myfunc(GenericFunction):\n            inherit_cache = True\n            name = quoted_name(\"NotMyFunc\", quote=True)\n\n        # note this requires that the quoted name be lower cased for\n        # correct lookup\n        self.assert_compile(func.notmyfunc(), '\"NotMyFunc\"()')\n\n    def test_custom_package_namespace(self):\n        def cls1(pk_name):\n            class myfunc(GenericFunction):\n                inherit_cache = True\n                package = pk_name\n\n            return myfunc\n\n        f1 = cls1(\"mypackage\")\n        f2 = cls1(\"myotherpackage\")\n\n        assert isinstance(func.mypackage.myfunc(), f1)\n        assert isinstance(func.myotherpackage.myfunc(), f2)\n\n    def test_custom_name(self):\n        class MyFunction(GenericFunction):\n            name = \"my_func\"\n            inherit_cache = True\n\n            def __init__(self, *args):\n                args = args + (3,)\n                super().__init__(*args)\n\n        self.assert_compile(\n            func.my_func(1, 2), \"my_func(:my_func_1, :my_func_2, :my_func_3)\"\n        )\n\n    def test_custom_registered_identifier(self):\n        class GeoBuffer(GenericFunction):\n            type = Integer\n            package = \"geo\"\n            name = \"BufferOne\"\n            identifier = \"buf1\"\n            inherit_cache = True\n\n        class GeoBuffer2(GenericFunction):\n            type = Integer\n            name = \"BufferTwo\"\n            identifier = \"buf2\"\n            inherit_cache = True\n\n        class BufferThree(GenericFunction):\n            type = Integer\n            identifier = \"buf3\"\n            inherit_cache = True\n\n        class GeoBufferFour(GenericFunction):\n            type = Integer\n            name = \"BufferFour\"\n            identifier = \"Buf4\"\n            inherit_cache = True\n\n        self.assert_compile(func.geo.buf1(), \"BufferOne()\")\n        self.assert_compile(func.buf2(), \"BufferTwo()\")\n        self.assert_compile(func.buf3(), \"BufferThree()\")\n        self.assert_compile(func.Buf4(), \"BufferFour()\")\n        self.assert_compile(func.BuF4(), \"BufferFour()\")\n        self.assert_compile(func.bUf4(), \"BufferFour()\")\n        self.assert_compile(func.bUf4_(), \"BufferFour()\")\n        self.assert_compile(func.buf4(), \"BufferFour()\")\n\n    def test_custom_args(self):\n        class myfunc(GenericFunction):\n            inherit_cache = True\n\n        self.assert_compile(\n            myfunc(1, 2, 3), \"myfunc(:myfunc_1, :myfunc_2, :myfunc_3)\"\n        )\n\n    def test_namespacing_conflicts(self):\n        self.assert_compile(func.text(\"foo\"), \"text(:text_1)\")\n\n    def test_generic_count(self):\n        assert isinstance(func.count().type, sqltypes.Integer)\n\n        self.assert_compile(func.count(), \"count(*)\")\n        self.assert_compile(func.count(1), \"count(:count_1)\")\n        c = column(\"abc\")\n        self.assert_compile(func.count(c), \"count(abc)\")\n\n    def test_ansi_functions_with_args(self):\n        ct = func.current_timestamp(\"somearg\")\n        self.assert_compile(ct, \"CURRENT_TIMESTAMP(:current_timestamp_1)\")\n\n    def test_char_length_fixed_args(self):\n        assert_raises(TypeError, func.char_length, \"a\", \"b\")\n        assert_raises(TypeError, func.char_length)\n\n    def test_return_type_detection(self):\n        for fn in [func.coalesce, func.max, func.min, func.sum]:\n            for args, type_ in [\n                (\n                    (datetime.date(2007, 10, 5), datetime.date(2005, 10, 15)),\n                    sqltypes.Date,\n                ),\n                ((3, 5), sqltypes.Integer),\n                ((decimal.Decimal(3), decimal.Decimal(5)), sqltypes.Numeric),\n                ((\"foo\", \"bar\"), sqltypes.String),\n                (\n                    (\n                        datetime.datetime(2007, 10, 5, 8, 3, 34),\n                        datetime.datetime(2005, 10, 15, 14, 45, 33),\n                    ),\n                    sqltypes.DateTime,\n                ),\n            ]:\n                assert isinstance(fn(*args).type, type_), \"%s / %r != %s\" % (\n                    fn(),\n                    fn(*args).type,\n                    type_,\n                )\n\n        assert isinstance(func.concat(\"foo\", \"bar\").type, sqltypes.String)\n\n    def test_assorted(self):\n        table1 = table(\"mytable\", column(\"myid\", Integer))\n\n        table2 = table(\"myothertable\", column(\"otherid\", Integer))\n\n        # test an expression with a function\n        self.assert_compile(\n            func.lala(3, 4, literal(\"five\"), table1.c.myid) * table2.c.otherid,\n            \"lala(:lala_1, :lala_2, :param_1, mytable.myid) * \"\n            \"myothertable.otherid\",\n        )\n\n        # test it in a SELECT\n        self.assert_compile(\n            select(func.count(table1.c.myid)),\n            \"SELECT count(mytable.myid) AS count_1 FROM mytable\",\n        )\n\n        # test a \"dotted\" function name\n        self.assert_compile(\n            select(func.foo.bar.lala(table1.c.myid)),\n            \"SELECT foo.bar.lala(mytable.myid) AS lala_1 FROM mytable\",\n        )\n\n        # test the bind parameter name with a \"dotted\" function name is\n        # only the name (limits the length of the bind param name)\n        self.assert_compile(\n            select(func.foo.bar.lala(12)),\n            \"SELECT foo.bar.lala(:lala_2) AS lala_1\",\n        )\n\n        # test a dotted func off the engine itself\n        self.assert_compile(func.lala.hoho(7), \"lala.hoho(:hoho_1)\")\n\n        # test None becomes NULL\n        self.assert_compile(\n            func.my_func(1, 2, None, 3),\n            \"my_func(:my_func_1, :my_func_2, NULL, :my_func_3)\",\n        )\n\n        f1 = func.my_func(1, 2, None, 3)\n        f1._generate_cache_key()\n\n        # test pickling\n        self.assert_compile(\n            pickle.loads(pickle.dumps(f1)),\n            \"my_func(:my_func_1, :my_func_2, NULL, :my_func_3)\",\n        )\n\n        # assert func raises AttributeError for __bases__ attribute, since\n        # its not a class fixes pydoc\n        try:\n            func.__bases__\n            assert False\n        except AttributeError:\n            assert True\n\n    def test_pickle_over(self):\n        # TODO: the test/sql package lacks a comprehensive pickling\n        # test suite even though there are __reduce__ methods in several\n        # places in sql/elements.py.   likely as part of\n        # test/sql/test_compare.py might be a place this can happen but\n        # this still relies upon a strategy for table metadata as we have\n        # in serializer.\n\n        f1 = func.row_number().over()\n\n        self.assert_compile(\n            pickle.loads(pickle.dumps(f1)),\n            \"row_number() OVER ()\",\n        )\n\n    def test_pickle_within_group(self):\n        \"\"\"test #6520\"\"\"\n\n        # TODO: the test/sql package lacks a comprehensive pickling\n        # test suite even though there are __reduce__ methods in several\n        # places in sql/elements.py.   likely as part of\n        # test/sql/test_compare.py might be a place this can happen but\n        # this still relies upon a strategy for table metadata as we have\n        # in serializer.\n\n        f1 = func.percentile_cont(literal(1)).within_group()\n\n        self.assert_compile(\n            pickle.loads(pickle.dumps(f1)),\n            \"percentile_cont(:param_1) WITHIN GROUP (ORDER BY )\",\n        )\n\n        f1 = func.percentile_cont(literal(1)).within_group(\n            column(\"q\"), column(\"p\").desc()\n        )\n        self.assert_compile(\n            pickle.loads(pickle.dumps(f1)),\n            \"percentile_cont(:param_1) WITHIN GROUP (ORDER BY q, p DESC)\",\n        )\n\n    def test_functions_with_cols(self):\n        users = table(\n            \"users\", column(\"id\"), column(\"name\"), column(\"fullname\")\n        )\n        calculate = (\n            select(column(\"q\"), column(\"z\"), column(\"r\"))\n            .select_from(\n                func.calculate(bindparam(\"x\", None), bindparam(\"y\", None))\n            )\n            .subquery()\n        )\n\n        self.assert_compile(\n            select(users).where(users.c.id > calculate.c.z),\n            \"SELECT users.id, users.name, users.fullname \"\n            \"FROM users, (SELECT q, z, r \"\n            \"FROM calculate(:x, :y)) AS anon_1 \"\n            \"WHERE users.id > anon_1.z\",\n        )\n\n        s = select(users).where(\n            users.c.id.between(\n                calculate.alias(\"c1\").unique_params(x=17, y=45).c.z,\n                calculate.alias(\"c2\").unique_params(x=5, y=12).c.z,\n            ),\n        )\n\n        self.assert_compile(\n            s,\n            \"SELECT users.id, users.name, users.fullname \"\n            \"FROM users, (SELECT q, z, r \"\n            \"FROM calculate(:x_1, :y_1)) AS c1, (SELECT q, z, r \"\n            \"FROM calculate(:x_2, :y_2)) AS c2 \"\n            \"WHERE users.id BETWEEN c1.z AND c2.z\",\n            checkparams={\"y_1\": 45, \"x_1\": 17, \"y_2\": 12, \"x_2\": 5},\n        )\n\n    def test_non_functions(self):\n        expr = func.cast(\"foo\", Integer)\n        self.assert_compile(expr, \"CAST(:param_1 AS INTEGER)\")\n\n        expr = func.extract(\"year\", datetime.date(2010, 12, 5))\n        self.assert_compile(expr, \"EXTRACT(year FROM :param_1)\")\n\n    def test_select_method_one(self):\n        expr = func.rows(\"foo\")\n        self.assert_compile(expr.select(), \"SELECT rows(:rows_2) AS rows_1\")\n\n    def test_alias_method_one(self):\n        expr = func.rows(\"foo\")\n        self.assert_compile(expr.alias(), \"rows(:rows_1)\")\n\n    def test_select_method_two(self):\n        expr = func.rows(\"foo\")\n        self.assert_compile(\n            select(\"*\").select_from(expr.select().subquery()),\n            \"SELECT * FROM (SELECT rows(:rows_2) AS rows_1) AS anon_1\",\n        )\n\n    def test_select_method_three(self):\n        expr = func.rows(\"foo\")\n        self.assert_compile(\n            select(column(\"foo\")).select_from(expr),\n            \"SELECT foo FROM rows(:rows_1)\",\n        )\n\n    def test_alias_method_two(self):\n        expr = func.rows(\"foo\")\n        self.assert_compile(\n            select(\"*\").select_from(expr.alias(\"bar\")),\n            \"SELECT * FROM rows(:rows_1) AS bar\",\n        )\n\n    def test_alias_method_columns(self):\n        expr = func.rows(\"foo\").alias(\"bar\")\n\n        # this isn't very useful but is the old behavior\n        # prior to #2974.\n        # testing here that the expression exports its column\n        # list in a way that at least doesn't break.\n        self.assert_compile(\n            select(expr), \"SELECT bar.rows_1 FROM rows(:rows_2) AS bar\"\n        )\n\n    def test_alias_method_columns_two(self):\n        expr = func.rows(\"foo\").alias(\"bar\")\n        assert len(expr.c)\n\n    def test_funcfilter_empty(self):\n        self.assert_compile(func.count(1).filter(), \"count(:count_1)\")\n\n    def test_funcfilter_criterion(self):\n        self.assert_compile(\n            func.count(1).filter(table1.c.name != None),  # noqa\n            \"count(:count_1) FILTER (WHERE mytable.name IS NOT NULL)\",\n        )\n\n    def test_funcfilter_compound_criterion(self):\n        self.assert_compile(\n            func.count(1).filter(\n                table1.c.name == None, table1.c.myid > 0  # noqa\n            ),\n            \"count(:count_1) FILTER (WHERE mytable.name IS NULL AND \"\n            \"mytable.myid > :myid_1)\",\n        )\n\n    def test_funcfilter_arrayagg_subscript(self):\n        num = column(\"q\")\n        self.assert_compile(\n            func.array_agg(num).filter(num % 2 == 0)[1],\n            \"(array_agg(q) FILTER (WHERE q %% %(q_1)s = \"\n            \"%(param_1)s))[%(param_2)s]\",\n            dialect=\"postgresql\",\n        )\n\n    def test_funcfilter_label(self):\n        self.assert_compile(\n            select(\n                func.count(1)\n                .filter(table1.c.description != None)  # noqa\n                .label(\"foo\")\n            ),\n            \"SELECT count(:count_1) FILTER (WHERE mytable.description \"\n            \"IS NOT NULL) AS foo FROM mytable\",\n        )\n\n    def test_funcfilter_fromobj_fromfunc(self):\n        # test from_obj generation.\n        # from func:\n        self.assert_compile(\n            select(\n                func.max(table1.c.name).filter(\n                    literal_column(\"description\") != None  # noqa\n                )\n            ),\n            \"SELECT max(mytable.name) FILTER (WHERE description \"\n            \"IS NOT NULL) AS anon_1 FROM mytable\",\n        )\n\n    def test_funcfilter_fromobj_fromcriterion(self):\n        # from criterion:\n        self.assert_compile(\n            select(func.count(1).filter(table1.c.name == \"name\")),\n            \"SELECT count(:count_1) FILTER (WHERE mytable.name = :name_1) \"\n            \"AS anon_1 FROM mytable\",\n        )\n\n    def test_funcfilter_chaining(self):\n        # test chaining:\n        self.assert_compile(\n            select(\n                func.count(1)\n                .filter(table1.c.name == \"name\")\n                .filter(table1.c.description == \"description\")\n            ),\n            \"SELECT count(:count_1) FILTER (WHERE \"\n            \"mytable.name = :name_1 AND mytable.description = :description_1) \"\n            \"AS anon_1 FROM mytable\",\n        )\n\n    def test_funcfilter_windowing_orderby(self):\n        # test filtered windowing:\n        self.assert_compile(\n            select(\n                func.rank()\n                .filter(table1.c.name > \"foo\")\n                .over(order_by=table1.c.name)\n            ),\n            \"SELECT rank() FILTER (WHERE mytable.name > :name_1) \"\n            \"OVER (ORDER BY mytable.name) AS anon_1 FROM mytable\",\n        )\n\n    def test_funcfilter_windowing_orderby_partitionby(self):\n        self.assert_compile(\n            select(\n                func.rank()\n                .filter(table1.c.name > \"foo\")\n                .over(order_by=table1.c.name, partition_by=[\"description\"])\n            ),\n            \"SELECT rank() FILTER (WHERE mytable.name > :name_1) \"\n            \"OVER (PARTITION BY mytable.description ORDER BY mytable.name) \"\n            \"AS anon_1 FROM mytable\",\n        )\n\n    def test_funcfilter_windowing_range(self):\n        self.assert_compile(\n            select(\n                func.rank()\n                .filter(table1.c.name > \"foo\")\n                .over(range_=(1, 5), partition_by=[\"description\"])\n            ),\n            \"SELECT rank() FILTER (WHERE mytable.name > :name_1) \"\n            \"OVER (PARTITION BY mytable.description RANGE BETWEEN :param_1 \"\n            \"FOLLOWING AND :param_2 FOLLOWING) \"\n            \"AS anon_1 FROM mytable\",\n            checkparams={\"name_1\": \"foo\", \"param_1\": 1, \"param_2\": 5},\n        )\n\n    def test_funcfilter_windowing_range_positional(self):\n        self.assert_compile(\n            select(\n                func.rank()\n                .filter(table1.c.name > \"foo\")\n                .over(range_=(1, 5), partition_by=[\"description\"])\n            ),\n            \"SELECT rank() FILTER (WHERE mytable.name > ?) \"\n            \"OVER (PARTITION BY mytable.description RANGE BETWEEN ? \"\n            \"FOLLOWING AND ? FOLLOWING) \"\n            \"AS anon_1 FROM mytable\",\n            checkpositional=(\"foo\", 1, 5),\n            dialect=\"default_qmark\",\n        )\n\n    def test_funcfilter_windowing_rows(self):\n        self.assert_compile(\n            select(\n                func.rank()\n                .filter(table1.c.name > \"foo\")\n                .over(rows=(1, 5), partition_by=[\"description\"])\n            ),\n            \"SELECT rank() FILTER (WHERE mytable.name > :name_1) \"\n            \"OVER (PARTITION BY mytable.description ROWS BETWEEN :param_1 \"\n            \"FOLLOWING AND :param_2 FOLLOWING) \"\n            \"AS anon_1 FROM mytable\",\n        )\n\n    def test_funcfilter_more_criteria(self):\n        ff = func.rank().filter(table1.c.name > \"foo\")\n        ff2 = ff.filter(table1.c.myid == 1)\n        self.assert_compile(\n            select(ff, ff2),\n            \"SELECT rank() FILTER (WHERE mytable.name > :name_1) AS anon_1, \"\n            \"rank() FILTER (WHERE mytable.name > :name_1 AND \"\n            \"mytable.myid = :myid_1) AS anon_2 FROM mytable\",\n            {\"name_1\": \"foo\", \"myid_1\": 1},\n        )\n\n    def test_funcfilter_within_group(self):\n        self.assert_compile(\n            select(\n                func.rank()\n                .filter(table1.c.name > \"foo\")\n                .within_group(table1.c.name)\n            ),\n            \"SELECT rank() FILTER (WHERE mytable.name > :name_1) \"\n            \"WITHIN GROUP (ORDER BY mytable.name) \"\n            \"AS anon_1 FROM mytable\",\n        )\n\n    def test_within_group(self):\n        stmt = select(\n            table1.c.myid,\n            func.percentile_cont(0.5).within_group(table1.c.name),\n        )\n        self.assert_compile(\n            stmt,\n            \"SELECT mytable.myid, percentile_cont(:percentile_cont_1) \"\n            \"WITHIN GROUP (ORDER BY mytable.name) \"\n            \"AS anon_1 \"\n            \"FROM mytable\",\n            {\"percentile_cont_1\": 0.5},\n        )\n\n    def test_within_group_multi(self):\n        stmt = select(\n            table1.c.myid,\n            func.percentile_cont(0.5).within_group(\n                table1.c.name, table1.c.description\n            ),\n        )\n        self.assert_compile(\n            stmt,\n            \"SELECT mytable.myid, percentile_cont(:percentile_cont_1) \"\n            \"WITHIN GROUP (ORDER BY mytable.name, mytable.description) \"\n            \"AS anon_1 \"\n            \"FROM mytable\",\n            {\"percentile_cont_1\": 0.5},\n        )\n\n    def test_within_group_desc(self):\n        stmt = select(\n            table1.c.myid,\n            func.percentile_cont(0.5).within_group(table1.c.name.desc()),\n        )\n        self.assert_compile(\n            stmt,\n            \"SELECT mytable.myid, percentile_cont(:percentile_cont_1) \"\n            \"WITHIN GROUP (ORDER BY mytable.name DESC) \"\n            \"AS anon_1 \"\n            \"FROM mytable\",\n            {\"percentile_cont_1\": 0.5},\n        )\n\n    def test_within_group_w_over(self):\n        stmt = select(\n            table1.c.myid,\n            func.percentile_cont(0.5)\n            .within_group(table1.c.name.desc())\n            .over(partition_by=table1.c.description),\n        )\n        self.assert_compile(\n            stmt,\n            \"SELECT mytable.myid, percentile_cont(:percentile_cont_1) \"\n            \"WITHIN GROUP (ORDER BY mytable.name DESC) \"\n            \"OVER (PARTITION BY mytable.description) AS anon_1 \"\n            \"FROM mytable\",\n            {\"percentile_cont_1\": 0.5},\n        )\n\n    def test_within_group_filter(self):\n        stmt = select(\n            table1.c.myid,\n            func.percentile_cont(0.5)\n            .within_group(table1.c.name)\n            .filter(table1.c.myid > 42),\n        )\n        self.assert_compile(\n            stmt,\n            \"SELECT mytable.myid, percentile_cont(:percentile_cont_1) \"\n            \"WITHIN GROUP (ORDER BY mytable.name) \"\n            \"FILTER (WHERE mytable.myid > :myid_1) \"\n            \"AS anon_1 \"\n            \"FROM mytable\",\n            {\"percentile_cont_1\": 0.5, \"myid_1\": 42},\n        )\n\n    def test_incorrect_none_type(self):\n        from sqlalchemy.sql.expression import FunctionElement\n\n        class MissingType(FunctionElement):\n            name = \"mt\"\n            type = None\n\n        assert_raises_message(\n            TypeError,\n            \"Object None associated with '.type' attribute is \"\n            \"not a TypeEngine class or object\",\n            lambda: column(\"x\", MissingType()) == 5,\n        )\n\n    def test_as_comparison(self):\n        fn = func.substring(\"foo\", \"foobar\").as_comparison(1, 2)\n        is_(fn.type._type_affinity, Boolean)\n\n        self.assert_compile(\n            fn.left, \":substring_1\", checkparams={\"substring_1\": \"foo\"}\n        )\n        self.assert_compile(\n            fn.right, \":substring_1\", checkparams={\"substring_1\": \"foobar\"}\n        )\n\n        self.assert_compile(\n            fn,\n            \"substring(:substring_1, :substring_2)\",\n            checkparams={\"substring_1\": \"foo\", \"substring_2\": \"foobar\"},\n        )\n\n    def test_as_comparison_annotate(self):\n        fn = func.foobar(\"x\", \"y\", \"q\", \"p\", \"r\").as_comparison(2, 5)\n\n        from sqlalchemy.sql import annotation\n\n        fn_annotated = annotation._deep_annotate(fn, {\"token\": \"yes\"})\n\n        eq_(fn.left._annotations, {})\n        eq_(fn_annotated.left._annotations, {\"token\": \"yes\"})\n\n    def test_as_comparison_many_argument(self):\n        fn = func.some_comparison(\"x\", \"y\", \"z\", \"p\", \"q\", \"r\").as_comparison(\n            2, 5\n        )\n        is_(fn.type._type_affinity, Boolean)\n\n        self.assert_compile(\n            fn.left,\n            \":some_comparison_1\",\n            checkparams={\"some_comparison_1\": \"y\"},\n        )\n        self.assert_compile(\n            fn.right,\n            \":some_comparison_1\",\n            checkparams={\"some_comparison_1\": \"q\"},\n        )\n\n        from sqlalchemy.sql import visitors\n\n        fn_2 = visitors.cloned_traverse(fn, {}, {})\n        fn_2.right = literal_column(\"ABC\")\n\n        self.assert_compile(\n            fn,\n            \"some_comparison(:some_comparison_1, :some_comparison_2, \"\n            \":some_comparison_3, \"\n            \":some_comparison_4, :some_comparison_5, :some_comparison_6)\",\n            checkparams={\n                \"some_comparison_1\": \"x\",\n                \"some_comparison_2\": \"y\",\n                \"some_comparison_3\": \"z\",\n                \"some_comparison_4\": \"p\",\n                \"some_comparison_5\": \"q\",\n                \"some_comparison_6\": \"r\",\n            },\n        )\n\n        self.assert_compile(\n            fn_2,\n            \"some_comparison(:some_comparison_1, :some_comparison_2, \"\n            \":some_comparison_3, \"\n            \":some_comparison_4, ABC, :some_comparison_5)\",\n            checkparams={\n                \"some_comparison_1\": \"x\",\n                \"some_comparison_2\": \"y\",\n                \"some_comparison_3\": \"z\",\n                \"some_comparison_4\": \"p\",\n                \"some_comparison_5\": \"r\",\n            },\n        )\n",
      "name_column": 6,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "setup_test",
      "md_content": [
        "**setup_test**: The function of setup_test is to initialize the test environment by creating a deep copy of the function registry.\n\n**parameters**: This function does not take any parameters.\n\n**Code Description**: The `setup_test` function is responsible for preparing the test environment. It achieves this by creating a deep copy of the `_registry` attribute from the `functions` module and assigning it to the `_registry` attribute of the current instance. The `deepcopy` function from the `copy` module is used to ensure that the copied registry is independent of the original, preventing any unintended side effects during testing. This setup is crucial for ensuring that tests run in an isolated environment, where modifications to the registry do not affect other tests or the main application.\n\n**Note**: Ensure that the `functions` module and its `_registry` attribute are properly initialized before calling this function. Additionally, the use of `deepcopy` implies that the registry should be serializable and not contain any unpicklable objects."
      ],
      "code_start_line": 70,
      "code_end_line": 71,
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def setup_test(self):\n        self._registry = deepcopy(functions._registry)\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "teardown_test",
      "md_content": [
        "**teardown_test**: The function of teardown_test is to restore the state of the `_registry` attribute in the `functions` module to its original state after a test has been executed.\n\n**parameters**: This function does not take any parameters.\n\n**Code Description**: The `teardown_test` function is responsible for resetting the `_registry` attribute of the `functions` module to the value stored in the instance attribute `self._registry`. This is typically used in a testing context to ensure that the state of the `_registry` is restored after a test has been run, preventing any side effects from affecting subsequent tests. The function achieves this by directly assigning the value of `self._registry` to `functions._registry`.\n\n**Note**: This function assumes that `self._registry` holds the original state of `functions._registry` before the test was executed. It is crucial to ensure that `self._registry` is properly initialized with the correct state before the test begins to avoid unintended behavior during the teardown process."
      ],
      "code_start_line": 73,
      "code_end_line": 74,
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def teardown_test(self):\n        functions._registry = self._registry\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "test_compile",
      "md_content": [
        "**test_compile**: The function of test_compile is to verify the correct compilation of various SQL functions and a custom generic function across different SQL dialects.\n\n**parameters**: The parameters of this Function.\n· self: The instance of the test class, used to access the test methods and assertions.\n\n**Code Description**: \nThe `test_compile` function is designed to test the compilation of SQL functions and a custom generic function across all supported SQL dialects. It iterates over each dialect using the `all_dialects()` function. For each dialect, it performs the following steps:\n\n1. **Standard SQL Function Compilation**:\n   - It tests the compilation of standard SQL functions such as `current_timestamp()`, `localtime()`, and `nosuchfunction()` using the `assert_compile` method. The expected SQL string for each function is provided, and the function ensures that the compiled SQL matches the expected output for the given dialect.\n\n2. **Custom Generic Function Compilation**:\n   - It defines a custom generic function `fake_func` that inherits from `GenericFunction`. This function is initialized with a single argument and is configured to return an integer type.\n   - The function then tests the compilation of `fake_func(\"foo\")` using the `assert_compile` method. The expected SQL string is constructed using the `bindtemplate` specific to the dialect's parameter style, ensuring that the compiled SQL matches the expected format.\n\n3. **Cleanup**:\n   - After testing, the custom function `fake_func` is removed from the function registry to avoid any side effects in subsequent tests.\n\n**Note**: \n- The function relies on the `all_dialects()` function to retrieve all supported SQL dialects, ensuring comprehensive testing across different database systems.\n- The `assert_compile` method is used to compare the compiled SQL with the expected output, raising an assertion error if they do not match.\n- The custom function `fake_func` is dynamically added and removed from the function registry to maintain test isolation.\n\n**Output Example**: \nThe function does not return a value but ensures that the compiled SQL matches the expected output for each tested function and dialect. If any assertion fails, an `AssertionError` will be raised, indicating a mismatch between the compiled SQL and the expected result."
      ],
      "code_start_line": 76,
      "code_end_line": 103,
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def test_compile(self):\n        for dialect in all_dialects():\n            bindtemplate = BIND_TEMPLATES[dialect.paramstyle]\n            self.assert_compile(\n                func.current_timestamp(), \"CURRENT_TIMESTAMP\", dialect=dialect\n            )\n            self.assert_compile(func.localtime(), \"LOCALTIME\", dialect=dialect)\n            self.assert_compile(\n                func.nosuchfunction(), \"nosuchfunction()\", dialect=dialect\n            )\n\n            # test generic function compile\n            class fake_func(GenericFunction):\n                inherit_cache = True\n                __return_type__ = sqltypes.Integer\n\n                def __init__(self, arg, **kwargs):\n                    GenericFunction.__init__(self, arg, **kwargs)\n\n            self.assert_compile(\n                fake_func(\"foo\"),\n                \"fake_func(%s)\"\n                % bindtemplate\n                % {\"name\": \"fake_func_1\", \"position\": 1},\n                dialect=dialect,\n            )\n\n            functions._registry[\"_default\"].pop(\"fake_func\")\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "ClassDef",
      "name": "fake_func",
      "md_content": [
        "**fake_func**: The function of fake_func is to serve as a placeholder or mock function within a testing or development environment, specifically designed to simulate a function that returns an integer value.\n\n**attributes**: The attributes of this Class.\n· inherit_cache: A boolean attribute set to True, indicating that the function inherits caching behavior from its parent class.\n· __return_type__: A class attribute specifying the return type of the function as an SQL integer type (sqltypes.Integer).\n\n**Code Description**: The fake_func class is a subclass of GenericFunction, which is typically used in scenarios where a function needs to be simulated or mocked for testing purposes. The class is initialized with a single positional argument, `arg`, and accepts additional keyword arguments (`**kwargs`). The initialization method (`__init__`) calls the parent class's initialization method, ensuring that the base functionality of GenericFunction is preserved. The `__return_type__` attribute is explicitly set to `sqltypes.Integer`, indicating that the function is expected to return an integer value when executed. The `inherit_cache` attribute is set to True, suggesting that the function will utilize caching behavior inherited from its parent class, which can improve performance in certain contexts.\n\n**Note**: This class is primarily intended for use in testing or development environments where a mock function is required. It should not be used in production code unless its behavior is explicitly understood and required. The `__return_type__` attribute is crucial for ensuring that the function's return type is correctly interpreted by the system, particularly in SQL-related contexts.\n\n**Output Example**: When the fake_func is invoked, it will return an integer value as specified by the `__return_type__` attribute. For example, if the function is called with an argument that results in a calculation or lookup, it might return a value like `42`."
      ],
      "code_start_line": 88,
      "code_end_line": 93,
      "params": [],
      "have_return": true,
      "code_content": "            class fake_func(GenericFunction):\n                inherit_cache = True\n                __return_type__ = sqltypes.Integer\n\n                def __init__(self, arg, **kwargs):\n                    GenericFunction.__init__(self, arg, **kwargs)\n",
      "name_column": 18,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [
        "**__init__**: The function of __init__ is to initialize an instance of the class by calling the parent class's initialization method.\n\n**parameters**: The parameters of this Function.\n· arg: A required argument passed to the initialization method. The exact type and purpose of this argument depend on the context in which the class is used.\n· **kwargs: A dictionary of keyword arguments that can be passed to the initialization method. These arguments are forwarded to the parent class's initialization method.\n\n**Code Description**: The __init__ method is the constructor for the class. It takes a required argument `arg` and any additional keyword arguments (`**kwargs`). Inside the method, it calls the `__init__` method of the parent class (`GenericFunction`) with the provided `arg` and `**kwargs`. This ensures that the parent class is properly initialized before any additional setup is performed in the child class. The use of `**kwargs` allows for flexibility in passing additional parameters to the parent class without explicitly defining them in the child class's constructor.\n\n**Note**: When using this class, ensure that the `arg` parameter is provided and that any additional keyword arguments are compatible with the parent class's initialization method. Failure to do so may result in initialization errors or unexpected behavior."
      ],
      "code_start_line": 92,
      "code_end_line": 93,
      "params": [
        "self",
        "arg"
      ],
      "have_return": false,
      "code_content": "                def __init__(self, arg, **kwargs):\n                    GenericFunction.__init__(self, arg, **kwargs)\n",
      "name_column": 20,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "test_operators_custom",
      "md_content": [
        "**test_operators_custom**: The function of test_operators_custom is to test the compilation of custom SQL operators with or without a custom function implementation.\n\n**parameters**: The parameters of this Function.\n· op: The operator function to be tested.\n· other: The operand or value to be used in conjunction with the operator.\n· expected: The expected SQL string output after compilation.\n· use_custom: A boolean flag indicating whether to use a custom function implementation or a predefined function.\n\n**Code Description**: \nThe `test_operators_custom` function is designed to verify the correct compilation of SQL expressions involving custom operators. The function accepts four parameters: `op`, `other`, `expected`, and `use_custom`. \n\nIf `use_custom` is `True`, the function defines a custom SQL function named `MyFunc` using the `FunctionElement` class. This custom function is configured with a specific name (`myfunc`) and a return type (`Integer`). The function also includes a compilation rule using the `@compiles` decorator, which specifies how the custom function should be translated into SQL. The SQL expression is then constructed using the provided operator (`op`) applied to the custom function and the `other` operand.\n\nIf `use_custom` is `False`, the function constructs the SQL expression using a predefined function (`func.myfunc`) with the same return type (`Integer`).\n\nFinally, the function uses `self.assert_compile` to verify that the compiled SQL expression matches the expected SQL string. The `assert_compile` method checks the SQL output against the provided `expected` string, ensuring that the compilation process is correct. The `literal_binds`, `render_postcompile`, and `dialect` parameters are used to control the compilation behavior and ensure accurate comparison.\n\n**Note**: \n- The `use_custom` parameter determines whether the test uses a custom function implementation or a predefined function. This allows for testing both scenarios within the same function.\n- The `expected` parameter should be carefully crafted to match the exact SQL output that the function is expected to generate.\n- The `dialect=\"default_enhanced\"` parameter ensures that the SQL compilation is tested against a specific SQL dialect, which may affect the output.\n\n**Output Example**: \nThe function does not return a value directly but instead asserts that the compiled SQL matches the expected output. For example, if the expected SQL string is `\"SELECT 1 WHERE myfunc(5) = 10\"`, the function will pass if the compiled SQL matches this string exactly."
      ],
      "code_start_line": 112,
      "code_end_line": 134,
      "params": [
        "self",
        "op",
        "other",
        "expected",
        "use_custom"
      ],
      "have_return": true,
      "code_content": "    def test_operators_custom(self, op, other, expected, use_custom):\n        if use_custom:\n\n            class MyFunc(FunctionElement):\n                inherit_cache = True\n                name = \"myfunc\"\n                type = Integer()\n\n            @compiles(MyFunc)\n            def visit_myfunc(element, compiler, **kw):\n                return \"myfunc(%s)\" % compiler.process(element.clauses, **kw)\n\n            expr = op(MyFunc(), other)\n        else:\n            expr = op(func.myfunc(type_=Integer), other)\n\n        self.assert_compile(\n            select(1).where(expr),\n            \"SELECT 1 WHERE %s\" % (expected,),\n            literal_binds=True,\n            render_postcompile=True,\n            dialect=\"default_enhanced\",\n        )\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "ClassDef",
      "name": "MyFunc",
      "md_content": [
        "**MyFunc**: The function of MyFunc is to define a custom SQL function that returns an integer value.\n\n**attributes**: The attributes of this Class.\n· inherit_cache: A boolean attribute set to True, indicating that the function's result can be cached for reuse.\n· name: A string attribute set to \"myfunc\", representing the name of the SQL function.\n· type: An attribute set to Integer(), specifying that the return type of the function is an integer.\n\n**Code Description**: The MyFunc class is a subclass of FunctionElement, which is typically used in SQLAlchemy to define custom SQL functions. By setting `inherit_cache = True`, the class ensures that the results of this function can be cached, improving performance by avoiding redundant computations. The `name` attribute is set to \"myfunc\", which will be the name of the function when used in SQL queries. The `type` attribute is set to `Integer()`, indicating that the function will return an integer value. This class is designed to be used in scenarios where a custom SQL function returning an integer is required.\n\n**Note**: When using MyFunc, ensure that the function is properly registered and utilized within the SQLAlchemy environment. The caching behavior (`inherit_cache = True`) should be considered in contexts where the function's output is expected to remain consistent for the same inputs."
      ],
      "code_start_line": 115,
      "code_end_line": 118,
      "params": [],
      "have_return": false,
      "code_content": "            class MyFunc(FunctionElement):\n                inherit_cache = True\n                name = \"myfunc\"\n                type = Integer()\n",
      "name_column": 18,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "visit_myfunc",
      "md_content": [
        "**visit_myfunc**: The function of visit_myfunc is to generate a string representation of a custom function call, specifically for a function named \"myfunc\", by processing its clauses using a provided compiler.\n\n**parameters**: The parameters of this Function.\n· element: The element containing the clauses that need to be processed. This is typically an object or structure that holds the necessary data for the function call.\n· compiler: The compiler object responsible for processing the clauses of the element. This object must have a `process` method that can handle the clauses.\n· **kw: Additional keyword arguments that can be passed to the compiler's `process` method for further customization or processing.\n\n**Code Description**: The `visit_myfunc` function takes an `element`, a `compiler`, and optional keyword arguments. It constructs a string representation of a function call to \"myfunc\" by processing the clauses of the `element` using the `compiler.process` method. The result of this processing is then formatted into the string \"myfunc(%s)\", where `%s` is replaced by the processed clauses. This function is typically used in scenarios where custom function calls need to be dynamically generated or compiled, such as in code generation or query compilation.\n\n**Note**: Ensure that the `compiler` object passed to this function has a `process` method capable of handling the `element.clauses`. Additionally, any keyword arguments provided should be compatible with the `process` method of the compiler.\n\n**Output Example**: If the `element.clauses` are processed by the compiler to yield \"arg1, arg2\", the function will return the string \"myfunc(arg1, arg2)\"."
      ],
      "code_start_line": 121,
      "code_end_line": 122,
      "params": [
        "element",
        "compiler"
      ],
      "have_return": true,
      "code_content": "            def visit_myfunc(element, compiler, **kw):\n                return \"myfunc(%s)\" % compiler.process(element.clauses, **kw)\n",
      "name_column": 16,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "test_use_labels",
      "md_content": [
        "**test_use_labels**: The function of test_use_labels is to verify that the SQL query generated with a specific label style matches the expected output.\n\n**parameters**: The parameters of this Function.\n· self: The instance of the test class, used to access the assert_compile method and other class attributes.\n\n**Code Description**: The description of this Function.\nThe `test_use_labels` function is a test case that checks whether the SQL query generated with a specific label style is correct. It uses the `assert_compile` method to compare the generated SQL query with the expected SQL string. \n\nIn this function, the `select(func.foo())` constructs a SQL SELECT statement that calls the `foo()` function. The `set_label_style(LABEL_STYLE_TABLENAME_PLUS_COL)` method is used to set the label style for the query, which in this case is `LABEL_STYLE_TABLENAME_PLUS_COL`. This label style appends the table name and column name to the alias in the generated SQL query.\n\nThe expected SQL output is `\"SELECT foo() AS foo_1\"`, where `foo_1` is the alias generated by the label style. The `assert_compile` method is then used to verify that the generated SQL query matches this expected output.\n\n**Note**: Points to note about the use of the code\n- Ensure that the `LABEL_STYLE_TABLENAME_PLUS_COL` label style is correctly defined and applied to generate the expected alias format.\n- The `assert_compile` method must be implemented to compare the generated SQL query with the expected string and raise an assertion error if they do not match."
      ],
      "code_start_line": 136,
      "code_end_line": 140,
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def test_use_labels(self):\n        self.assert_compile(\n            select(func.foo()).set_label_style(LABEL_STYLE_TABLENAME_PLUS_COL),\n            \"SELECT foo() AS foo_1\",\n        )\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "test_use_labels_function_element",
      "md_content": [
        "**test_use_labels_function_element**: The function of test_use_labels_function_element is to test the compilation of a custom SQL function element with label styling applied.\n\n**parameters**: The parameters of this Function.\n· self: The instance of the test class, used to access assertion methods and other class attributes.\n\n**Code Description**: \nThe `test_use_labels_function_element` function defines a custom SQL function element named `max_` using the `FunctionElement` class. This custom function is designed to represent the SQL `max` function. The `inherit_cache` attribute is set to `True`, indicating that the function's caching behavior should be inherited from its parent class.\n\nA compilation function `visit_max` is then defined using the `@compiles` decorator. This function specifies how the `max_` function should be compiled into SQL. The `visit_max` function takes three parameters: `element` (the function element being compiled), `compiler` (the SQL compiler), and `**kw` (additional keyword arguments). The function returns a string that represents the SQL `max` function call, with the function's clauses processed by the compiler.\n\nFinally, the function uses the `assert_compile` method to verify that the SQL query generated by the `select` statement, which includes the custom `max_` function, matches the expected SQL string. The `set_label_style` method is used to apply a specific label style (`LABEL_STYLE_TABLENAME_PLUS_COL`) to the generated SQL.\n\n**Note**: \n- The `max_` function element is a custom implementation and should be used in contexts where the standard SQL `max` function is not sufficient or needs to be extended.\n- The `LABEL_STYLE_TABLENAME_PLUS_COL` label style is used to format the column labels in the generated SQL, which may affect the readability and structure of the output.\n\n**Output Example**: \nThe function does not return a value directly but asserts that the generated SQL matches the expected output. For example, the expected SQL output for the test case is:\n```\nSELECT max(:max_2, :max_3) AS max_1\n```"
      ],
      "code_start_line": 142,
      "code_end_line": 154,
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def test_use_labels_function_element(self):\n        class max_(FunctionElement):\n            name = \"max\"\n            inherit_cache = True\n\n        @compiles(max_)\n        def visit_max(element, compiler, **kw):\n            return \"max(%s)\" % compiler.process(element.clauses, **kw)\n\n        self.assert_compile(\n            select(max_(5, 6)).set_label_style(LABEL_STYLE_TABLENAME_PLUS_COL),\n            \"SELECT max(:max_2, :max_3) AS max_1\",\n        )\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "ClassDef",
      "name": "max_",
      "md_content": [
        "**max_**: The function of max_ is to represent the SQL \"MAX\" aggregate function in a query.\n\n**attributes**: The attributes of this Class.\n· name: A string attribute set to \"max\", which defines the name of the SQL function this class represents.\n· inherit_cache: A boolean attribute set to True, indicating that this class inherits caching behavior from its parent class, FunctionElement.\n\n**Code Description**: The max_ class is a subclass of FunctionElement, which is typically used in SQLAlchemy to represent SQL functions in a query. The class is designed to encapsulate the SQL \"MAX\" aggregate function, which is used to find the maximum value in a set of values. By setting the name attribute to \"max\", this class ensures that when it is used in a query, it will generate the appropriate SQL syntax for the \"MAX\" function. The inherit_cache attribute is set to True, meaning that this class will inherit caching behavior from its parent class, FunctionElement, which can improve performance by reusing previously computed results when possible.\n\n**Note**: When using the max_ class in a query, ensure that it is applied to a column or expression that is compatible with the \"MAX\" function in SQL. This class is typically used in conjunction with SQLAlchemy's ORM or Core to construct queries that require the maximum value of a particular column or expression."
      ],
      "code_start_line": 143,
      "code_end_line": 145,
      "params": [],
      "have_return": false,
      "code_content": "        class max_(FunctionElement):\n            name = \"max\"\n            inherit_cache = True\n",
      "name_column": 14,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "visit_max",
      "md_content": [
        "**visit_max**: The function of visit_max is to generate a SQL `MAX` function expression for a given element using a compiler.\n\n**parameters**: The parameters of this Function.\n· element: The element for which the `MAX` function expression is to be generated. This element typically contains clauses that need to be processed.\n· compiler: The compiler object responsible for processing the clauses of the element. It is used to convert the clauses into a format suitable for SQL.\n· **kw: Additional keyword arguments that may be passed to the compiler's `process` method for further customization.\n\n**Code Description**: The `visit_max` function takes an `element`, a `compiler`, and optional keyword arguments. It processes the clauses of the `element` using the `compiler.process` method, which converts the clauses into a SQL-compatible format. The function then returns a string that represents the SQL `MAX` function applied to the processed clauses. The format of the returned string is `max(processed_clauses)`, where `processed_clauses` is the result of the `compiler.process` method.\n\n**Note**: Ensure that the `compiler` object passed to this function has a `process` method capable of handling the clauses of the `element`. The `element` should contain valid clauses that can be processed into a SQL expression.\n\n**Output Example**: If the `element.clauses` are processed into the string `column_name`, the function will return `max(column_name)`."
      ],
      "code_start_line": 148,
      "code_end_line": 149,
      "params": [
        "element",
        "compiler"
      ],
      "have_return": true,
      "code_content": "        def visit_max(element, compiler, **kw):\n            return \"max(%s)\" % compiler.process(element.clauses, **kw)\n",
      "name_column": 12,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "test_underscores",
      "md_content": [
        "**test_underscores**: The function of test_underscores is to verify the correct compilation of a function named `if_` into its expected string representation \"if()\".\n\n**parameters**: The parameters of this Function.\n· self: Represents the instance of the test class, allowing access to the class's methods and attributes, including the `assert_compile` method.\n\n**Code Description**: The `test_underscores` function is a unit test method designed to validate the behavior of a function named `if_`. The function calls `func.if_()` and uses the `assert_compile` method to check if the output of `func.if_()` matches the expected string \"if()\". The `assert_compile` method is likely a custom assertion method that compares the compiled output of a function to an expected string. This test ensures that the function `if_` is correctly compiled into the string \"if()\", which is crucial for verifying the proper handling of function names containing underscores.\n\n**Note**: This test assumes that the `func.if_()` function and the `assert_compile` method are properly defined and functional. Ensure that the `func` module and the `assert_compile` method are correctly implemented before running this test."
      ],
      "code_start_line": 156,
      "code_end_line": 157,
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def test_underscores(self):\n        self.assert_compile(func.if_(), \"if()\")\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "test_underscores_packages",
      "md_content": [
        "**test_underscores_packages**: The function of test_underscores_packages is to verify the correct compilation of a function call that includes underscores in package and method names.\n\n**parameters**: The function does not take any external parameters. It uses the `self` parameter, which is a reference to the current instance of the test class.\n\n**Code Description**: The `test_underscores_packages` function is a test case that checks whether a specific function call, involving nested packages and methods with underscores in their names, compiles correctly. The function calls `self.assert_compile`, which is a method used to assert that the given function call compiles to the expected string representation. In this case, the function call `func.foo_.bar_.if_()` is expected to compile to the string `\"foo.bar.if()\"`. This test ensures that the compiler or interpreter correctly handles package and method names containing underscores.\n\n**Note**: This test is specifically designed to validate the handling of underscores in package and method names. It assumes that the `func.foo_.bar_.if_()` function call is valid and that the `assert_compile` method is correctly implemented to compare the compiled output with the expected string. Ensure that the `func` module and its nested packages (`foo_`, `bar_`, and `if_`) are properly defined and accessible for this test to pass."
      ],
      "code_start_line": 159,
      "code_end_line": 160,
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def test_underscores_packages(self):\n        self.assert_compile(func.foo_.bar_.if_(), \"foo.bar.if()\")\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "test_uppercase",
      "md_content": [
        "**test_uppercase**: The function of test_uppercase is to verify that the unregistered function is compiled correctly with uppercase formatting.\n\n**parameters**: The parameters of this Function.\n· self: Represents the instance of the test class, allowing access to its methods and attributes.\n\n**Code Description**: The description of this Function.\nThe `test_uppercase` function is a test case designed to ensure that the unregistered function (`func.UNREGISTERED_FN()`) is compiled correctly with its name in uppercase. The function uses the `assert_compile` method to compare the output of the unregistered function with the expected string \"UNREGISTERED_FN()\". This test ensures that the function name is treated in a case-insensitive manner during compilation, as indicated by the comment in the code. The test is part of a larger suite aimed at validating the correctness of function compilation in the system.\n\n**Note**: Points to note about the use of the code\n- The test assumes that the function name should be compiled in uppercase, which is currently required for case insensitivity.\n- The `assert_compile` method is used to verify the correctness of the compilation process, and its behavior should be consistent with the expected output.\n- This test is specific to the unregistered function and may need to be updated if the function's behavior or naming convention changes."
      ],
      "code_start_line": 162,
      "code_end_line": 164,
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def test_uppercase(self):\n        # for now, we need to keep case insensitivity\n        self.assert_compile(func.UNREGISTERED_FN(), \"UNREGISTERED_FN()\")\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "test_uppercase_packages",
      "md_content": [
        "**test_uppercase_packages**: The function of test_uppercase_packages is to verify that the compilation of a specific function call, represented in uppercase package format, produces the expected output.\n\n**parameters**: The parameters of this Function.\n· self: Represents the instance of the test class. It is used to access methods and attributes within the test class.\n\n**Code Description**: The function `test_uppercase_packages` is a test case designed to ensure that a function call, structured in uppercase package format (e.g., `FOO.BAR.NOW()`), compiles correctly and produces the expected string output. The function uses the `assert_compile` method to compare the result of the function call `func.FOO.BAR.NOW()` with the string `\"FOO.BAR.NOW()\"`. If the compiled output matches the expected string, the test passes; otherwise, it fails. This test is particularly important for maintaining case insensitivity in the compilation process, ensuring that the system correctly handles uppercase package names.\n\n**Note**: This test assumes that the function `func.FOO.BAR.NOW()` is properly defined and that the `assert_compile` method is implemented to compare the compiled output with the expected string. Ensure that the function and method are correctly set up before running this test."
      ],
      "code_start_line": 166,
      "code_end_line": 168,
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def test_uppercase_packages(self):\n        # for now, we need to keep case insensitivity\n        self.assert_compile(func.FOO.BAR.NOW(), \"FOO.BAR.NOW()\")\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "test_mixed_case",
      "md_content": [
        "**test_mixed_case**: The function of test_mixed_case is to verify that a function with mixed case naming is correctly compiled and matches the expected output.\n\n**parameters**: The parameters of this Function.\n· self: Represents the instance of the test class, allowing access to its methods and attributes.\n\n**Code Description**: The function `test_mixed_case` is a test case designed to ensure that a function with mixed case naming, such as `SomeFunction`, is correctly compiled and produces the expected output. The function uses the `assert_compile` method to compare the compiled output of `func.SomeFunction()` with the string `\"SomeFunction()\"`. This test ensures that the compilation process preserves the case sensitivity of the function name, which is crucial for maintaining the correct behavior of the code.\n\n**Note**: This test assumes that the function `SomeFunction` is defined elsewhere in the codebase and is accessible via the `func` module or object. The test is currently focused on verifying case insensitivity, so any changes to the function's naming convention or compilation logic should be carefully reviewed to avoid breaking this test."
      ],
      "code_start_line": 170,
      "code_end_line": 172,
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def test_mixed_case(self):\n        # for now, we need to keep case insensitivity\n        self.assert_compile(func.SomeFunction(), \"SomeFunction()\")\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "test_mixed_case_packages",
      "md_content": [
        "**test_mixed_case_packages**: The function of test_mixed_case_packages is to verify that a function call with mixed-case package names is correctly compiled into the expected string representation.\n\n**parameters**: The parameters of this Function.\n· self: The instance of the test class, used to access the test methods and assertions.\n\n**Code Description**: The description of this Function.\nThe `test_mixed_case_packages` function is a test case that checks whether a function call involving mixed-case package names is correctly compiled into its expected string representation. The function uses the `assert_compile` method to compare the output of `func.Foo.Bar.SomeFunction()` with the string `\"Foo.Bar.SomeFunction()\"`. This test ensures that the compilation process preserves the case sensitivity of the package and function names, which is crucial for maintaining the correct behavior of the code when dealing with mixed-case identifiers.\n\n**Note**: Points to note about the use of the code.\n- This test assumes that the `assert_compile` method is already defined and correctly implemented in the test class.\n- The test is designed to ensure that the case sensitivity of package and function names is preserved during compilation, which is important for compatibility and correctness in environments where case sensitivity matters."
      ],
      "code_start_line": 174,
      "code_end_line": 178,
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def test_mixed_case_packages(self):\n        # for now, we need to keep case insensitivity\n        self.assert_compile(\n            func.Foo.Bar.SomeFunction(), \"Foo.Bar.SomeFunction()\"\n        )\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "test_quote_special_chars",
      "md_content": [
        "**test_quote_special_chars**: The function of test_quote_special_chars is to verify that special characters in function names are properly quoted during compilation.\n\n**parameters**: The function does not take any parameters explicitly. It operates within the context of a test case class, utilizing the `self` parameter to access the `assert_compile` method.\n\n**Code Description**: \nThe `test_quote_special_chars` function is a test case designed to ensure that function names containing special characters, such as spaces, are correctly quoted when compiled. The function uses the `assert_compile` method to compare the compiled output of a dynamically accessed function with the expected quoted string. Specifically, it retrieves a function named \"im a function\" using the `getattr` function and then compiles it. The expected output is the function name enclosed in double quotes, followed by parentheses, i.e., `\"im a function\"()`. The `assert_compile` method checks that the actual compiled output matches this expected format.\n\n**Note**: This test is crucial for ensuring that identifiers with special characters are handled correctly during compilation, preventing potential syntax errors or misinterpretations in the code. Developers should ensure that any function names containing special characters are properly quoted to maintain compatibility with the compilation process."
      ],
      "code_start_line": 180,
      "code_end_line": 184,
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def test_quote_special_chars(self):\n        # however we need to be quoting any other identifiers\n        self.assert_compile(\n            getattr(func, \"im a function\")(), '\"im a function\"()'\n        )\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "test_quote_special_chars_packages",
      "md_content": [
        "**test_quote_special_chars_packages**: The function of test_quote_special_chars_packages is to verify that identifiers containing special characters (such as spaces) in package and function names are properly quoted during compilation.\n\n**parameters**: This function does not take any parameters explicitly. It operates within the context of the test class and uses the `self` reference to access the `assert_compile` method.\n\n**Code Description**: \nThe function `test_quote_special_chars_packages` tests the correct handling of identifiers with special characters (e.g., spaces) in package and function names during SQL compilation. It uses the `assert_compile` method to compare the compiled output of a function call with an expected string. \n\nThe function call is constructed dynamically using `getattr` to access nested attributes representing package and function names with spaces. Specifically:\n1. `getattr(func, \"im foo package\")` accesses the package named \"im foo package\".\n2. `getattr(getattr(func, \"im foo package\"), \"im bar package\")` accesses the sub-package named \"im bar package\" within the \"im foo package\".\n3. `getattr(getattr(getattr(func, \"im foo package\"), \"im bar package\"), \"im a function\")` accesses the function named \"im a function\" within the \"im bar package\".\n4. The function is then called using `()`.\n\nThe expected output is a string where each identifier (package and function names) is properly quoted using double quotes (`\"`), resulting in `\"im foo package\".\"im bar package\".\"im a function\"()`. The `assert_compile` method ensures that the compiled SQL matches this expected format.\n\n**Note**: This test ensures that identifiers with special characters, such as spaces, are correctly quoted in the generated SQL. This is crucial for compatibility with SQL databases that require quoted identifiers for names containing special characters or reserved keywords."
      ],
      "code_start_line": 186,
      "code_end_line": 194,
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def test_quote_special_chars_packages(self):\n        # however we need to be quoting any other identifiers\n        self.assert_compile(\n            getattr(\n                getattr(getattr(func, \"im foo package\"), \"im bar package\"),\n                \"im a function\",\n            )(),\n            '\"im foo package\".\"im bar package\".\"im a function\"()',\n        )\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "test_generic_now",
      "md_content": [
        "**test_generic_now**: The function of test_generic_now is to verify the correctness of the `func.now()` SQL function across different database dialects by ensuring it compiles to the expected SQL expression and returns the correct data type.\n\n**parameters**: The function does not take any external parameters. It operates within the context of the test class and uses predefined database dialects for testing.\n\n**Code Description**: \nThe `test_generic_now` function performs two main tasks:\n1. **Type Assertion**: It first checks that the return type of `func.now()` is an instance of `sqltypes.DateTime`. This ensures that the function `func.now()` returns a valid datetime type as expected.\n2. **Dialect-Specific Compilation Assertion**: The function then iterates over a list of tuples, where each tuple contains an expected SQL expression and a corresponding database dialect. For each tuple, it asserts that the compilation of `func.now()` using the specified dialect matches the expected SQL expression. The dialects tested include SQLite, PostgreSQL, MySQL, and Oracle, and the expected SQL expressions are `\"CURRENT_TIMESTAMP\"` for SQLite and Oracle, and `\"now()\"` for PostgreSQL and MySQL.\n\n**Note**: This function is designed to test the compatibility and correctness of the `func.now()` function across multiple database systems. It assumes that the `func.now()` function and the database dialects are correctly implemented and available in the testing environment. Ensure that the dialects and SQL expressions used in the test are up-to-date with the database specifications."
      ],
      "code_start_line": 196,
      "code_end_line": 205,
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def test_generic_now(self):\n        assert isinstance(func.now().type, sqltypes.DateTime)\n\n        for ret, dialect in [\n            (\"CURRENT_TIMESTAMP\", sqlite.dialect()),\n            (\"now()\", postgresql.dialect()),\n            (\"now()\", mysql.dialect()),\n            (\"CURRENT_TIMESTAMP\", oracle.dialect()),\n        ]:\n            self.assert_compile(func.now(), ret, dialect=dialect)\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "test_generic_random",
      "md_content": [
        "**test_generic_random**: The function of test_generic_random is to test the behavior and compilation of the `random` function across different SQL dialects.\n\n**parameters**: The parameters of this Function.\n· self: The instance of the test class, used to access assertion methods and other class attributes.\n\n**Code Description**: The description of this Function.\nThe `test_generic_random` function performs the following tasks:\n1. It first asserts that the type of the result of `func.random()` without any type specification is `sqltypes.NULLTYPE`, indicating that the function returns a NULL type by default.\n2. It then asserts that when the `random` function is called with an explicit `Integer` type, the result type is indeed an instance of `Integer`.\n3. The function iterates over a list of tuples, where each tuple contains an expected SQL string representation of the `random` function and the corresponding SQL dialect. The dialects tested include SQLite, PostgreSQL, MySQL, and Oracle.\n4. For each dialect, the function uses `self.assert_compile` to verify that the `random` function compiles to the expected SQL string when used with that dialect. This ensures that the `random` function behaves correctly across different database systems.\n\n**Note**: Points to note about the use of the code\n- This function is part of a test suite and is designed to validate the correctness of the `random` function's implementation and its compatibility with various SQL dialects.\n- The function assumes that the `func.random()` method and the `sqltypes.NULLTYPE`, `Integer`, and dialect objects are properly imported and available in the context where this test is executed.\n- The `self.assert_compile` method is used to compare the compiled SQL output with the expected string, ensuring that the function behaves as intended across different database backends."
      ],
      "code_start_line": 207,
      "code_end_line": 217,
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def test_generic_random(self):\n        assert func.random().type == sqltypes.NULLTYPE\n        assert isinstance(func.random(type_=Integer).type, Integer)\n\n        for ret, dialect in [\n            (\"random()\", sqlite.dialect()),\n            (\"random()\", postgresql.dialect()),\n            (\"rand()\", mysql.dialect()),\n            (\"random()\", oracle.dialect()),\n        ]:\n            self.assert_compile(func.random(), ret, dialect=dialect)\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "test_return_type_aggregate_strings",
      "md_content": [
        "**test_return_type_aggregate_strings**: The function of test_return_type_aggregate_strings is to verify that the return type of the `aggregate_strings` function is of type `String`.\n\n**parameters**: The parameters of this Function.\n· self: The instance of the test class, used to access test methods and attributes.\n\n**Code Description**: \nThe function `test_return_type_aggregate_strings` is a test method designed to ensure that the `aggregate_strings` function returns a value with a type affinity of `String`. \n\n1. A table `t` is created with a single column named `value` of type `String`.\n2. The `aggregate_strings` function is called with the column `t.c.value` and a delimiter `\",\"`. This function is expected to aggregate the string values in the column, separated by the specified delimiter.\n3. The `is_` function is used to assert that the type affinity of the expression returned by `aggregate_strings` is indeed `String`. This ensures that the function behaves as expected in terms of its return type.\n\n**Note**: \n- This test is crucial for validating that the `aggregate_strings` function adheres to the expected type constraints, which is important for type safety and consistency in the application.\n- The test assumes that the `aggregate_strings` function is correctly implemented and that the `String` type is properly defined in the context of the application.\n\n**Output Example**: \nSince this is a test function, it does not return a value in the traditional sense. Instead, it will pass if the assertion is true (i.e., the type affinity of the expression is `String`) or fail if the assertion is false. No explicit output is produced other than the test result (pass/fail)."
      ],
      "code_start_line": 219,
      "code_end_line": 222,
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def test_return_type_aggregate_strings(self):\n        t = table(\"t\", column(\"value\", String))\n        expr = func.aggregate_strings(t.c.value, \",\")\n        is_(expr.type._type_affinity, String)\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "test_aggregate_strings",
      "md_content": [
        "**test_aggregate_strings**: The function of test_aggregate_strings is to test the compilation of an SQL statement that aggregates strings using a custom SQL function.\n\n**parameters**: The parameters of this Function.\n· expected_sql: A string representing the expected SQL output after compilation.\n· dialect: The SQL dialect to be used for compiling the SQL statement.\n\n**Code Description**: The function `test_aggregate_strings` is designed to verify that the SQL statement generated by the `aggregate_strings` function compiles correctly according to the specified SQL dialect. The function begins by creating a table `t` with a single column named `value` of type `String`. It then constructs a SQL `SELECT` statement using the `aggregate_strings` function, which aggregates the values in the `value` column, separated by a comma. The `assert_compile` method is used to compare the compiled SQL statement with the `expected_sql` parameter, ensuring that the output matches the expected SQL syntax for the given dialect.\n\n**Note**: This function is primarily used for testing purposes to ensure that the `aggregate_strings` function behaves as expected across different SQL dialects. It is important to provide the correct `expected_sql` and `dialect` parameters to accurately validate the SQL compilation."
      ],
      "code_start_line": 251,
      "code_end_line": 255,
      "params": [
        "self",
        "expected_sql",
        "dialect"
      ],
      "have_return": false,
      "code_content": "    def test_aggregate_strings(self, expected_sql, dialect):\n        t = table(\"t\", column(\"value\", String))\n        stmt = select(func.aggregate_strings(t.c.value, \",\"))\n\n        self.assert_compile(stmt, expected_sql, dialect=dialect)\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "test_cube_operators",
      "md_content": [
        "**test_cube_operators**: The function of test_cube_operators is to test the compilation of SQL statements that use cube, rollup, and grouping sets operators for grouping data in a table.\n\n**parameters**: The parameters of this Function.\n· self: The instance of the test class, used to access the assert_compile method and other class attributes.\n\n**Code Description**: The description of this Function.\nThe function `test_cube_operators` is designed to verify the correct compilation of SQL queries that utilize advanced grouping operators such as `CUBE`, `ROLLUP`, and `GROUPING SETS`. These operators are used in SQL to generate multiple grouping sets in a single query, which is particularly useful for generating subtotals and grand totals in reports.\n\n1. **Table Creation**: The function begins by defining a table `t` with columns `value`, `x`, `y`, `z`, and `q`. This table is used as the basis for constructing SQL queries.\n\n2. **Basic Query Construction**: A SQL `SELECT` statement is created using the `select` function, which sums the `value` column from the table `t`. This forms the core of the query that will be grouped using different operators.\n\n3. **CUBE Operator Test**: The function tests the `CUBE` operator by grouping the query results based on columns `x` and `y`. The `assert_compile` method is used to verify that the SQL statement is correctly compiled into the expected string: `\"SELECT sum(t.value) AS sum_1 FROM t GROUP BY CUBE(t.x, t.y)\"`.\n\n4. **ROLLUP Operator Test**: Similarly, the `ROLLUP` operator is tested by grouping the query results based on columns `x` and `y`. The `assert_compile` method checks that the SQL statement compiles to: `\"SELECT sum(t.value) AS sum_1 FROM t GROUP BY ROLLUP(t.x, t.y)\"`.\n\n5. **GROUPING SETS Operator Test**: The function then tests the `GROUPING SETS` operator by grouping the query results based on columns `x` and `y`. The `assert_compile` method ensures that the SQL statement compiles to: `\"SELECT sum(t.value) AS sum_1 FROM t GROUP BY GROUPING SETS(t.x, t.y)\"`.\n\n6. **Complex GROUPING SETS Test**: Finally, the function tests a more complex use of the `GROUPING SETS` operator by grouping the query results based on tuples of columns `(x, y)` and `(z, q)`. The `assert_compile` method verifies that the SQL statement compiles to: `\"SELECT sum(t.value) AS sum_1 FROM t GROUP BY GROUPING SETS((t.x, t.y), (t.z, t.q))\"`.\n\n**Note**: Points to note about the use of the code\n- The function relies on the `assert_compile` method to verify the correctness of the SQL compilation. Ensure that this method is properly implemented in the test class.\n- The function assumes that the SQL dialect being used supports the `CUBE`, `ROLLUP`, and `GROUPING SETS` operators. If the dialect does not support these operators, the tests will fail.\n- The function is part of a test suite, so it should be executed in the context of a testing framework that supports assertions and test case management."
      ],
      "code_start_line": 257,
      "code_end_line": 293,
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def test_cube_operators(self):\n        t = table(\n            \"t\",\n            column(\"value\"),\n            column(\"x\"),\n            column(\"y\"),\n            column(\"z\"),\n            column(\"q\"),\n        )\n\n        stmt = select(func.sum(t.c.value))\n\n        self.assert_compile(\n            stmt.group_by(func.cube(t.c.x, t.c.y)),\n            \"SELECT sum(t.value) AS sum_1 FROM t GROUP BY CUBE(t.x, t.y)\",\n        )\n\n        self.assert_compile(\n            stmt.group_by(func.rollup(t.c.x, t.c.y)),\n            \"SELECT sum(t.value) AS sum_1 FROM t GROUP BY ROLLUP(t.x, t.y)\",\n        )\n\n        self.assert_compile(\n            stmt.group_by(func.grouping_sets(t.c.x, t.c.y)),\n            \"SELECT sum(t.value) AS sum_1 FROM t \"\n            \"GROUP BY GROUPING SETS(t.x, t.y)\",\n        )\n\n        self.assert_compile(\n            stmt.group_by(\n                func.grouping_sets(\n                    sql.tuple_(t.c.x, t.c.y), sql.tuple_(t.c.z, t.c.q)\n                )\n            ),\n            \"SELECT sum(t.value) AS sum_1 FROM t GROUP BY \"\n            \"GROUPING SETS((t.x, t.y), (t.z, t.q))\",\n        )\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "test_generic_annotation",
      "md_content": [
        "**test_generic_annotation**: The function of test_generic_annotation is to verify the behavior of a generic annotation applied to a SQL function and ensure that the SQL compilation process works as expected.\n\n**parameters**: The parameters of this Function.\n· self: The instance of the test class, used to access assertion methods and other test utilities.\n\n**Code Description**: \nThe function `test_generic_annotation` performs the following steps:\n1. It creates a SQL function `coalesce` with two arguments, \"x\" and \"y\". The `coalesce` function is a common SQL function that returns the first non-null value among its arguments.\n2. The `_annotate` method is then called on the `coalesce` function, applying a generic annotation `{\"foo\": \"bar\"}`. This annotation is typically used to attach metadata or additional information to the SQL function, which can be useful for debugging or further processing.\n3. Finally, the function calls `self.assert_compile` to verify that the SQL compilation of the annotated `coalesce` function produces the expected SQL string: `\"coalesce(:coalesce_1, :coalesce_2)\"`. This assertion ensures that the annotation does not interfere with the correct compilation of the SQL function.\n\n**Note**: \n- The `_annotate` method is used here to attach metadata to the SQL function, but it does not affect the SQL compilation process itself. The primary purpose of this test is to ensure that the annotation does not alter the expected SQL output.\n- The `coalesce` function is used as an example, but the test is focused on the annotation and compilation process rather than the specific behavior of `coalesce`."
      ],
      "code_start_line": 295,
      "code_end_line": 297,
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def test_generic_annotation(self):\n        fn = func.coalesce(\"x\", \"y\")._annotate({\"foo\": \"bar\"})\n        self.assert_compile(fn, \"coalesce(:coalesce_1, :coalesce_2)\")\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "test_annotation_dialect_specific",
      "md_content": [
        "**test_annotation_dialect_specific**: The function of test_annotation_dialect_specific is to verify that SQL function annotations do not affect the SQL compilation process when using a specific SQL dialect.\n\n**parameters**: The parameters of this Function.\n· self: The instance of the test class, used to access assertion methods and other class-specific functionalities.\n\n**Code Description**: The description of this Function.\nThe function begins by creating a SQL function object `fn` using `func.current_date()`, which represents the SQL `CURRENT_DATE` function. The `assert_compile` method is then called to verify that the SQL compilation of `fn` results in the string `\"CURRENT_DATE\"` when using the SQLite dialect. This ensures that the function compiles correctly for the specified dialect.\n\nNext, the function annotates the `fn` object with a dictionary `{\"foo\": \"bar\"}` using the `_annotate` method. This annotation is intended to add metadata to the SQL function object. After annotation, the `assert_compile` method is called again to verify that the SQL compilation of the annotated `fn` still results in the string `\"CURRENT_DATE\"` when using the SQLite dialect. This confirms that the annotation does not alter the SQL compilation output for the specified dialect.\n\n**Note**: Points to note about the use of the code\n- This test specifically checks the behavior of SQL function annotations in the context of the SQLite dialect. If other dialects are used, the behavior might differ.\n- The `_annotate` method is used to add metadata to the SQL function object, but this metadata does not affect the SQL compilation process in this case."
      ],
      "code_start_line": 299,
      "code_end_line": 304,
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def test_annotation_dialect_specific(self):\n        fn = func.current_date()\n        self.assert_compile(fn, \"CURRENT_DATE\", dialect=\"sqlite\")\n\n        fn = fn._annotate({\"foo\": \"bar\"})\n        self.assert_compile(fn, \"CURRENT_DATE\", dialect=\"sqlite\")\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "test_custom_default_namespace",
      "md_content": [
        "**test_custom_default_namespace**: The function of test_custom_default_namespace is to verify the behavior of a custom function defined within a custom default namespace and ensure it compiles correctly.\n\n**parameters**: The parameters of this Function.\n· self: The instance of the test class, used to access test methods and assertions.\n\n**Code Description**: The description of this Function.\nThe `test_custom_default_namespace` function is a test case that validates the functionality of a custom function defined within a custom default namespace. It begins by defining a class `myfunc` that inherits from `GenericFunction`. The `inherit_cache` attribute is set to `True`, indicating that the function should inherit caching behavior from its parent class. \n\nThe test then asserts two conditions:\n1. It checks that an instance of `func.myfunc()` is indeed an instance of the `myfunc` class, ensuring that the custom function is correctly instantiated.\n2. It verifies that the custom function compiles correctly by using the `self.assert_compile` method, which checks that the compiled output of `func.myfunc()` matches the expected string `\"myfunc()\"`.\n\nThis test ensures that the custom function behaves as expected within the specified namespace and that its compilation output is correct.\n\n**Note**: Points to note about the use of the code.\n- Ensure that the `GenericFunction` class and the `func` namespace are properly defined and imported in the test environment.\n- The `inherit_cache` attribute should be used carefully, as it affects the caching behavior of the function.\n- The `self.assert_compile` method must be implemented to correctly compare the compiled output with the expected string."
      ],
      "code_start_line": 306,
      "code_end_line": 311,
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def test_custom_default_namespace(self):\n        class myfunc(GenericFunction):\n            inherit_cache = True\n\n        assert isinstance(func.myfunc(), myfunc)\n        self.assert_compile(func.myfunc(), \"myfunc()\")\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "ClassDef",
      "name": "myfunc",
      "md_content": [
        "**myfunc**: The function of myfunc is to serve as a custom function within a namespace, inheriting caching behavior from its parent class.\n\n**attributes**: The attributes of this Class.\n· inherit_cache: A boolean attribute set to True, indicating that this function inherits caching behavior from its parent class, GenericFunction.\n\n**Code Description**: The `myfunc` class is a subclass of `GenericFunction`, which is typically used to define custom functions within a specific namespace. By setting `inherit_cache = True`, the class ensures that it inherits the caching mechanism from its parent class. This caching mechanism is useful for optimizing performance by storing and reusing the results of expensive function calls, avoiding redundant computations. The class itself does not define additional methods or attributes beyond this inheritance behavior, making it a lightweight extension of `GenericFunction`.\n\n**Note**: When using `myfunc`, ensure that the parent class `GenericFunction` provides the necessary caching infrastructure. If caching is not required, consider setting `inherit_cache` to False or overriding the caching behavior in a subclass."
      ],
      "code_start_line": 307,
      "code_end_line": 308,
      "params": [],
      "have_return": false,
      "code_content": "        class myfunc(GenericFunction):\n            inherit_cache = True\n",
      "name_column": 14,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "test_custom_type",
      "md_content": [
        "**test_custom_type**: The function of test_custom_type is to verify the behavior of a custom function type defined using the `GenericFunction` class, ensuring that the type is correctly assigned and that the function compiles as expected.\n\n**parameters**: This function does not take any parameters.\n\n**Code Description**: \nThe `test_custom_type` function performs the following steps:\n1. It defines a custom function class `myfunc` that inherits from `GenericFunction`. The `type` attribute of this class is set to `DateTime`, and the `inherit_cache` attribute is set to `True`. This indicates that the function will operate on `DateTime` type data and will inherit caching behavior.\n2. The function then asserts that the `type` attribute of an instance of `myfunc` is indeed an instance of `DateTime`. This ensures that the custom type assignment is correctly implemented.\n3. Finally, the function uses `self.assert_compile` to verify that the custom function `myfunc` compiles correctly and produces the expected output string \"myfunc()\".\n\n**Note**: \n- This test function is designed to validate the correct implementation and compilation of custom function types, particularly when using the `GenericFunction` class.\n- Ensure that the `GenericFunction` class and `DateTime` type are properly defined and imported in the context where this test is executed."
      ],
      "code_start_line": 313,
      "code_end_line": 319,
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def test_custom_type(self):\n        class myfunc(GenericFunction):\n            type = DateTime\n            inherit_cache = True\n\n        assert isinstance(func.myfunc().type, DateTime)\n        self.assert_compile(func.myfunc(), \"myfunc()\")\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "ClassDef",
      "name": "myfunc",
      "md_content": [
        "**myfunc**: The function of myfunc is to define a custom function that operates on DateTime data types, inheriting caching behavior from its parent class.\n\n**attributes**: The attributes of this Class.\n· type: Specifies the data type that the function operates on, which is DateTime in this case.\n· inherit_cache: A boolean attribute that determines whether the function inherits caching behavior from its parent class. It is set to True, enabling caching.\n\n**Code Description**: The `myfunc` class is a subclass of `GenericFunction`, designed to handle operations specifically for DateTime data types. By setting the `type` attribute to `DateTime`, it ensures that the function is tailored to work with DateTime objects. The `inherit_cache` attribute is set to `True`, indicating that the function will utilize caching mechanisms inherited from its parent class, which can improve performance by avoiding redundant computations. This class is typically used in scenarios where DateTime-specific operations need to be performed efficiently, leveraging caching to optimize repeated function calls.\n\n**Note**: When using `myfunc`, ensure that the input data is of the DateTime type to avoid runtime errors. Additionally, the caching behavior can be beneficial for performance but may require careful management in scenarios where data changes frequently, as cached results might become outdated."
      ],
      "code_start_line": 314,
      "code_end_line": 316,
      "params": [],
      "have_return": false,
      "code_content": "        class myfunc(GenericFunction):\n            type = DateTime\n            inherit_cache = True\n",
      "name_column": 14,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "test_custom_legacy_type",
      "md_content": [
        "**test_custom_legacy_type**: The function of test_custom_legacy_type is to verify the functionality of a custom legacy type by defining a custom function class and checking its return type.\n\n**parameters**: This function does not take any parameters.\n\n**Code Description**: \nThe `test_custom_legacy_type` function is designed to test the behavior of a custom legacy type system. Inside the function, a custom class `myfunc` is defined, which inherits from `GenericFunction`. The class has two key attributes:\n1. `inherit_cache = True`: This indicates that the function should inherit caching behavior from its parent class.\n2. `__return_type__ = DateTime`: This specifies that the return type of the function should be `DateTime`.\n\nAfter defining the `myfunc` class, the function asserts that the type of the result returned by `func.myfunc()` is an instance of `DateTime`. This assertion ensures that the custom function class correctly adheres to the specified return type.\n\n**Note**: This function is primarily used for testing purposes and assumes the existence of a `GenericFunction` base class and a `DateTime` type. It is important to ensure that these dependencies are available in the environment where this test is executed.\n\n**Output Example**: \nSince this function is a test and does not return a value, there is no output example. However, if the assertion passes, the test will complete successfully without raising any errors. If the assertion fails, an `AssertionError` will be raised, indicating that the custom function's return type does not match the expected `DateTime` type."
      ],
      "code_start_line": 321,
      "code_end_line": 327,
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def test_custom_legacy_type(self):\n        # in case someone was using this system\n        class myfunc(GenericFunction):\n            inherit_cache = True\n            __return_type__ = DateTime\n\n        assert isinstance(func.myfunc().type, DateTime)\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "ClassDef",
      "name": "myfunc",
      "md_content": [
        "**myfunc**: The function of myfunc is to serve as a custom function that returns a DateTime type value.\n\n**attributes**: The attributes of this Class.\n· inherit_cache: A boolean attribute set to True, indicating that the function's results can be cached for reuse.\n· __return_type__: Specifies the return type of the function, which is DateTime.\n\n**Code Description**: The `myfunc` class is a subclass of `GenericFunction`, which implies it is designed to be a reusable and customizable function within the system. The `inherit_cache = True` attribute ensures that the results of this function can be cached, improving performance by avoiding redundant computations. The `__return_type__ = DateTime` attribute explicitly defines that the function will return a value of type `DateTime`. This is useful for type checking and ensuring consistency in the data returned by the function.\n\n**Note**: When using `myfunc`, ensure that the context in which it is used supports the `DateTime` type, as this is the expected return type. Additionally, since `inherit_cache` is set to True, be mindful of the caching behavior, especially in scenarios where the function's output might change over time or with different inputs.\n\n**Output Example**: The output of `myfunc` will be a `DateTime` object, such as `2023-10-05T14:30:00`."
      ],
      "code_start_line": 323,
      "code_end_line": 325,
      "params": [],
      "have_return": true,
      "code_content": "        class myfunc(GenericFunction):\n            inherit_cache = True\n            __return_type__ = DateTime\n",
      "name_column": 14,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "test_case_sensitive",
      "md_content": [
        "**test_case_sensitive**: The function of test_case_sensitive is to verify that the case sensitivity of function names does not affect the type of the returned object when using the GenericFunction class.\n\n**parameters**: The parameters of this Function.\n· self: The instance of the test class. This parameter is automatically passed when the method is called.\n\n**Code Description**: The description of this Function.\nThe test_case_sensitive function defines a nested class MYFUNC that inherits from GenericFunction. This class has two attributes: inherit_cache, which is set to True, and type, which is set to DateTime. The function then performs a series of assertions to ensure that the type attribute of instances of MYFUNC remains consistent regardless of the case used in the function name. Specifically, it checks that the type attribute is an instance of DateTime for various case variations of the function name, including MYFUNC, MyFunc, mYfUnC, and myfunc. This ensures that the case sensitivity of the function name does not affect the functionality or the type of the returned object.\n\n**Note**: Points to note about the use of the code\n- This test function is designed to validate the behavior of the GenericFunction class in a case-sensitive context. It is important to ensure that the GenericFunction class is implemented in a way that is case-insensitive when it comes to function names.\n- The test assumes that the func module or object is available and correctly configured to return instances of the MYFUNC class when called with different case variations of the function name."
      ],
      "code_start_line": 329,
      "code_end_line": 337,
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def test_case_sensitive(self):\n        class MYFUNC(GenericFunction):\n            inherit_cache = True\n            type = DateTime\n\n        assert isinstance(func.MYFUNC().type, DateTime)\n        assert isinstance(func.MyFunc().type, DateTime)\n        assert isinstance(func.mYfUnC().type, DateTime)\n        assert isinstance(func.myfunc().type, DateTime)\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "ClassDef",
      "name": "MYFUNC",
      "md_content": [
        "**MYFUNC**: The function of MYFUNC is to serve as a specialized GenericFunction class for handling DateTime-related operations.\n\n**attributes**: The attributes of this Class.\n· inherit_cache: A boolean attribute set to True, indicating that the class inherits caching behavior from its parent class.\n· type: A class-level attribute set to DateTime, specifying the data type this function is designed to handle.\n\n**Code Description**: The MYFUNC class is a subclass of GenericFunction, which is typically used to define custom functions or operations within a framework or library. By setting `inherit_cache = True`, the class inherits caching mechanisms from its parent class, which can improve performance by reusing previously computed results. The `type` attribute is explicitly set to DateTime, indicating that this function is specifically designed to work with DateTime data types. This suggests that MYFUNC is tailored for operations involving date and time, such as parsing, formatting, or calculations.\n\n**Note**: When using MYFUNC, ensure that the input data aligns with the DateTime type, as the function is explicitly designed for this data type. Additionally, the caching behavior inherited from the parent class may impact performance, so consider the implications of caching in your specific use case."
      ],
      "code_start_line": 330,
      "code_end_line": 332,
      "params": [],
      "have_return": false,
      "code_content": "        class MYFUNC(GenericFunction):\n            inherit_cache = True\n            type = DateTime\n",
      "name_column": 14,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "test_replace_function",
      "md_content": [
        "**test_replace_function**: The function of test_replace_function is to test the replacement and overriding behavior of a GenericFunction with a specific identifier.\n\n**parameters**: This function does not take any parameters.\n\n**Code Description**: \nThe `test_replace_function` function is designed to verify the behavior of replacing and overriding a `GenericFunction` with a specific identifier. The function begins by defining a class `replaceable_func` that inherits from `GenericFunction`. This class is configured with a `type` of `Integer` and an `identifier` of `\"replaceable_func\"`. \n\nThe function then performs a series of assertions to confirm that instances of the function, accessed through different naming conventions (`Replaceable_Func`, `RePlAcEaBlE_fUnC`, and `replaceable_func`), correctly return the `Integer` type.\n\nNext, the function uses a context manager `expect_warnings` to handle a warning that is expected to occur when the `GenericFunction` with the identifier `\"replaceable_func\"` is overridden. Inside this context, a new class `replaceable_func_override` is defined, which also inherits from `GenericFunction` but changes the `type` to `DateTime` while keeping the same identifier `\"replaceable_func\"`.\n\nAfter the override, the function performs another series of assertions to confirm that instances of the function, accessed through the same naming conventions, now correctly return the `DateTime` type, indicating that the original function has been successfully overridden.\n\n**Note**: \n- The function relies on the `expect_warnings` context manager to handle the warning that occurs when a `GenericFunction` is overridden. This ensures that the test does not fail due to the expected warning.\n- The function demonstrates the flexibility of `GenericFunction` by showing how it can be replaced and overridden with a new type while maintaining the same identifier. This behavior is crucial for scenarios where function definitions need to be dynamically updated or replaced in a codebase."
      ],
      "code_start_line": 339,
      "code_end_line": 360,
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def test_replace_function(self):\n        class replaceable_func(GenericFunction):\n            type = Integer\n            identifier = \"replaceable_func\"\n\n        assert isinstance(func.Replaceable_Func().type, Integer)\n        assert isinstance(func.RePlAcEaBlE_fUnC().type, Integer)\n        assert isinstance(func.replaceable_func().type, Integer)\n\n        with expect_warnings(\n            \"The GenericFunction 'replaceable_func' is already registered and \"\n            \"is going to be overridden.\",\n            regex=False,\n        ):\n\n            class replaceable_func_override(GenericFunction):\n                type = DateTime\n                identifier = \"replaceable_func\"\n\n        assert isinstance(func.Replaceable_Func().type, DateTime)\n        assert isinstance(func.RePlAcEaBlE_fUnC().type, DateTime)\n        assert isinstance(func.replaceable_func().type, DateTime)\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "ClassDef",
      "name": "replaceable_func",
      "md_content": [
        "**replaceable_func**: The function of replaceable_func is to serve as a generic function with a specific type and identifier, designed to be replaceable in a testing or functional context.\n\n**attributes**: The attributes of this Class.\n· type: Specifies the type of the function, which is set to `Integer`. This indicates that the function is expected to handle or return integer values.\n· identifier: A unique identifier for the function, set to `\"replaceable_func\"`. This identifier is used to distinguish this function from others in the system.\n\n**Code Description**: The `replaceable_func` class is a subclass of `GenericFunction`, which implies it inherits functionality from a more general function class. The class is designed to be replaceable, meaning it can be substituted with other functions or implementations as needed. The `type` attribute is set to `Integer`, indicating that this function is intended to work with integer data types. The `identifier` attribute is a string that uniquely identifies this function within the system, allowing it to be easily referenced or replaced. This class is likely used in a testing or modular system where functions need to be dynamically swapped or replaced.\n\n**Note**: When using `replaceable_func`, ensure that the `type` and `identifier` attributes are correctly set to match the expected behavior and context in which the function is used. This class is designed for flexibility, so it should be integrated into systems that support dynamic function replacement."
      ],
      "code_start_line": 340,
      "code_end_line": 342,
      "params": [],
      "have_return": false,
      "code_content": "        class replaceable_func(GenericFunction):\n            type = Integer\n            identifier = \"replaceable_func\"\n",
      "name_column": 14,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "ClassDef",
      "name": "replaceable_func_override",
      "md_content": [
        "**replaceable_func_override**: The function of replaceable_func_override is to define a custom function that can be overridden or replaced, specifically for operations involving DateTime data types.\n\n**attributes**: The attributes of this Class.\n· type: Specifies the data type associated with this function, which is DateTime. This indicates that the function is designed to handle or operate on DateTime objects.\n· identifier: A unique identifier for the function, set as \"replaceable_func\". This identifier is used to reference or identify the function within the system.\n\n**Code Description**: The replaceable_func_override class is a subclass of GenericFunction, which implies it inherits functionality from a generic function framework. The class is specifically tailored for DateTime operations, as indicated by the `type` attribute. The `identifier` attribute, set to \"replaceable_func\", serves as a unique name for this function, allowing it to be referenced or overridden in other parts of the system. This design allows developers to create custom or replaceable functions for DateTime-related operations, providing flexibility and extensibility in handling DateTime data.\n\n**Note**: When using this class, ensure that the `type` attribute is correctly set to DateTime if the function is intended to operate on DateTime objects. Additionally, the `identifier` should remain unique to avoid conflicts with other functions in the system. Overriding or replacing this function should be done with caution to maintain consistency in DateTime operations."
      ],
      "code_start_line": 354,
      "code_end_line": 356,
      "params": [],
      "have_return": false,
      "code_content": "            class replaceable_func_override(GenericFunction):\n                type = DateTime\n                identifier = \"replaceable_func\"\n",
      "name_column": 18,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "test_replace_function_case_insensitive",
      "md_content": [
        "**test_replace_function_case_insensitive**: The function of test_replace_function_case_insensitive is to test the case-insensitive replacement of a registered GenericFunction and verify that the function's type is correctly overridden.\n\n**parameters**: This function does not take any parameters.\n\n**Code Description**: \nThe `test_replace_function_case_insensitive` function is designed to test the behavior of a case-insensitive function replacement mechanism in a system that uses `GenericFunction`. The function performs the following steps:\n\n1. It defines a class `replaceable_func` that inherits from `GenericFunction`. This class is configured with a `type` of `Integer` and an `identifier` of `\"replaceable_func\"`.\n\n2. The function then asserts that instances of the function, accessed in different case variations (`func.Replaceable_Func`, `func.RePlAcEaBlE_fUnC`, and `func.replaceable_func`), all correctly return an instance of `Integer` as their type.\n\n3. The function uses a context manager `expect_warnings` to handle a warning that is expected when a `GenericFunction` with the same identifier is overridden. Inside this context, a new class `replaceable_func_override` is defined, which also inherits from `GenericFunction`. This new class overrides the `type` to `DateTime` and uses the same `identifier` but in a different case (`\"REPLACEABLE_Func\"`).\n\n4. After the override, the function asserts that instances of the function, accessed in the same case variations as before, now correctly return an instance of `DateTime` as their type, confirming that the override was successful and case-insensitive.\n\n**Note**: \n- This test function is crucial for ensuring that the system correctly handles case-insensitive function identifiers and that overrides are applied as expected.\n- The use of `expect_warnings` indicates that overriding a function with the same identifier (case-insensitive) will trigger a warning, which is an expected behavior in this context."
      ],
      "code_start_line": 362,
      "code_end_line": 383,
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def test_replace_function_case_insensitive(self):\n        class replaceable_func(GenericFunction):\n            type = Integer\n            identifier = \"replaceable_func\"\n\n        assert isinstance(func.Replaceable_Func().type, Integer)\n        assert isinstance(func.RePlAcEaBlE_fUnC().type, Integer)\n        assert isinstance(func.replaceable_func().type, Integer)\n\n        with expect_warnings(\n            \"The GenericFunction 'replaceable_func' is already registered and \"\n            \"is going to be overridden.\",\n            regex=False,\n        ):\n\n            class replaceable_func_override(GenericFunction):\n                type = DateTime\n                identifier = \"REPLACEABLE_Func\"\n\n        assert isinstance(func.Replaceable_Func().type, DateTime)\n        assert isinstance(func.RePlAcEaBlE_fUnC().type, DateTime)\n        assert isinstance(func.replaceable_func().type, DateTime)\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "ClassDef",
      "name": "replaceable_func",
      "md_content": [
        "**replaceable_func**: The function of replaceable_func is to serve as a generic function with a specific identifier and return type, which can be used in contexts where a replaceable or customizable function is required.\n\n**attributes**: The attributes of this Class.\n· type: Specifies the return type of the function, which is set to `Integer`. This indicates that the function is expected to return an integer value.\n· identifier: A unique identifier for the function, set to `\"replaceable_func\"`. This identifier can be used to reference or distinguish this function in a larger system or framework.\n\n**Code Description**: The `replaceable_func` class is a subclass of `GenericFunction`, which implies it inherits properties and behaviors from a generic function framework. The class defines two key attributes: `type` and `identifier`. The `type` attribute is set to `Integer`, indicating that instances of this class are expected to return integer values. The `identifier` attribute is set to `\"replaceable_func\"`, providing a unique name or label for this function. This setup allows the function to be easily identified and replaced or customized in a larger system where multiple functions might be used interchangeably.\n\n**Note**: When using `replaceable_func`, ensure that the context in which it is applied supports the `Integer` return type. Additionally, the `identifier` should be unique within the system to avoid conflicts with other functions. This class is designed to be flexible and replaceable, making it suitable for scenarios where function behavior needs to be dynamically altered or extended."
      ],
      "code_start_line": 363,
      "code_end_line": 365,
      "params": [],
      "have_return": false,
      "code_content": "        class replaceable_func(GenericFunction):\n            type = Integer\n            identifier = \"replaceable_func\"\n",
      "name_column": 14,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "ClassDef",
      "name": "replaceable_func_override",
      "md_content": [
        "**replaceable_func_override**: The function of replaceable_func_override is to define a custom generic function that can be overridden or replaced in a case-insensitive manner, specifically for operations involving DateTime data types.\n\n**attributes**: The attributes of this Class.\n· type: Specifies the data type associated with this function, which is DateTime. This indicates that the function is intended to operate on DateTime objects or values.\n· identifier: A string identifier for the function, set to \"REPLACEABLE_Func\". This identifier is used to reference or override the function in a case-insensitive context.\n\n**Code Description**: The replaceable_func_override class is a subclass of GenericFunction, which implies it inherits functionality from a generic function framework. The class is designed to allow the function to be overridden or replaced in a case-insensitive manner, making it flexible for use in scenarios where case sensitivity might be an issue. The type attribute is set to DateTime, indicating that this function is specifically tailored for DateTime-related operations. The identifier attribute, \"REPLACEABLE_Func\", serves as a unique name for the function, enabling it to be referenced or overridden in a case-insensitive way.\n\n**Note**: When using this class, ensure that the identifier is unique within the context of your application to avoid conflicts with other functions. Additionally, since the function is designed to be case-insensitive, be mindful of how it interacts with other case-sensitive components in your codebase."
      ],
      "code_start_line": 377,
      "code_end_line": 379,
      "params": [],
      "have_return": false,
      "code_content": "            class replaceable_func_override(GenericFunction):\n                type = DateTime\n                identifier = \"REPLACEABLE_Func\"\n",
      "name_column": 18,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "test_custom_w_custom_name",
      "md_content": [
        "**test_custom_w_custom_name**: The function of test_custom_w_custom_name is to verify the behavior of a custom function class with a custom name when instantiated through the `func` module.\n\n**parameters**: This function does not take any parameters.\n\n**Code Description**: \nThe `test_custom_w_custom_name` function defines a custom function class named `myfunc` that inherits from `GenericFunction`. The class has two attributes:\n1. `inherit_cache = True`: This indicates that the function class should inherit caching behavior from its parent class.\n2. `name = \"notmyfunc\"`: This sets the name of the function class to \"notmyfunc\" instead of the default class name.\n\nThe function then performs two assertions:\n1. `assert isinstance(func.notmyfunc(), myfunc)`: This checks that an instance created using `func.notmyfunc()` is indeed an instance of the `myfunc` class. This confirms that the custom name \"notmyfunc\" is correctly associated with the `myfunc` class.\n2. `assert not isinstance(func.myfunc(), myfunc)`: This verifies that attempting to create an instance using `func.myfunc()` does not result in an instance of the `myfunc` class. This ensures that the default class name \"myfunc\" is not mistakenly associated with the class due to the custom name override.\n\n**Note**: This test function is designed to validate the correct association of custom names with function classes and ensure that the default class name does not interfere with the custom naming behavior. It is important to ensure that the `func` module is properly configured to handle custom function names as expected."
      ],
      "code_start_line": 385,
      "code_end_line": 391,
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def test_custom_w_custom_name(self):\n        class myfunc(GenericFunction):\n            inherit_cache = True\n            name = \"notmyfunc\"\n\n        assert isinstance(func.notmyfunc(), myfunc)\n        assert not isinstance(func.myfunc(), myfunc)\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "ClassDef",
      "name": "myfunc",
      "md_content": [
        "**myfunc**: The function of myfunc is to serve as a custom function with a specific name and caching behavior.\n\n**attributes**: The attributes of this Class.\n· inherit_cache: A boolean attribute set to True, indicating that this function inherits caching behavior from its parent class.\n· name: A string attribute set to \"notmyfunc\", which defines the name of the function.\n\n**Code Description**: The `myfunc` class is a subclass of `GenericFunction`, which implies it inherits functionality from a generic function framework. The class is designed to be a custom function with specific attributes. The `inherit_cache` attribute is set to True, meaning that this function will utilize caching behavior inherited from its parent class, potentially improving performance by reusing previously computed results. The `name` attribute is explicitly set to \"notmyfunc\", which overrides any default naming convention and assigns a specific identifier to this function. This allows the function to be referenced or called using the name \"notmyfunc\" instead of the class name \"myfunc\".\n\n**Note**: When using this class, ensure that the `GenericFunction` parent class provides the necessary caching mechanism, as the `inherit_cache` attribute relies on it. Additionally, be aware that the function will be identified by the name \"notmyfunc\" in any context where the function name is used, which may differ from the class name \"myfunc\"."
      ],
      "code_start_line": 386,
      "code_end_line": 388,
      "params": [],
      "have_return": false,
      "code_content": "        class myfunc(GenericFunction):\n            inherit_cache = True\n            name = \"notmyfunc\"\n",
      "name_column": 14,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "test_custom_w_quoted_name",
      "md_content": [
        "**test_custom_w_quoted_name**: The function of test_custom_w_quoted_name is to test the compilation of a custom SQL function with a quoted name.\n\n**parameters**: The parameters of this Function.\n· self: The instance of the test class, used to access assertion methods and other class attributes.\n\n**Code Description**: \nThe function `test_custom_w_quoted_name` defines a custom SQL function within a test case. It creates a class `myfunc` that inherits from `GenericFunction`. The class has two key attributes:\n1. `inherit_cache = True`: This indicates that the function should inherit caching behavior from its parent class.\n2. `name = quoted_name(\"NotMyFunc\", quote=True)`: This sets the name of the function to `\"NotMyFunc\"` with the `quote=True` parameter, ensuring that the name is treated as a quoted identifier in SQL. This is useful for cases where the function name contains special characters or reserved keywords.\n3. `identifier = \"myfunc\"`: This sets the identifier for the function, which is used internally.\n\nThe function then uses `self.assert_compile` to verify that the SQL compilation of `func.myfunc()` results in the expected SQL string `\"NotMyFunc\"()`. This ensures that the custom function with a quoted name is correctly compiled into SQL.\n\n**Note**: \n- The `quoted_name` utility is used to handle SQL identifiers that need to be quoted, such as those containing special characters or reserved keywords.\n- The `assert_compile` method is used to validate that the SQL generated by the custom function matches the expected output. This is crucial for ensuring the correctness of SQL function definitions in the codebase."
      ],
      "code_start_line": 393,
      "code_end_line": 399,
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def test_custom_w_quoted_name(self):\n        class myfunc(GenericFunction):\n            inherit_cache = True\n            name = quoted_name(\"NotMyFunc\", quote=True)\n            identifier = \"myfunc\"\n\n        self.assert_compile(func.myfunc(), '\"NotMyFunc\"()')\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "ClassDef",
      "name": "myfunc",
      "md_content": [
        "**myfunc**: The function of myfunc is to serve as a custom SQL function with a quoted name, allowing it to be referenced in SQL queries using a specific identifier.\n\n**attributes**: The attributes of this Class.\n· inherit_cache: A boolean attribute set to True, indicating that the function's results can be cached to improve performance.\n· name: An attribute of type `quoted_name`, set to \"NotMyFunc\" with `quote=True`, which ensures the name is treated as a quoted identifier in SQL.\n· identifier: A string attribute set to \"myfunc\", which serves as the unique identifier for this function.\n\n**Code Description**: The `myfunc` class is a subclass of `GenericFunction`, which is typically used to define custom SQL functions in SQLAlchemy. The `inherit_cache` attribute is set to True, enabling caching of the function's results to optimize performance. The `name` attribute is defined using `quoted_name(\"NotMyFunc\", quote=True)`, which ensures that the function's name is treated as a quoted identifier in SQL, preserving case sensitivity and special characters. The `identifier` attribute is set to \"myfunc\", providing a unique identifier for this function within the SQLAlchemy framework. This setup allows `myfunc` to be referenced in SQL queries using the identifier \"myfunc\" while being represented in SQL with the quoted name \"NotMyFunc\".\n\n**Note**: When using `myfunc`, ensure that the quoted name \"NotMyFunc\" is correctly referenced in SQL queries, as the quoting ensures case sensitivity and special character handling. Additionally, the caching behavior enabled by `inherit_cache` should be considered in performance-critical applications."
      ],
      "code_start_line": 394,
      "code_end_line": 397,
      "params": [],
      "have_return": false,
      "code_content": "        class myfunc(GenericFunction):\n            inherit_cache = True\n            name = quoted_name(\"NotMyFunc\", quote=True)\n            identifier = \"myfunc\"\n",
      "name_column": 14,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "test_custom_w_quoted_name_no_identifier",
      "md_content": [
        "**test_custom_w_quoted_name_no_identifier**: The function of `test_custom_w_quoted_name_no_identifier` is to test the compilation of a custom SQL function with a quoted name that does not match the identifier used in the code.\n\n**parameters**: The function does not take any parameters other than the implicit `self` parameter, which is a reference to the test class instance.\n\n**Code Description**: \nThe function defines a custom SQL function named `myfunc` by subclassing `GenericFunction`. The `name` attribute of this function is set to a quoted name `\"NotMyFunc\"` using the `quoted_name` utility, with the `quote` parameter set to `True`. This ensures that the name is treated as a literal string in SQL, preserving its case and special characters. The `inherit_cache` attribute is set to `True`, indicating that the function's cache behavior should be inherited from its parent class.\n\nThe function then asserts that the SQL compilation of `func.notmyfunc()` results in the string `'\"NotMyFunc\"()'`. This test verifies that the SQL function is correctly compiled with the quoted name, even though the identifier used in the code (`notmyfunc`) is in lowercase. This is important because SQL function names are often case-insensitive, and the test ensures that the quoted name is correctly handled during compilation.\n\n**Note**: \n- The test assumes that the quoted name `\"NotMyFunc\"` will be lowercased for correct lookup in the SQL compilation process. This is a critical detail to ensure the test passes, as the function name in the code (`notmyfunc`) is in lowercase.\n- The use of `quoted_name` with `quote=True` is essential for preserving the exact case and formatting of the function name in the generated SQL."
      ],
      "code_start_line": 401,
      "code_end_line": 408,
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def test_custom_w_quoted_name_no_identifier(self):\n        class myfunc(GenericFunction):\n            inherit_cache = True\n            name = quoted_name(\"NotMyFunc\", quote=True)\n\n        # note this requires that the quoted name be lower cased for\n        # correct lookup\n        self.assert_compile(func.notmyfunc(), '\"NotMyFunc\"()')\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "ClassDef",
      "name": "myfunc",
      "md_content": [
        "**myfunc**: The function of myfunc is to define a custom SQL function with a quoted name that does not match the class name.\n\n**attributes**: The attributes of this Class.\n· inherit_cache: A boolean attribute set to True, indicating that the function's cache behavior is inherited from its parent class.\n· name: An attribute of type `quoted_name`, set to \"NotMyFunc\" with the `quote` parameter set to True, ensuring the name is treated as a quoted identifier in SQL.\n\n**Code Description**: The `myfunc` class is a subclass of `GenericFunction`, which is typically used to define custom SQL functions in SQLAlchemy. The class is designed to create a SQL function with a name that is explicitly quoted, ensuring it is treated as a literal identifier in SQL queries. The `name` attribute is set using `quoted_name(\"NotMyFunc\", quote=True)`, which means the function will be referenced in SQL as `\"NotMyFunc\"` (with quotes). This is useful when the function name needs to be case-sensitive or contains special characters that require quoting. The `inherit_cache` attribute is set to True, indicating that the caching behavior of this function is inherited from its parent class, `GenericFunction`.\n\n**Note**: When using this class, ensure that the quoted name \"NotMyFunc\" is unique and does not conflict with other SQL identifiers in your database schema. The `quote=True` parameter ensures that the name is treated as a literal identifier, which may be necessary for compatibility with certain database systems or naming conventions."
      ],
      "code_start_line": 402,
      "code_end_line": 404,
      "params": [],
      "have_return": false,
      "code_content": "        class myfunc(GenericFunction):\n            inherit_cache = True\n            name = quoted_name(\"NotMyFunc\", quote=True)\n",
      "name_column": 14,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "test_custom_package_namespace",
      "md_content": [
        "**test_custom_package_namespace**: The function of test_custom_package_namespace is to verify the correct instantiation of custom package namespaces for dynamically created classes.\n\n**parameters**: The parameters of this Function.\n· self: The instance of the test class, used to access class-level attributes or methods.\n\n**Code Description**: \nThe `test_custom_package_namespace` function defines a nested function `cls1` that creates a custom class `myfunc` inheriting from `GenericFunction`. The `myfunc` class is configured with a custom package name passed as an argument `pk_name`. The `inherit_cache` attribute is set to `True`, indicating that the class should inherit caching behavior.\n\nThe function then creates two instances of `myfunc` using different package names: \"mypackage\" and \"myotherpackage\". These instances are stored in variables `f1` and `f2`, respectively. The function asserts that instances of `myfunc` created under the custom package namespaces (`func.mypackage.myfunc()` and `func.myotherpackage.myfunc()`) are correctly identified as instances of the corresponding `f1` and `f2` classes.\n\nThis test ensures that the custom package namespaces are correctly applied and that the dynamically created classes can be instantiated and identified as expected.\n\n**Note**: \n- The `GenericFunction` class is assumed to be defined elsewhere in the codebase and provides the base functionality for the `myfunc` class.\n- The `func` module or object is assumed to provide access to the custom package namespaces.\n\n**Output Example**: \nThe function does not return any value. Instead, it performs assertions to validate the correct instantiation of classes under custom package namespaces. If the assertions pass, the test is considered successful; otherwise, an assertion error will be raised."
      ],
      "code_start_line": 410,
      "code_end_line": 422,
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def test_custom_package_namespace(self):\n        def cls1(pk_name):\n            class myfunc(GenericFunction):\n                inherit_cache = True\n                package = pk_name\n\n            return myfunc\n\n        f1 = cls1(\"mypackage\")\n        f2 = cls1(\"myotherpackage\")\n\n        assert isinstance(func.mypackage.myfunc(), f1)\n        assert isinstance(func.myotherpackage.myfunc(), f2)\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "cls1",
      "md_content": [
        "**cls1**: The function of cls1 is to dynamically create a custom function class with a specified package namespace.\n\n**parameters**: The parameters of this Function.\n· pk_name: A string representing the package name that will be assigned to the custom function class.\n\n**Code Description**: \nThe `cls1` function takes a single parameter, `pk_name`, which is used to define the package namespace for a dynamically created function class. Inside the function, a new class named `myfunc` is defined, which inherits from `GenericFunction`. The `inherit_cache` attribute is set to `True`, indicating that the class should inherit caching behavior from its parent class. The `package` attribute of the `myfunc` class is assigned the value of the `pk_name` parameter, effectively setting the package namespace for this class. Finally, the function returns the newly created `myfunc` class.\n\n**Note**: \n- The `GenericFunction` class is assumed to be defined elsewhere in the codebase, and it provides the base functionality for the custom function class.\n- The `inherit_cache` attribute is set to `True`, which means that the caching behavior of the parent class will be inherited by the `myfunc` class.\n- The `package` attribute is crucial for defining the namespace under which the custom function class will operate.\n\n**Output Example**: \nThe function returns a class object. For example, if `pk_name` is set to `\"my_package\"`, the returned class will have the `package` attribute set to `\"my_package\"`. The returned class can then be instantiated or used as needed in the code."
      ],
      "code_start_line": 411,
      "code_end_line": 416,
      "params": [
        "pk_name"
      ],
      "have_return": true,
      "code_content": "        def cls1(pk_name):\n            class myfunc(GenericFunction):\n                inherit_cache = True\n                package = pk_name\n\n            return myfunc\n",
      "name_column": 12,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "ClassDef",
      "name": "myfunc",
      "md_content": [
        "**myfunc**: The function of myfunc is to serve as a custom function within a specific package namespace, inheriting behavior from the GenericFunction class.\n\n**attributes**: The attributes of this Class.\n· inherit_cache: A boolean attribute set to True, indicating that the function inherits caching behavior from its parent class.\n· package: A string attribute that specifies the package name (pk_name) to which this function belongs.\n\n**Code Description**: The `myfunc` class is a subclass of `GenericFunction`, which implies it inherits functionality and behavior from its parent class. The `inherit_cache` attribute is set to True, meaning that this function will utilize caching mechanisms inherited from `GenericFunction`. The `package` attribute is assigned the value of `pk_name`, which defines the package namespace this function is associated with. This setup allows `myfunc` to operate within a specific package context while leveraging the capabilities of the `GenericFunction` class.\n\n**Note**: Ensure that `pk_name` is properly defined and corresponds to the intended package namespace. The `inherit_cache` attribute should remain True if caching behavior is desired, as it directly impacts the function's performance and resource usage."
      ],
      "code_start_line": 412,
      "code_end_line": 414,
      "params": [],
      "have_return": false,
      "code_content": "            class myfunc(GenericFunction):\n                inherit_cache = True\n                package = pk_name\n",
      "name_column": 18,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "test_custom_name",
      "md_content": [
        "**test_custom_name**: The function of test_custom_name is to verify the correct compilation of a custom function with a specific name and behavior.\n\n**parameters**: The parameters of this Function.\n· self: The instance of the test class, used to access the test methods and assertions.\n\n**Code Description**: \nThe `test_custom_name` function is a test case designed to validate the behavior of a custom function named `my_func`. This custom function is defined within the test case as a subclass of `GenericFunction`. The custom function, `MyFunction`, has the following characteristics:\n- It is assigned a name `\"my_func\"` through the `name` attribute.\n- It inherits caching behavior via the `inherit_cache` attribute set to `True`.\n- Its `__init__` method modifies the input arguments by appending the value `3` to the provided arguments before passing them to the parent class's `__init__` method.\n\nThe test case then uses the `assert_compile` method to verify that calling `func.my_func(1, 2)` results in the expected SQL-like string output: `\"my_func(:my_func_1, :my_func_2, :my_func_3)\"`. This ensures that the custom function is correctly compiled and formatted according to the expected pattern.\n\n**Note**: \n- The test assumes that the `func.my_func` call is properly set up to use the `MyFunction` class.\n- The `assert_compile` method is likely a custom assertion method used in the test framework to compare the compiled output of the function with the expected string.\n- The test case is designed to validate both the naming and argument handling of the custom function."
      ],
      "code_start_line": 424,
      "code_end_line": 435,
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def test_custom_name(self):\n        class MyFunction(GenericFunction):\n            name = \"my_func\"\n            inherit_cache = True\n\n            def __init__(self, *args):\n                args = args + (3,)\n                super().__init__(*args)\n\n        self.assert_compile(\n            func.my_func(1, 2), \"my_func(:my_func_1, :my_func_2, :my_func_3)\"\n        )\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "ClassDef",
      "name": "MyFunction",
      "md_content": [
        "**MyFunction**: The function of MyFunction is to extend the functionality of a generic function by appending a fixed argument during initialization and inheriting caching behavior.\n\n**attributes**: The attributes of this Class.\n· name: A string attribute set to \"my_func\", which defines the name of the function.\n· inherit_cache: A boolean attribute set to True, indicating that this function inherits caching behavior from its parent class.\n\n**Code Description**: The description of this Class.\nMyFunction is a subclass of GenericFunction, which means it inherits the properties and methods of its parent class. The class has two attributes: `name` and `inherit_cache`. The `name` attribute is set to \"my_func\", which identifies the function. The `inherit_cache` attribute is set to True, enabling the function to inherit caching behavior from its parent class, which can improve performance by reusing previously computed results.\n\nThe `__init__` method is overridden in MyFunction. When an instance of MyFunction is created, it accepts a variable number of arguments (`*args`). Inside the `__init__` method, the provided arguments are extended by appending the integer `3` to the tuple of arguments. This modified tuple of arguments is then passed to the parent class's `__init__` method using `super().__init__(*args)`. This ensures that the parent class is properly initialized with the extended arguments.\n\n**Note**: Points to note about the use of the code\n- The `inherit_cache` attribute should be used with caution, as enabling caching may lead to unexpected behavior if the function's output depends on external state or mutable inputs.\n- The fixed argument `3` appended during initialization is hardcoded, so ensure this behavior aligns with the intended use case of the function. If dynamic arguments are required, consider modifying the `__init__` method accordingly."
      ],
      "code_start_line": 425,
      "code_end_line": 431,
      "params": [],
      "have_return": false,
      "code_content": "        class MyFunction(GenericFunction):\n            name = \"my_func\"\n            inherit_cache = True\n\n            def __init__(self, *args):\n                args = args + (3,)\n                super().__init__(*args)\n",
      "name_column": 14,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [
        "**__init__**: The function of __init__ is to initialize an instance of the class by extending the provided arguments and passing them to the parent class's initialization method.\n\n**parameters**: The parameters of this Function.\n· *args: A variable-length argument list that allows the function to accept any number of positional arguments.\n\n**Code Description**: The description of this Function.\nThe `__init__` method is the constructor for the class. It takes a variable number of positional arguments through `*args`. Inside the method, the provided arguments are extended by appending the integer `3` to the tuple of arguments. This modified tuple of arguments is then passed to the `__init__` method of the parent class using `super().__init__(*args)`. This ensures that the parent class is properly initialized with the extended set of arguments.\n\n**Note**: Points to note about the use of the code\n- The `__init__` method assumes that the parent class can accept the extended arguments, including the additional integer `3`. Ensure that the parent class's `__init__` method is compatible with this modification.\n- The use of `*args` allows for flexibility in the number of arguments passed to the constructor, but it is important to understand how these arguments are processed and extended within the method."
      ],
      "code_start_line": 429,
      "code_end_line": 431,
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "            def __init__(self, *args):\n                args = args + (3,)\n                super().__init__(*args)\n",
      "name_column": 16,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "test_custom_registered_identifier",
      "md_content": [
        "**test_custom_registered_identifier**: The function of test_custom_registered_identifier is to test the custom registration and compilation of identifiers for GenericFunction classes.\n\n**parameters**: This function does not take any parameters.\n\n**Code Description**: \nThe `test_custom_registered_identifier` function defines four custom classes (`GeoBuffer`, `GeoBuffer2`, `BufferThree`, and `GeoBufferFour`) that inherit from `GenericFunction`. Each class is configured with specific attributes:\n- `type`: Specifies the return type of the function, which is `Integer` for all classes.\n- `package`: Specifies the package name, which is only defined for `GeoBuffer` as \"geo\".\n- `name`: Specifies the name of the function, which is used during compilation.\n- `identifier`: Specifies the custom identifier used to register the function.\n- `inherit_cache`: A boolean flag indicating whether the function should inherit caching behavior, set to `True` for all classes.\n\nThe function then tests the compilation of these custom functions using the `assert_compile` method. The tests verify that the custom identifiers (`buf1`, `buf2`, `buf3`, and `Buf4`) are correctly registered and that the functions are compiled into their respective names (`BufferOne`, `BufferTwo`, `BufferThree`, and `BufferFour`). Additionally, the function tests case insensitivity and underscores in the identifiers, ensuring that variations like `BuF4`, `bUf4`, `bUf4_`, and `buf4` all correctly compile to `BufferFour`.\n\n**Note**: \n- The function assumes that the `func` object is properly initialized and supports the registration and compilation of custom identifiers.\n- The `assert_compile` method is used to verify that the custom identifiers are correctly mapped to their respective function names during compilation.\n- The test cases cover various identifier formats, including case insensitivity and underscores, to ensure robust registration and compilation behavior."
      ],
      "code_start_line": 437,
      "code_end_line": 469,
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def test_custom_registered_identifier(self):\n        class GeoBuffer(GenericFunction):\n            type = Integer\n            package = \"geo\"\n            name = \"BufferOne\"\n            identifier = \"buf1\"\n            inherit_cache = True\n\n        class GeoBuffer2(GenericFunction):\n            type = Integer\n            name = \"BufferTwo\"\n            identifier = \"buf2\"\n            inherit_cache = True\n\n        class BufferThree(GenericFunction):\n            type = Integer\n            identifier = \"buf3\"\n            inherit_cache = True\n\n        class GeoBufferFour(GenericFunction):\n            type = Integer\n            name = \"BufferFour\"\n            identifier = \"Buf4\"\n            inherit_cache = True\n\n        self.assert_compile(func.geo.buf1(), \"BufferOne()\")\n        self.assert_compile(func.buf2(), \"BufferTwo()\")\n        self.assert_compile(func.buf3(), \"BufferThree()\")\n        self.assert_compile(func.Buf4(), \"BufferFour()\")\n        self.assert_compile(func.BuF4(), \"BufferFour()\")\n        self.assert_compile(func.bUf4(), \"BufferFour()\")\n        self.assert_compile(func.bUf4_(), \"BufferFour()\")\n        self.assert_compile(func.buf4(), \"BufferFour()\")\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "ClassDef",
      "name": "GeoBuffer",
      "md_content": [
        "**GeoBuffer**: The function of GeoBuffer is to define a custom SQL function for geographic buffer operations, specifically named \"BufferOne,\" which returns an integer value.\n\n**attributes**: The attributes of this Class.\n· type: Specifies the return type of the function, which is `Integer`.\n· package: Indicates the package or namespace to which this function belongs, set to \"geo\".\n· name: Defines the name of the function as \"BufferOne\".\n· identifier: Provides a unique identifier for the function, set to \"buf1\".\n· inherit_cache: A boolean attribute set to `True`, indicating that the function's result can be cached for optimization purposes.\n\n**Code Description**: The `GeoBuffer` class is a subclass of `GenericFunction`, which is typically used to define custom SQL functions in a database context. This class is specifically designed to represent a geographic buffer operation, likely used in spatial or geographic data processing. The function is named \"BufferOne\" and is associated with the \"geo\" package. The `identifier` attribute, set to \"buf1,\" ensures that this function can be uniquely referenced in SQL queries or other database operations. The `inherit_cache` attribute is set to `True`, enabling caching of the function's results to improve performance when the same operation is repeated. The return type of the function is explicitly defined as `Integer`, indicating that the result of the buffer operation will be an integer value.\n\n**Note**: When using the `GeoBuffer` class, ensure that the database or system supports the \"geo\" package and the \"BufferOne\" function. Additionally, caching should be used judiciously, as it may lead to outdated results if the underlying data changes frequently."
      ],
      "code_start_line": 438,
      "code_end_line": 443,
      "params": [],
      "have_return": false,
      "code_content": "        class GeoBuffer(GenericFunction):\n            type = Integer\n            package = \"geo\"\n            name = \"BufferOne\"\n            identifier = \"buf1\"\n            inherit_cache = True\n",
      "name_column": 14,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "ClassDef",
      "name": "GeoBuffer2",
      "md_content": [
        "**GeoBuffer2**: The function of GeoBuffer2 is to define a custom SQL function named \"BufferTwo\" that returns an integer value and is identified by the alias \"buf2\".\n\n**attributes**: The attributes of this Class.\n· type: Specifies the return type of the function as Integer.\n· name: Defines the name of the function as \"BufferTwo\".\n· identifier: Provides an alias for the function as \"buf2\".\n· inherit_cache: Indicates that the function inherits caching behavior, set to True.\n\n**Code Description**: The GeoBuffer2 class is a subclass of GenericFunction, which is typically used to define custom SQL functions in a database context. The class is configured to return an integer value, as indicated by the `type` attribute. The `name` attribute sets the function's name to \"BufferTwo\", which is how it will be referenced in SQL queries. The `identifier` attribute provides a shorthand alias \"buf2\" for the function, allowing for more concise usage in queries. The `inherit_cache` attribute is set to True, meaning that the function will inherit caching behavior from its parent class, which can improve performance by reusing previously computed results.\n\n**Note**: When using GeoBuffer2, ensure that the function's name and identifier are unique within the database context to avoid conflicts. Additionally, the caching behavior (inherit_cache) should be considered carefully, as it may not be suitable for all use cases, especially when the function's output depends on dynamic or frequently changing data."
      ],
      "code_start_line": 445,
      "code_end_line": 449,
      "params": [],
      "have_return": false,
      "code_content": "        class GeoBuffer2(GenericFunction):\n            type = Integer\n            name = \"BufferTwo\"\n            identifier = \"buf2\"\n            inherit_cache = True\n",
      "name_column": 14,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "ClassDef",
      "name": "BufferThree",
      "md_content": [
        "**BufferThree**: The function of BufferThree is to serve as a custom registered function with a specific identifier and return type, primarily used in a context where generic functions are managed and cached.\n\n**attributes**: The attributes of this Class.\n· type: Specifies the return type of the function, which is `Integer`.\n· identifier: A unique string identifier for the function, set to `\"buf3\"`.\n· inherit_cache: A boolean flag indicating whether the function should inherit caching behavior, set to `True`.\n\n**Code Description**: The `BufferThree` class is a subclass of `GenericFunction`, designed to represent a specific function within a system that manages generic functions. The class defines three key attributes:\n1. `type`: This attribute specifies that the function will return an `Integer` type. This is useful for type checking and ensuring the function's output aligns with expected data types.\n2. `identifier`: The `identifier` attribute is set to `\"buf3\"`, which serves as a unique identifier for this function. This identifier is likely used to register or reference the function within a larger system or framework.\n3. `inherit_cache`: The `inherit_cache` attribute is set to `True`, indicating that this function should inherit caching behavior from its parent class or framework. This can improve performance by reusing previously computed results when the same inputs are provided.\n\n**Note**: When using `BufferThree`, ensure that the context in which it is registered supports the `GenericFunction` base class and the specified attributes. The caching behavior (`inherit_cache`) should be compatible with the system's caching mechanism to avoid unexpected results."
      ],
      "code_start_line": 451,
      "code_end_line": 454,
      "params": [],
      "have_return": false,
      "code_content": "        class BufferThree(GenericFunction):\n            type = Integer\n            identifier = \"buf3\"\n            inherit_cache = True\n",
      "name_column": 14,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "ClassDef",
      "name": "GeoBufferFour",
      "md_content": [
        "**GeoBufferFour**: The function of GeoBufferFour is to define a custom SQL function that operates on geographic data, specifically returning an integer value. This function is registered with a unique identifier and is designed to be cached for performance optimization.\n\n**attributes**: The attributes of this Class.\n· type: Specifies the return type of the function as `Integer`, indicating that the function will return an integer value.\n· name: Defines the name of the function as \"BufferFour\", which is used to reference the function in SQL queries.\n· identifier: Sets the unique identifier for the function as \"Buf4\", which is used internally to distinguish this function from others.\n· inherit_cache: A boolean attribute set to `True`, enabling the function to inherit caching behavior for improved performance.\n\n**Code Description**: The `GeoBufferFour` class is a subclass of `GenericFunction`, which is typically used to define custom SQL functions in a database context. By setting the `type` attribute to `Integer`, the function is designed to return an integer value. The `name` attribute is set to \"BufferFour\", which is the name used to call this function in SQL queries. The `identifier` attribute, \"Buf4\", ensures that this function can be uniquely identified within the system. The `inherit_cache` attribute is set to `True`, allowing the function to leverage caching mechanisms to enhance performance, particularly useful when the function is called repeatedly with the same inputs.\n\n**Note**: When using `GeoBufferFour`, ensure that the function is properly registered in the database system where it will be used. The caching behavior (`inherit_cache = True`) should be considered carefully, as it may not be suitable for all use cases, especially when the function's output depends on dynamic or frequently changing data."
      ],
      "code_start_line": 456,
      "code_end_line": 460,
      "params": [],
      "have_return": false,
      "code_content": "        class GeoBufferFour(GenericFunction):\n            type = Integer\n            name = \"BufferFour\"\n            identifier = \"Buf4\"\n            inherit_cache = True\n",
      "name_column": 14,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "test_custom_args",
      "md_content": [
        "**test_custom_args**: The function of test_custom_args is to test the compilation of a custom function with specific arguments and ensure that the generated SQL representation matches the expected output.\n\n**parameters**: The parameters of this Function.\n· self: The instance of the test class, used to access assertion methods and other class attributes.\n\n**Code Description**: The description of this Function.\nThe `test_custom_args` function defines a custom function class `myfunc` that inherits from `GenericFunction`. The `inherit_cache` attribute is set to `True`, indicating that the function should inherit caching behavior from its parent class. The function then calls `self.assert_compile`, which is an assertion method used to verify that the SQL representation of the custom function matches the expected string. Specifically, it checks that the SQL representation of `myfunc(1, 2, 3)` is `\"myfunc(:myfunc_1, :myfunc_2, :myfunc_3)\"`. This ensures that the custom function correctly handles and formats its arguments in the generated SQL.\n\n**Note**: Points to note about the use of the code.\n- The `inherit_cache` attribute is set to `True` to enable caching behavior, which may improve performance in certain scenarios.\n- The `assert_compile` method is crucial for verifying the correctness of the SQL generation process. Ensure that the expected SQL string accurately reflects the intended output of the custom function.\n- This test is designed to validate the behavior of custom functions with specific arguments, so it should be used in conjunction with other tests to ensure comprehensive coverage of function behavior."
      ],
      "code_start_line": 471,
      "code_end_line": 477,
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def test_custom_args(self):\n        class myfunc(GenericFunction):\n            inherit_cache = True\n\n        self.assert_compile(\n            myfunc(1, 2, 3), \"myfunc(:myfunc_1, :myfunc_2, :myfunc_3)\"\n        )\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "ClassDef",
      "name": "myfunc",
      "md_content": [
        "**myfunc**: The function of myfunc is to serve as a custom function class that inherits from `GenericFunction` and utilizes caching for inheritance.\n\n**attributes**: The attributes of this Class.\n· `inherit_cache`: A boolean attribute set to `True`, indicating that the class will cache inherited properties or behaviors to optimize performance.\n\n**Code Description**: The `myfunc` class is a subclass of `GenericFunction`, which suggests it is designed to implement or extend functionality in a generic or reusable manner. The key feature of this class is the `inherit_cache` attribute, which is set to `True`. This attribute enables caching for inherited properties or methods, ensuring that repeated access to these elements does not require redundant computations or lookups. This optimization is particularly useful in scenarios where the class is part of a larger framework or system where performance and efficiency are critical.\n\n**Note**: When using `myfunc`, ensure that the parent class `GenericFunction` is properly implemented and supports the caching mechanism. Additionally, be cautious when modifying or overriding inherited methods, as the caching behavior may need to be adjusted accordingly to avoid unintended side effects."
      ],
      "code_start_line": 472,
      "code_end_line": 473,
      "params": [],
      "have_return": false,
      "code_content": "        class myfunc(GenericFunction):\n            inherit_cache = True\n",
      "name_column": 14,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "test_namespacing_conflicts",
      "md_content": [
        "**test_namespacing_conflicts**: The function of test_namespacing_conflicts is to verify that the compilation of a text function does not result in namespace conflicts.  \n**parameters**: The parameters of this Function.  \n· self: The instance of the test class, used to access assertion methods and other class attributes.  \n**Code Description**: The description of this Function.  \nThe `test_namespacing_conflicts` function is a unit test method designed to ensure that the compilation of a text function, represented by `func.text(\"foo\")`, does not lead to namespace conflicts. The function uses the `assert_compile` method to compare the output of the compilation process with the expected result, `\"text(:text_1)\"`. This test ensures that the compiler correctly handles namespace resolution and avoids conflicts when generating identifiers or references.  \n\n**Note**: This test is critical for validating the robustness of the compilation process, particularly in scenarios where multiple text functions or similar constructs might be used within the same namespace. Ensure that the `func.text` implementation and the `assert_compile` method are correctly configured to produce and validate the expected output."
      ],
      "code_start_line": 479,
      "code_end_line": 480,
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def test_namespacing_conflicts(self):\n        self.assert_compile(func.text(\"foo\"), \"text(:text_1)\")\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "test_generic_count",
      "md_content": [
        "**test_generic_count**: The function of test_generic_count is to test the behavior and SQL compilation of the `count` function in a generic context.\n\n**parameters**: The function does not take any external parameters. It operates within the context of the test class and uses its own internal assertions and test cases.\n\n**Code Description**: \nThe `test_generic_count` function is designed to verify the correctness of the SQL compilation for the `count` function in different scenarios. It performs the following checks:\n\n1. **Type Assertion**: The function first asserts that the type of the result of `func.count()` is an instance of `sqltypes.Integer`. This ensures that the `count` function returns an integer type, as expected in SQL.\n\n2. **Basic Count Compilation**: The function then checks the SQL compilation of `func.count()` without any arguments. It asserts that the compiled SQL string is `\"count(*)\"`, which is the standard SQL syntax for counting all rows in a table.\n\n3. **Count with Literal**: Next, the function tests the SQL compilation of `func.count(1)`. It asserts that the compiled SQL string is `\"count(:count_1)\"`, where `:count_1` is a placeholder for the literal value `1`. This verifies that the `count` function can handle literal values correctly.\n\n4. **Count with Column**: Finally, the function tests the SQL compilation of `func.count(c)`, where `c` is a column named `\"abc\"`. It asserts that the compiled SQL string is `\"count(abc)\"`, ensuring that the `count` function correctly handles column references.\n\n**Note**: \n- This function is part of a test suite and is intended to validate the behavior of the `count` function in SQL compilation. It does not interact with an actual database but rather checks the generated SQL strings.\n- The function relies on the `assert_compile` method, which is assumed to be part of the test class, to compare the generated SQL strings with the expected results."
      ],
      "code_start_line": 482,
      "code_end_line": 488,
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def test_generic_count(self):\n        assert isinstance(func.count().type, sqltypes.Integer)\n\n        self.assert_compile(func.count(), \"count(*)\")\n        self.assert_compile(func.count(1), \"count(:count_1)\")\n        c = column(\"abc\")\n        self.assert_compile(func.count(c), \"count(abc)\")\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "test_ansi_functions_with_args",
      "md_content": [
        "**test_ansi_functions_with_args**: The function of test_ansi_functions_with_args is to test the compilation of ANSI SQL functions with arguments.\n\n**parameters**: The parameters of this Function.\n· self: The instance of the test class, used to access methods and assertions within the test case.\n\n**Code Description**: \nThe function `test_ansi_functions_with_args` is a test case designed to verify the correct compilation of an ANSI SQL function that accepts arguments. In this specific test, the function `current_timestamp` is called with the argument `\"somearg\"`. The result of this function call is stored in the variable `ct`. \n\nThe test then uses the `assert_compile` method to check whether the compiled SQL output of `ct` matches the expected SQL string `\"CURRENT_TIMESTAMP(:current_timestamp_1)\"`. This ensures that the SQL function is correctly compiled with the provided argument and that the placeholder `:current_timestamp_1` is properly generated in the SQL output.\n\n**Note**: \n- This test is specifically designed to validate the behavior of SQL function compilation when arguments are passed. It is important to ensure that the expected SQL output matches the actual compiled output to avoid runtime errors in SQL queries.\n- The placeholder `:current_timestamp_1` is likely generated dynamically, so the test ensures that the argument is correctly bound to the placeholder in the compiled SQL."
      ],
      "code_start_line": 490,
      "code_end_line": 492,
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def test_ansi_functions_with_args(self):\n        ct = func.current_timestamp(\"somearg\")\n        self.assert_compile(ct, \"CURRENT_TIMESTAMP(:current_timestamp_1)\")\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "test_char_length_fixed_args",
      "md_content": [
        "**test_char_length_fixed_args**: The function of test_char_length_fixed_args is to test the behavior of the `char_length` function when provided with incorrect or missing arguments.\n\n**parameters**: The parameters of this Function.\n· self: Represents the instance of the test class. This parameter is implicitly passed when the method is called.\n\n**Code Description**: The description of this Function.\nThe `test_char_length_fixed_args` function is a test case designed to verify that the `char_length` function raises a `TypeError` when it is called with incorrect or missing arguments. The function uses the `assert_raises` method to check for these specific error conditions.\n\n1. The first `assert_raises` call checks if the `char_length` function raises a `TypeError` when it is called with two arguments, \"a\" and \"b\". This suggests that the `char_length` function is not designed to accept two arguments, and passing them should result in a `TypeError`.\n\n2. The second `assert_raises` call checks if the `char_length` function raises a `TypeError` when it is called without any arguments. This indicates that the `char_length` function requires at least one argument to operate correctly, and calling it without any arguments should result in a `TypeError`.\n\n**Note**: Points to note about the use of the code\n- This function is part of a test suite and is intended to validate the error handling of the `char_length` function. It should be used in conjunction with other test cases to ensure comprehensive coverage of the `char_length` function's behavior.\n- The `assert_raises` method is used to assert that a specific exception is raised when the function is called with invalid arguments. This is a common pattern in unit testing to verify that functions handle errors correctly."
      ],
      "code_start_line": 494,
      "code_end_line": 496,
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def test_char_length_fixed_args(self):\n        assert_raises(TypeError, func.char_length, \"a\", \"b\")\n        assert_raises(TypeError, func.char_length)\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "test_return_type_detection",
      "md_content": [
        "**test_return_type_detection**: The function of test_return_type_detection is to verify that the return type of specific SQL functions matches the expected SQL type based on the input arguments.\n\n**parameters**: This function does not take any external parameters. It operates on predefined functions and argument sets within the function itself.\n\n**Code Description**: \nThe `test_return_type_detection` function is designed to test the return type detection of several SQL functions, including `coalesce`, `max`, `min`, and `sum`. It iterates over a list of these functions and a set of predefined argument pairs, each paired with an expected SQL type. For each function and argument pair, the function asserts that the return type of the function call matches the expected SQL type. The argument pairs include various data types such as `datetime.date`, integers, `decimal.Decimal`, strings, and `datetime.datetime`. Additionally, the function includes a specific test for the `concat` function to ensure it returns a `sqltypes.String` type when concatenating two strings.\n\nThe function uses the `isinstance` method to check if the return type of the function call matches the expected type. If the assertion fails, it provides a detailed error message indicating the function, the actual return type, and the expected type.\n\n**Note**: This function is primarily used for testing purposes to ensure that SQL functions return the correct SQL type based on the input arguments. It assumes that the SQL functions and types are correctly implemented and available in the environment where the test is run.\n\n**Output Example**: Since this is a test function, it does not return a value but raises an assertion error if any of the tests fail. For example, if the `max` function with integer arguments does not return a `sqltypes.Integer` type, the function will raise an assertion error with a message like: \"max / <actual_type> != <sqltypes.Integer>\"."
      ],
      "code_start_line": 498,
      "code_end_line": 522,
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def test_return_type_detection(self):\n        for fn in [func.coalesce, func.max, func.min, func.sum]:\n            for args, type_ in [\n                (\n                    (datetime.date(2007, 10, 5), datetime.date(2005, 10, 15)),\n                    sqltypes.Date,\n                ),\n                ((3, 5), sqltypes.Integer),\n                ((decimal.Decimal(3), decimal.Decimal(5)), sqltypes.Numeric),\n                ((\"foo\", \"bar\"), sqltypes.String),\n                (\n                    (\n                        datetime.datetime(2007, 10, 5, 8, 3, 34),\n                        datetime.datetime(2005, 10, 15, 14, 45, 33),\n                    ),\n                    sqltypes.DateTime,\n                ),\n            ]:\n                assert isinstance(fn(*args).type, type_), \"%s / %r != %s\" % (\n                    fn(),\n                    fn(*args).type,\n                    type_,\n                )\n\n        assert isinstance(func.concat(\"foo\", \"bar\").type, sqltypes.String)\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "test_assorted",
      "md_content": [
        "**test_assorted**: The function of test_assorted is to test various SQL expression compilations and functionalities, including function calls, dotted function names, bind parameters, NULL handling, pickling, and attribute access.\n\n**parameters**: The parameters of this Function.\n· self: The instance of the test class, used to access the test methods and assertions.\n\n**Code Description**: The description of this Function.\nThe `test_assorted` function is a comprehensive test case that verifies the correct compilation of SQL expressions and function calls in different scenarios. It performs the following tests:\n\n1. **Function Expression with Table Columns**: \n   - Tests the compilation of a SQL expression involving a custom function (`func.lala`) with multiple arguments, including a literal value and a table column. The result is verified against the expected SQL string.\n\n2. **SELECT Statement with Aggregate Function**:\n   - Tests the compilation of a `SELECT` statement using the `func.count` aggregate function on a table column. The expected SQL string is compared to ensure correctness.\n\n3. **Dotted Function Name**:\n   - Tests the compilation of a `SELECT` statement using a function with a dotted name (`func.foo.bar.lala`). The function is applied to a table column, and the resulting SQL string is verified.\n\n4. **Bind Parameter with Dotted Function Name**:\n   - Tests the compilation of a `SELECT` statement using a function with a dotted name and a bind parameter. The bind parameter name is verified to ensure it is correctly generated.\n\n5. **Dotted Function Name Off Engine**:\n   - Tests the compilation of a function call with a dotted name directly off the engine. The resulting SQL string is verified.\n\n6. **NULL Handling in Function Calls**:\n   - Tests the compilation of a function call with `NULL` as one of the arguments. The SQL string is verified to ensure `NULL` is correctly represented.\n\n7. **Pickling of Function Call**:\n   - Tests the pickling and unpickling of a function call object. The unpickled object is verified to ensure it compiles to the same SQL string as the original.\n\n8. **AttributeError for __bases__**:\n   - Tests that the `func` object raises an `AttributeError` when accessing the `__bases__` attribute, ensuring it is not treated as a class.\n\n**Note**: Points to note about the use of the code\n- The function uses `self.assert_compile` to verify the correctness of SQL expression compilation. This method is crucial for ensuring that the generated SQL matches the expected output.\n- The test cases cover a wide range of scenarios, including complex function calls, dotted function names, and special cases like `NULL` handling and pickling.\n- The function also includes a test for attribute access, ensuring that the `func` object behaves correctly when accessed in ways that are not typical for classes."
      ],
      "code_start_line": 524,
      "code_end_line": 579,
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def test_assorted(self):\n        table1 = table(\"mytable\", column(\"myid\", Integer))\n\n        table2 = table(\"myothertable\", column(\"otherid\", Integer))\n\n        # test an expression with a function\n        self.assert_compile(\n            func.lala(3, 4, literal(\"five\"), table1.c.myid) * table2.c.otherid,\n            \"lala(:lala_1, :lala_2, :param_1, mytable.myid) * \"\n            \"myothertable.otherid\",\n        )\n\n        # test it in a SELECT\n        self.assert_compile(\n            select(func.count(table1.c.myid)),\n            \"SELECT count(mytable.myid) AS count_1 FROM mytable\",\n        )\n\n        # test a \"dotted\" function name\n        self.assert_compile(\n            select(func.foo.bar.lala(table1.c.myid)),\n            \"SELECT foo.bar.lala(mytable.myid) AS lala_1 FROM mytable\",\n        )\n\n        # test the bind parameter name with a \"dotted\" function name is\n        # only the name (limits the length of the bind param name)\n        self.assert_compile(\n            select(func.foo.bar.lala(12)),\n            \"SELECT foo.bar.lala(:lala_2) AS lala_1\",\n        )\n\n        # test a dotted func off the engine itself\n        self.assert_compile(func.lala.hoho(7), \"lala.hoho(:hoho_1)\")\n\n        # test None becomes NULL\n        self.assert_compile(\n            func.my_func(1, 2, None, 3),\n            \"my_func(:my_func_1, :my_func_2, NULL, :my_func_3)\",\n        )\n\n        f1 = func.my_func(1, 2, None, 3)\n        f1._generate_cache_key()\n\n        # test pickling\n        self.assert_compile(\n            pickle.loads(pickle.dumps(f1)),\n            \"my_func(:my_func_1, :my_func_2, NULL, :my_func_3)\",\n        )\n\n        # assert func raises AttributeError for __bases__ attribute, since\n        # its not a class fixes pydoc\n        try:\n            func.__bases__\n            assert False\n        except AttributeError:\n            assert True\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "test_pickle_over",
      "md_content": [
        "**test_pickle_over**: The function of test_pickle_over is to verify the correct serialization and deserialization of a SQL window function using Python's `pickle` module.\n\n**parameters**: This function does not take any parameters other than the implicit `self` parameter, which refers to the instance of the test class.\n\n**Code Description**: \nThe `test_pickle_over` function tests the pickling (serialization) and unpickling (deserialization) process of a SQL window function, specifically the `row_number()` function with an `OVER()` clause. The function begins by creating an instance of the `row_number()` window function using `func.row_number().over()`. This function is then serialized using `pickle.dumps()`, which converts the function object into a byte stream. The byte stream is subsequently deserialized back into a function object using `pickle.loads()`. The test then uses `self.assert_compile` to verify that the deserialized function object compiles to the expected SQL string, `\"row_number() OVER ()\"`. This ensures that the pickling process preserves the functionality and structure of the SQL window function.\n\n**Note**: This test is currently marked as incomplete with a TODO comment, indicating that the broader SQL package lacks a comprehensive pickling test suite. The test is a placeholder for future development, particularly in the context of testing SQL elements' serialization and deserialization capabilities."
      ],
      "code_start_line": 581,
      "code_end_line": 594,
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def test_pickle_over(self):\n        # TODO: the test/sql package lacks a comprehensive pickling\n        # test suite even though there are __reduce__ methods in several\n        # places in sql/elements.py.   likely as part of\n        # test/sql/test_compare.py might be a place this can happen but\n        # this still relies upon a strategy for table metadata as we have\n        # in serializer.\n\n        f1 = func.row_number().over()\n\n        self.assert_compile(\n            pickle.loads(pickle.dumps(f1)),\n            \"row_number() OVER ()\",\n        )\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "test_pickle_within_group",
      "md_content": [
        "**test_pickle_within_group**: The function of test_pickle_within_group is to test the pickling functionality of SQL expressions involving the `WITHIN GROUP` clause, specifically for the `percentile_cont` function.\n\n**parameters**: This function does not take any parameters.\n\n**Code Description**: \nThe `test_pickle_within_group` function is designed to verify that SQL expressions using the `WITHIN GROUP` clause can be properly serialized and deserialized using Python's `pickle` module. The test focuses on the `percentile_cont` function, which is a window function used in SQL for calculating percentiles.\n\n1. The function begins by creating an SQL expression using `func.percentile_cont(literal(1)).within_group()`. This expression represents a percentile calculation with an empty `WITHIN GROUP` clause.\n2. The expression is then serialized using `pickle.dumps` and immediately deserialized using `pickle.loads`. The deserialized expression is passed to `self.assert_compile`, which checks if the compiled SQL matches the expected output: `\"percentile_cont(:param_1) WITHIN GROUP (ORDER BY )\"`.\n3. Next, the function creates a more complex SQL expression using `func.percentile_cont(literal(1)).within_group(column(\"q\"), column(\"p\").desc())`. This expression includes a `WITHIN GROUP` clause with two columns, `q` and `p`, where `p` is sorted in descending order.\n4. Similar to the first part, this expression is serialized and deserialized, and the result is checked using `self.assert_compile` to ensure it matches the expected SQL: `\"percentile_cont(:param_1) WITHIN GROUP (ORDER BY q, p DESC)\"`.\n\n**Note**: \n- This test is part of a larger effort to ensure that SQL expressions, particularly those involving complex clauses like `WITHIN GROUP`, can be reliably pickled and unpickled. This is important for scenarios where SQL expressions need to be serialized for storage or transmission.\n- The test currently focuses on the `percentile_cont` function, but the underlying issue (test #6520) suggests that a more comprehensive pickling test suite for SQL expressions is needed. This test is a step towards addressing that gap."
      ],
      "code_start_line": 596,
      "code_end_line": 619,
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def test_pickle_within_group(self):\n        \"\"\"test #6520\"\"\"\n\n        # TODO: the test/sql package lacks a comprehensive pickling\n        # test suite even though there are __reduce__ methods in several\n        # places in sql/elements.py.   likely as part of\n        # test/sql/test_compare.py might be a place this can happen but\n        # this still relies upon a strategy for table metadata as we have\n        # in serializer.\n\n        f1 = func.percentile_cont(literal(1)).within_group()\n\n        self.assert_compile(\n            pickle.loads(pickle.dumps(f1)),\n            \"percentile_cont(:param_1) WITHIN GROUP (ORDER BY )\",\n        )\n\n        f1 = func.percentile_cont(literal(1)).within_group(\n            column(\"q\"), column(\"p\").desc()\n        )\n        self.assert_compile(\n            pickle.loads(pickle.dumps(f1)),\n            \"percentile_cont(:param_1) WITHIN GROUP (ORDER BY q, p DESC)\",\n        )\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "test_functions_with_cols",
      "md_content": [
        "**test_functions_with_cols**: The function of test_functions_with_cols is to test the compilation of SQL queries that involve user-defined functions and column references within a subquery, ensuring the generated SQL matches the expected output.\n\n**parameters**: The parameters of this Function.\n· self: The instance of the test class, used to access assertion methods and other class attributes.\n\n**Code Description**: The description of this Function.\nThe `test_functions_with_cols` function is designed to verify the correct compilation of SQL queries that utilize user-defined functions and column references within subqueries. The function begins by defining a table named \"users\" with columns \"id\", \"name\", and \"fullname\". It then creates a subquery named \"calculate\" using the `func.calculate` function, which takes two bind parameters, \"x\" and \"y\". This subquery selects columns \"q\", \"z\", and \"r\" from the result of the `func.calculate` function.\n\nThe function proceeds to test the compilation of a SQL query that selects from the \"users\" table where the \"id\" column is greater than the \"z\" column from the \"calculate\" subquery. The expected SQL output is compared using the `assert_compile` method, ensuring the generated SQL matches the expected string.\n\nNext, the function tests a more complex scenario where the \"id\" column of the \"users\" table is checked to be between the \"z\" column values of two aliased versions of the \"calculate\" subquery. Each alias is given unique parameter values for \"x\" and \"y\". The `assert_compile` method is used again to verify that the generated SQL matches the expected output, including the correct parameter bindings.\n\n**Note**: Points to note about the use of the code.\n- The `func.calculate` function is assumed to be a user-defined SQL function that takes two parameters, \"x\" and \"y\".\n- The `assert_compile` method is used to compare the generated SQL string with the expected output, ensuring the SQL compilation is correct.\n- The use of `unique_params` ensures that each alias of the subquery has distinct parameter values, which is crucial for the correctness of the SQL query.\n- The `checkparams` argument in the second `assert_compile` call verifies that the correct parameter values are bound in the generated SQL."
      ],
      "code_start_line": 621,
      "code_end_line": 656,
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def test_functions_with_cols(self):\n        users = table(\n            \"users\", column(\"id\"), column(\"name\"), column(\"fullname\")\n        )\n        calculate = (\n            select(column(\"q\"), column(\"z\"), column(\"r\"))\n            .select_from(\n                func.calculate(bindparam(\"x\", None), bindparam(\"y\", None))\n            )\n            .subquery()\n        )\n\n        self.assert_compile(\n            select(users).where(users.c.id > calculate.c.z),\n            \"SELECT users.id, users.name, users.fullname \"\n            \"FROM users, (SELECT q, z, r \"\n            \"FROM calculate(:x, :y)) AS anon_1 \"\n            \"WHERE users.id > anon_1.z\",\n        )\n\n        s = select(users).where(\n            users.c.id.between(\n                calculate.alias(\"c1\").unique_params(x=17, y=45).c.z,\n                calculate.alias(\"c2\").unique_params(x=5, y=12).c.z,\n            ),\n        )\n\n        self.assert_compile(\n            s,\n            \"SELECT users.id, users.name, users.fullname \"\n            \"FROM users, (SELECT q, z, r \"\n            \"FROM calculate(:x_1, :y_1)) AS c1, (SELECT q, z, r \"\n            \"FROM calculate(:x_2, :y_2)) AS c2 \"\n            \"WHERE users.id BETWEEN c1.z AND c2.z\",\n            checkparams={\"y_1\": 45, \"x_1\": 17, \"y_2\": 12, \"x_2\": 5},\n        )\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "test_non_functions",
      "md_content": [
        "**test_non_functions**: The function of test_non_functions is to test the compilation of non-function expressions, specifically focusing on SQL casting and extraction operations.\n\n**parameters**: The parameters of this Function.\n· self: The instance of the test class, used to access assertion methods and other class attributes.\n\n**Code Description**: The description of this Function.\nThe `test_non_functions` function is designed to verify the correct compilation of SQL expressions that involve casting and extraction operations. It performs two main tests:\n\n1. **Casting Operation**: The first part of the function tests the SQL `CAST` operation. It creates an expression using `func.cast(\"foo\", Integer)`, which is intended to cast the string \"foo\" to an integer type. The function then asserts that the compiled SQL statement matches the expected output, `\"CAST(:param_1 AS INTEGER)\"`. This ensures that the SQL compiler correctly handles the casting of a string to an integer.\n\n2. **Extraction Operation**: The second part of the function tests the SQL `EXTRACT` operation. It creates an expression using `func.extract(\"year\", datetime.date(2010, 12, 5))`, which extracts the year from the given date. The function then asserts that the compiled SQL statement matches the expected output, `\"EXTRACT(year FROM :param_1)\"`. This ensures that the SQL compiler correctly handles the extraction of the year from a date.\n\n**Note**: Points to note about the use of the code\n- The function relies on the `func` module to generate SQL expressions, which is typically part of an ORM (Object-Relational Mapping) library like SQLAlchemy.\n- The `assert_compile` method is used to compare the generated SQL with the expected SQL string. This method is likely part of a testing framework or utility specific to the project.\n- The function assumes that the SQL dialect being tested supports the `CAST` and `EXTRACT` operations as described. If the dialect does not support these operations, the test may fail or require adjustments."
      ],
      "code_start_line": 658,
      "code_end_line": 663,
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def test_non_functions(self):\n        expr = func.cast(\"foo\", Integer)\n        self.assert_compile(expr, \"CAST(:param_1 AS INTEGER)\")\n\n        expr = func.extract(\"year\", datetime.date(2010, 12, 5))\n        self.assert_compile(expr, \"EXTRACT(year FROM :param_1)\")\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "test_select_method_one",
      "md_content": [
        "**test_select_method_one**: The function of test_select_method_one is to test the compilation of a SQL SELECT statement generated by the `rows` function from the `func` module.\n\n**parameters**: The parameters of this Function.\n· self: The instance of the test class, used to access assertion methods and other class attributes.\n\n**Code Description**: The function `test_select_method_one` performs a test to verify that the SQL SELECT statement generated by the `rows` function is correctly compiled. The `rows` function is called with the argument `\"foo\"`, which represents a placeholder or identifier for the rows being selected. The result of this function call is stored in the variable `expr`. \n\nThe `select()` method is then called on `expr` to generate a SQL SELECT statement. The `assert_compile` method is used to compare the generated SQL statement with the expected SQL string `\"SELECT rows(:rows_2) AS rows_1\"`. This assertion ensures that the SQL statement is correctly formatted and matches the expected output.\n\n**Note**: This test function is part of a larger test suite and assumes that the `func` module and the `assert_compile` method are properly set up and functional. The test is designed to validate the correctness of the SQL generation logic, specifically for the `rows` function."
      ],
      "code_start_line": 665,
      "code_end_line": 667,
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def test_select_method_one(self):\n        expr = func.rows(\"foo\")\n        self.assert_compile(expr.select(), \"SELECT rows(:rows_2) AS rows_1\")\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "test_alias_method_one",
      "md_content": [
        "**test_alias_method_one**: The function of test_alias_method_one is to test the alias functionality of a SQL expression generated by the `rows` function.\n\n**parameters**: The parameters of this Function.\n· self: The instance of the test class, used to access assertion methods and other class-specific functionalities.\n\n**Code Description**: \nThe `test_alias_method_one` function is a test case designed to verify the behavior of the `alias` method when applied to a SQL expression generated by the `rows` function. The function begins by creating a SQL expression using `func.rows(\"foo\")`, which generates a SQL expression representing a row with the value \"foo\". This expression is then aliased using the `alias()` method. The test asserts that the compiled SQL string of the aliased expression matches the expected string \"rows(:rows_1)\". This ensures that the `alias` method correctly applies an alias to the SQL expression and that the resulting SQL string is formatted as expected.\n\n**Note**: \n- This test is part of a larger suite of tests aimed at validating the functionality of SQL expression generation and manipulation.\n- The expected SQL string \"rows(:rows_1)\" indicates that the alias method is working as intended, and the placeholder \":rows_1\" is correctly generated for the value \"foo\"."
      ],
      "code_start_line": 669,
      "code_end_line": 671,
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def test_alias_method_one(self):\n        expr = func.rows(\"foo\")\n        self.assert_compile(expr.alias(), \"rows(:rows_1)\")\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "test_select_method_two",
      "md_content": [
        "**test_select_method_two**: The function of test_select_method_two is to test the compilation of a SQL SELECT statement that involves a subquery generated from a function call.\n\n**parameters**: The parameters of this Function.\n· self: The instance of the test class, used to access assertion methods and other class attributes.\n\n**Code Description**: The function begins by creating an expression `expr` using the `func.rows(\"foo\")` method, which represents a SQL function call to `rows` with the argument `\"foo\"`. This expression is then used to construct a subquery by calling `expr.select().subquery()`. The subquery is embedded within a larger SELECT statement using the `select(\"*\").select_from()` method, which selects all columns from the result of the subquery. The `self.assert_compile` method is used to verify that the generated SQL query matches the expected SQL string: `\"SELECT * FROM (SELECT rows(:rows_2) AS rows_1) AS anon_1\"`. This ensures that the SQL compilation process produces the correct output for the given input.\n\n**Note**: This test function is specifically designed to validate the SQL compilation logic for a SELECT statement involving a subquery derived from a function call. It assumes familiarity with SQLAlchemy's query-building and compilation mechanisms. Ensure that the `func.rows` function and the `select` method are properly defined and imported in the context where this test is executed."
      ],
      "code_start_line": 673,
      "code_end_line": 678,
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def test_select_method_two(self):\n        expr = func.rows(\"foo\")\n        self.assert_compile(\n            select(\"*\").select_from(expr.select().subquery()),\n            \"SELECT * FROM (SELECT rows(:rows_2) AS rows_1) AS anon_1\",\n        )\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "test_select_method_three",
      "md_content": [
        "**test_select_method_three**: The function of test_select_method_three is to test the compilation of a SQL SELECT statement that uses a custom function `rows` as the source table in the `FROM` clause.\n\n**parameters**: This function does not take any external parameters. It is a method within a test class, and `self` refers to the instance of the test class.\n\n**Code Description**: \n- The function begins by creating an expression `expr` using the `rows` function from the `func` module, passing the string `\"foo\"` as an argument. This expression represents a table or rowset named `\"foo\"`.\n- The `select` function is then used to create a SQL SELECT statement, where `column(\"foo\")` specifies the column to be selected. The `select_from` method is called on this SELECT statement, with `expr` (the result of `rows(\"foo\")`) as the source table.\n- The `self.assert_compile` method is used to verify that the generated SQL query matches the expected SQL string `\"SELECT foo FROM rows(:rows_1)\"`. This ensures that the SQL compilation process works correctly for this specific use case.\n\n**Note**: \n- This test function is designed to validate the correct compilation of SQL queries involving custom functions like `rows`. It assumes that the `rows` function and the `select` and `column` utilities are properly implemented and available in the context.\n- The expected SQL string includes a parameter placeholder `:rows_1`, which indicates that the `rows` function is parameterized and expects a value to be bound at runtime."
      ],
      "code_start_line": 680,
      "code_end_line": 685,
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def test_select_method_three(self):\n        expr = func.rows(\"foo\")\n        self.assert_compile(\n            select(column(\"foo\")).select_from(expr),\n            \"SELECT foo FROM rows(:rows_1)\",\n        )\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "test_alias_method_two",
      "md_content": [
        "**test_alias_method_two**: The function of test_alias_method_two is to test the alias functionality of a SQL expression using the `rows` function and verify that the generated SQL query matches the expected output.\n\n**parameters**: The parameters of this Function.\n· self: The instance of the test class, used to access assertion methods and other class attributes.\n\n**Code Description**: \nThe function `test_alias_method_two` performs the following steps:\n1. It creates a SQL expression using the `rows` function with the argument `\"foo\"`. This expression represents a SQL function call that generates rows.\n2. The expression is then aliased with the name `\"bar\"` using the `alias` method. This creates a named subquery or derived table in SQL.\n3. The function constructs a SQL `SELECT` query using the `select` function, selecting all columns (`*`) from the aliased expression.\n4. The `assert_compile` method is used to compare the compiled SQL query with the expected SQL string `\"SELECT * FROM rows(:rows_1) AS bar\"`. This ensures that the SQL generation logic works as intended.\n\n**Note**: \n- This test is specifically designed to verify the correct compilation of SQL queries involving aliased expressions.\n- The `rows` function and its alias are used to simulate a common SQL pattern where a function or subquery is given a temporary name for use in a larger query.\n- Ensure that the SQL dialect and the underlying database engine support the `rows` function and the alias syntax used in this test."
      ],
      "code_start_line": 687,
      "code_end_line": 692,
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def test_alias_method_two(self):\n        expr = func.rows(\"foo\")\n        self.assert_compile(\n            select(\"*\").select_from(expr.alias(\"bar\")),\n            \"SELECT * FROM rows(:rows_1) AS bar\",\n        )\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "test_alias_method_columns",
      "md_content": [
        "**test_alias_method_columns**: The function of test_alias_method_columns is to test the behavior of the `alias` method when applied to a SQL expression, ensuring that the column list is exported correctly without causing errors.\n\n**parameters**: The function does not take any external parameters. It operates within the context of the test class and uses the `self` parameter to access class methods and attributes.\n\n**Code Description**: \nThe function begins by creating a SQL expression using the `func.rows(\"foo\")` method, which generates a SQL function call for the `rows` function with the argument `\"foo\"`. This expression is then aliased using the `alias(\"bar\")` method, resulting in the alias `\"bar\"` being assigned to the expression. \n\nThe primary purpose of this test is to verify that the SQL expression, when aliased, correctly exports its column list in a way that does not break the SQL compilation process. This is particularly important for maintaining backward compatibility, as the behavior being tested was the default prior to a specific change (referenced as #2974 in the code comments).\n\nThe test uses the `self.assert_compile` method to compile the SQL expression into a SQL query string and compares it against the expected output: `\"SELECT bar.rows_1 FROM rows(:rows_2) AS bar\"`. This ensures that the alias is correctly applied and that the resulting SQL query is syntactically valid.\n\n**Note**: This test is primarily focused on ensuring backward compatibility and does not represent a particularly useful or common use case for the `alias` method. It serves as a safeguard to confirm that the column list export mechanism remains functional and does not introduce breaking changes."
      ],
      "code_start_line": 694,
      "code_end_line": 703,
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def test_alias_method_columns(self):\n        expr = func.rows(\"foo\").alias(\"bar\")\n\n        # this isn't very useful but is the old behavior\n        # prior to #2974.\n        # testing here that the expression exports its column\n        # list in a way that at least doesn't break.\n        self.assert_compile(\n            select(expr), \"SELECT bar.rows_1 FROM rows(:rows_2) AS bar\"\n        )\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "test_alias_method_columns_two",
      "md_content": [
        "**test_alias_method_columns_two**: The function of test_alias_method_columns_two is to test the behavior of the `alias` method when applied to a function that generates rows, specifically checking the length of the resulting columns.\n\n**parameters**: This function does not take any parameters. It is a test method within a class, and it operates on predefined or internally generated data.\n\n**Code Description**: \nThe function begins by creating an expression `expr` using the `func.rows(\"foo\")` method, which generates rows based on the input \"foo\". The `alias(\"bar\")` method is then applied to this expression, assigning it an alias \"bar\". The purpose of this alias is to provide a new name for the resulting column or set of rows. \n\nAfter creating the aliased expression, the function asserts that the length of the columns (`expr.c`) is as expected. This assertion is used to verify that the alias operation does not alter the structure or the number of columns generated by the original `func.rows(\"foo\")` method.\n\n**Note**: This function is primarily used for testing purposes and assumes that the `func.rows` method and the `alias` method are correctly implemented. It does not handle any exceptions or errors, as it is designed to validate specific behavior under controlled conditions."
      ],
      "code_start_line": 705,
      "code_end_line": 707,
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def test_alias_method_columns_two(self):\n        expr = func.rows(\"foo\").alias(\"bar\")\n        assert len(expr.c)\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "test_funcfilter_empty",
      "md_content": [
        "**test_funcfilter_empty**: The function of test_funcfilter_empty is to test the behavior of a filtered SQL function call when no filter condition is applied.\n\n**parameters**: The parameters of this Function.\n· self: Represents the instance of the test class, used to access assertion methods and other class attributes.\n\n**Code Description**: The description of this Function.\nThe `test_funcfilter_empty` function is a test case that verifies the SQL compilation behavior of a filtered function call when no filter condition is provided. Specifically, it tests the `func.count(1).filter()` expression, which is a SQLAlchemy construct representing a count function with an empty filter. The function uses the `assert_compile` method to compare the compiled SQL output of this expression with the expected SQL string `\"count(:count_1)\"`. This ensures that the SQLAlchemy query builder correctly handles the case where a filter is applied to a function but no condition is specified.\n\n**Note**: Points to note about the use of the code\n- This test case is designed to validate the SQLAlchemy ORM's behavior in a specific edge case where a filter is applied without any conditions. It is important to ensure that such cases are handled gracefully and produce the expected SQL output.\n- The `assert_compile` method is used to verify that the generated SQL matches the expected string, which is a common practice in SQLAlchemy test cases to ensure correctness in query generation."
      ],
      "code_start_line": 709,
      "code_end_line": 710,
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def test_funcfilter_empty(self):\n        self.assert_compile(func.count(1).filter(), \"count(:count_1)\")\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "test_funcfilter_criterion",
      "md_content": [
        "**test_funcfilter_criterion**: The function of test_funcfilter_criterion is to test the compilation of a SQL function with a FILTER clause applied to a count operation.\n\n**parameters**: This function does not take any external parameters. It is a method within a test class and uses the `self` parameter to access class attributes and methods.\n\n**Code Description**: \nThe `test_funcfilter_criterion` function is a test case that verifies the correct compilation of a SQL expression involving the `count` function combined with a `FILTER` clause. Specifically, it tests the scenario where the `count` function is applied to the value `1`, and a filter condition is added to exclude rows where the `name` column of `table1` is `NULL`.\n\nThe function uses the `assert_compile` method, which is presumably a helper method within the test class, to compare the generated SQL string with the expected SQL string. The SQL expression being tested is `func.count(1).filter(table1.c.name != None)`, which translates to counting the number of rows where the `name` column is not `NULL`. The expected SQL output is `\"count(:count_1) FILTER (WHERE mytable.name IS NOT NULL)\"`.\n\n**Note**: \n- The `filter` clause in SQL is used to apply a condition to an aggregate function, such as `count`, to include only rows that meet the specified condition.\n- The `assert_compile` method is crucial for verifying that the SQL expression is correctly translated into the expected SQL string. Ensure that this method is properly implemented in the test class.\n- The `# noqa` comment is used to suppress linting warnings, indicating that the code is intentionally written in this way and should not be flagged by linters."
      ],
      "code_start_line": 712,
      "code_end_line": 716,
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def test_funcfilter_criterion(self):\n        self.assert_compile(\n            func.count(1).filter(table1.c.name != None),  # noqa\n            \"count(:count_1) FILTER (WHERE mytable.name IS NOT NULL)\",\n        )\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "test_funcfilter_compound_criterion",
      "md_content": [
        "**test_funcfilter_compound_criterion**: The function of test_funcfilter_compound_criterion is to test the compilation of a SQL function with a compound filter criterion.  \n**parameters**: The parameters of this Function.  \n· self: The instance of the test class, used to access assertion methods and other class attributes.  \n\n**Code Description**:  \nThe function `test_funcfilter_compound_criterion` is a test case that verifies the correct compilation of a SQL function with a compound filter criterion. Specifically, it tests the SQL `COUNT` function combined with a `FILTER` clause that includes multiple conditions.  \n\nThe test uses the `assert_compile` method to compare the generated SQL string with the expected output. The SQL function being tested is `func.count(1)`, which counts the number of rows where the value `1` appears. This function is further filtered using the `filter` method, which applies two conditions:  \n1. `table1.c.name == None`: This condition checks if the `name` column in `table1` is `NULL`.  \n2. `table1.c.myid > 0`: This condition checks if the `myid` column in `table1` is greater than `0`.  \n\nThe expected SQL output is:  \n```sql\ncount(:count_1) FILTER (WHERE mytable.name IS NULL AND mytable.myid > :myid_1)\n```  \nThis output represents the SQL `COUNT` function with a `FILTER` clause that combines the two conditions using the `AND` operator.  \n\n**Note**:  \n- The `# noqa` comment is used to suppress linting warnings for the `None` comparison, which is a common practice in SQLAlchemy when comparing columns to `NULL`.  \n- The test assumes that `table1` is a predefined table object with columns `name` and `myid`.  \n- The `assert_compile` method is likely a custom utility method in the test framework that compares the generated SQL string with the expected output."
      ],
      "code_start_line": 718,
      "code_end_line": 725,
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def test_funcfilter_compound_criterion(self):\n        self.assert_compile(\n            func.count(1).filter(\n                table1.c.name == None, table1.c.myid > 0  # noqa\n            ),\n            \"count(:count_1) FILTER (WHERE mytable.name IS NULL AND \"\n            \"mytable.myid > :myid_1)\",\n        )\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "test_funcfilter_arrayagg_subscript",
      "md_content": [
        "**test_funcfilter_arrayagg_subscript**: The function of test_funcfilter_arrayagg_subscript is to test the compilation of a PostgreSQL-specific SQL expression involving the `array_agg` function with a filter and array subscript.\n\n**parameters**: The parameters of this Function.\n· self: The instance of the test class, used to access assertion methods and other class attributes.\n· num: A column object representing the column \"q\" in the SQL expression.\n\n**Code Description**: \nThe function `test_funcfilter_arrayagg_subscript` is designed to verify the correct compilation of a SQL expression that combines the `array_agg` function, a filter condition, and an array subscript. The `array_agg` function aggregates values from the column \"q\" into an array. The filter condition `num % 2 == 0` ensures that only even values from the column \"q\" are included in the aggregation. The `[1]` subscript is used to access the second element of the resulting array (since array indexing in SQL typically starts at 1).\n\nThe `assert_compile` method is used to compare the compiled SQL expression with the expected SQL string. The expected SQL string is:\n```\n\"(array_agg(q) FILTER (WHERE q %% %(q_1)s = %(param_1)s))[%(param_2)s]\"\n```\nThis string represents the SQL expression where `array_agg(q)` aggregates the values of column \"q\", the `FILTER` clause ensures only even values are included, and the `[1]` subscript accesses the second element of the array. The `dialect=\"postgresql\"` argument specifies that the SQL expression should be compiled using PostgreSQL syntax.\n\n**Note**: \n- The function assumes that the SQL dialect is PostgreSQL, as the `array_agg` function with a filter and array subscript is specific to PostgreSQL.\n- The `assert_compile` method is used to ensure that the SQL expression is correctly translated into the expected SQL string, which is crucial for verifying the correctness of the SQL generation logic."
      ],
      "code_start_line": 727,
      "code_end_line": 734,
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def test_funcfilter_arrayagg_subscript(self):\n        num = column(\"q\")\n        self.assert_compile(\n            func.array_agg(num).filter(num % 2 == 0)[1],\n            \"(array_agg(q) FILTER (WHERE q %% %(q_1)s = \"\n            \"%(param_1)s))[%(param_2)s]\",\n            dialect=\"postgresql\",\n        )\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "test_funcfilter_label",
      "md_content": [
        "**test_funcfilter_label**: The function of test_funcfilter_label is to test the SQL compilation of a query that uses the `FILTER` clause with a `COUNT` function and a `LABEL` alias.\n\n**parameters**: The parameters of this Function.\n· self: Represents the instance of the test class, allowing access to its methods and attributes, including the `assert_compile` method.\n\n**Code Description**: \nThe `test_funcfilter_label` function is a test case that verifies the correct compilation of an SQL query. The query involves the `COUNT` function combined with a `FILTER` clause and a `LABEL` alias. Specifically, the function tests the following SQL logic:\n\n1. The `COUNT(1)` function is used to count rows, where `1` is a placeholder indicating that all rows should be counted.\n2. The `FILTER` clause is applied to the `COUNT` function, filtering rows where the `description` column in `table1` is not `NULL`. This is achieved using the condition `table1.c.description != None`.\n3. The result of the `COUNT` function with the `FILTER` clause is given an alias `foo` using the `LABEL` method.\n4. The `select` function constructs the SQL query, and the `assert_compile` method is used to compare the compiled SQL query with the expected SQL string.\n\nThe expected SQL output is:\n```sql\nSELECT count(:count_1) FILTER (WHERE mytable.description IS NOT NULL) AS foo FROM mytable\n```\nThis ensures that the SQL query is correctly compiled with the `FILTER` clause and the `LABEL` alias.\n\n**Note**: \n- The `# noqa` comment is used to suppress linting warnings for the `!= None` comparison, which is a common practice in SQLAlchemy to avoid style warnings.\n- The `assert_compile` method is a custom testing utility that compares the generated SQL with the expected SQL string, ensuring the correctness of the SQL compilation process."
      ],
      "code_start_line": 736,
      "code_end_line": 745,
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def test_funcfilter_label(self):\n        self.assert_compile(\n            select(\n                func.count(1)\n                .filter(table1.c.description != None)  # noqa\n                .label(\"foo\")\n            ),\n            \"SELECT count(:count_1) FILTER (WHERE mytable.description \"\n            \"IS NOT NULL) AS foo FROM mytable\",\n        )\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "test_funcfilter_fromobj_fromfunc",
      "md_content": [
        "**test_funcfilter_fromobj_fromfunc**: The function of test_funcfilter_fromobj_fromfunc is to test the generation of a SQL query using the `filter` clause with a function and a condition derived from an object.\n\n**parameters**: The parameters of this Function.\n· self: The instance of the test class, used to access assertion methods and other class attributes.\n\n**Code Description**: \nThe function `test_funcfilter_fromobj_fromfunc` is a test case that verifies the correct generation of a SQL query using the `filter` clause in conjunction with the `func.max` function. The query is constructed using SQLAlchemy's `select` function, which selects the maximum value of the `name` column from `table1`. The `filter` clause is applied to this selection, specifying a condition where the `description` column is not `NULL`. The condition is expressed using `literal_column(\"description\") != None`, which translates to `description IS NOT NULL` in the generated SQL query. The `assert_compile` method is then used to compare the generated SQL query with the expected SQL string: `\"SELECT max(mytable.name) FILTER (WHERE description IS NOT NULL) AS anon_1 FROM mytable\"`. This ensures that the SQLAlchemy query construction behaves as expected.\n\n**Note**: \n- The `filter` clause is used to apply a condition to an aggregate function (`func.max` in this case), which is a common SQL feature supported by SQLAlchemy.\n- The `literal_column` function is used to directly include a column name in the SQL expression without any additional processing, ensuring that the condition is correctly interpreted in the SQL query.\n- The `assert_compile` method is crucial for verifying that the SQLAlchemy query object compiles to the expected SQL string, which is a standard practice in SQLAlchemy testing."
      ],
      "code_start_line": 747,
      "code_end_line": 758,
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def test_funcfilter_fromobj_fromfunc(self):\n        # test from_obj generation.\n        # from func:\n        self.assert_compile(\n            select(\n                func.max(table1.c.name).filter(\n                    literal_column(\"description\") != None  # noqa\n                )\n            ),\n            \"SELECT max(mytable.name) FILTER (WHERE description \"\n            \"IS NOT NULL) AS anon_1 FROM mytable\",\n        )\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "test_funcfilter_fromobj_fromcriterion",
      "md_content": [
        "**test_funcfilter_fromobj_fromcriterion**: The function of test_funcfilter_fromobj_fromcriterion is to test the SQL compilation of a filtered aggregate function using SQLAlchemy's `func.count` with a filter condition applied.\n\n**parameters**: The function does not take any external parameters. It operates within the context of a test case class, utilizing the `self` parameter to access the test case's methods and assertions.\n\n**Code Description**: \nThe function `test_funcfilter_fromobj_fromcriterion` is a test case that verifies the correct compilation of an SQL query involving a filtered aggregate function. Specifically, it tests the SQLAlchemy `func.count` function combined with a filter condition. The test constructs a SQL query using `select(func.count(1).filter(table1.c.name == \"name\"))`, which translates to counting rows where the `name` column in `table1` matches the string \"name\". \n\nThe `assert_compile` method is then used to compare the generated SQL query with the expected SQL string: \n```sql\n\"SELECT count(:count_1) FILTER (WHERE mytable.name = :name_1) AS anon_1 FROM mytable\"\n```\nThis ensures that the SQLAlchemy query builder correctly generates the SQL syntax for a filtered aggregate function.\n\n**Note**: This test assumes the existence of a table named `mytable` with a column `name`. The test is specific to SQLAlchemy's query-building capabilities and does not execute the query against a database. It is purely a compilation test to verify the correctness of the generated SQL syntax."
      ],
      "code_start_line": 760,
      "code_end_line": 766,
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def test_funcfilter_fromobj_fromcriterion(self):\n        # from criterion:\n        self.assert_compile(\n            select(func.count(1).filter(table1.c.name == \"name\")),\n            \"SELECT count(:count_1) FILTER (WHERE mytable.name = :name_1) \"\n            \"AS anon_1 FROM mytable\",\n        )\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "test_funcfilter_chaining",
      "md_content": [
        "**test_funcfilter_chaining**: The function of test_funcfilter_chaining is to test the chaining of filter conditions in a SQL function call using SQLAlchemy's `func` and `filter` methods.\n\n**parameters**: The parameters of this Function.\n· self: The instance of the test class, used to access the test methods and assertions.\n\n**Code Description**: The description of this Function.\nThe `test_funcfilter_chaining` function is a test case that verifies the correct compilation of a SQL query involving chained filter conditions within a SQL function. Specifically, it tests the `func.count` function combined with multiple `filter` conditions. The function constructs a SQL query using SQLAlchemy's `select` statement, where the `func.count(1)` function is applied with two filter conditions: one checking if the `name` column in `table1` equals \"name\", and the other checking if the `description` column equals \"description\". The `assert_compile` method is then used to compare the generated SQL query with the expected SQL string. The expected SQL string includes the `FILTER` clause with both conditions combined using the `AND` operator, ensuring that the chaining of filters is correctly translated into the SQL syntax.\n\n**Note**: Points to note about the use of the code\n- This test function assumes the existence of a table named `mytable` with columns `name` and `description`.\n- The `assert_compile` method is used to verify that the SQLAlchemy query is correctly compiled into the expected SQL string.\n- The test ensures that the chaining of `filter` conditions within a SQL function is properly handled and translated into the correct SQL syntax."
      ],
      "code_start_line": 768,
      "code_end_line": 779,
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def test_funcfilter_chaining(self):\n        # test chaining:\n        self.assert_compile(\n            select(\n                func.count(1)\n                .filter(table1.c.name == \"name\")\n                .filter(table1.c.description == \"description\")\n            ),\n            \"SELECT count(:count_1) FILTER (WHERE \"\n            \"mytable.name = :name_1 AND mytable.description = :description_1) \"\n            \"AS anon_1 FROM mytable\",\n        )\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "test_funcfilter_windowing_orderby",
      "md_content": [
        "**test_funcfilter_windowing_orderby**: The function of test_funcfilter_windowing_orderby is to test the compilation of a SQL query that uses a filtered windowing function with an ORDER BY clause.\n\n**parameters**: The parameters of this Function.\n· self: The instance of the test class, used to access the assert_compile method and other test utilities.\n\n**Code Description**: The description of this Function.\nThe function `test_funcfilter_windowing_orderby` is designed to verify the correct compilation of a SQL query that involves a windowing function (`func.rank()`) with a filter condition and an ORDER BY clause. The query is constructed using the `select` function, which is part of the SQLAlchemy library. The `func.rank()` function is used to generate a ranking value for each row in the result set. The `.filter(table1.c.name > \"foo\")` part applies a filter condition to the ranking function, ensuring that only rows where the `name` column in `table1` is greater than \"foo\" are considered for ranking. The `.over(order_by=table1.c.name)` clause specifies that the ranking should be computed over a window of rows ordered by the `name` column in `table1`.\n\nThe `assert_compile` method is then used to compare the compiled SQL query with the expected SQL string. The expected SQL string is:\n```sql\nSELECT rank() FILTER (WHERE mytable.name > :name_1) OVER (ORDER BY mytable.name) AS anon_1 FROM mytable\n```\nThis string represents the SQL query that should be generated by the SQLAlchemy expression. The `:name_1` placeholder is used for the parameterized value \"foo\".\n\n**Note**: Points to note about the use of the code\n- The function assumes that `table1` is a predefined table object with a column named `name`.\n- The `assert_compile` method is a utility provided by the test framework to verify that the SQLAlchemy expression compiles to the expected SQL string.\n- The filter condition and ORDER BY clause are critical parts of the query, and their correct implementation is essential for the test to pass."
      ],
      "code_start_line": 781,
      "code_end_line": 791,
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def test_funcfilter_windowing_orderby(self):\n        # test filtered windowing:\n        self.assert_compile(\n            select(\n                func.rank()\n                .filter(table1.c.name > \"foo\")\n                .over(order_by=table1.c.name)\n            ),\n            \"SELECT rank() FILTER (WHERE mytable.name > :name_1) \"\n            \"OVER (ORDER BY mytable.name) AS anon_1 FROM mytable\",\n        )\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "test_funcfilter_windowing_orderby_partitionby",
      "md_content": [
        "**test_funcfilter_windowing_orderby_partitionby**: The function of test_funcfilter_windowing_orderby_partitionby is to test the SQL compilation of a query that uses a window function with filtering, ordering, and partitioning.\n\n**parameters**: The parameters of this Function.\n· self: The instance of the test class, used to access the assert_compile method for testing the SQL compilation.\n\n**Code Description**: \nThe function `test_funcfilter_windowing_orderby_partitionby` is a test case that verifies the correct compilation of an SQL query involving a window function with filtering, ordering, and partitioning. The query is constructed using SQLAlchemy's `select` function, which generates a SELECT statement. \n\nIn this query, the `func.rank()` function is used to calculate the rank of rows within a window. The `filter` method is applied to the `func.rank()` function to include only rows where the `name` column of `table1` is greater than the string \"foo\". The `over` method is then used to define the window for the rank calculation. The `order_by` parameter specifies that the rows should be ordered by the `name` column of `table1`, and the `partition_by` parameter specifies that the rows should be partitioned by the `description` column of `table1`.\n\nThe `assert_compile` method is used to compare the compiled SQL query with the expected SQL string. The expected SQL string is:\n```sql\nSELECT rank() FILTER (WHERE mytable.name > :name_1) \nOVER (PARTITION BY mytable.description ORDER BY mytable.name) \nAS anon_1 FROM mytable\n```\nThis ensures that the SQLAlchemy query is correctly translated into the expected SQL syntax.\n\n**Note**: This test function is specifically designed to verify the correct compilation of SQL queries involving window functions with filtering, ordering, and partitioning. It is important to ensure that the expected SQL string matches the actual compiled SQL to confirm the correctness of the query construction."
      ],
      "code_start_line": 793,
      "code_end_line": 803,
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def test_funcfilter_windowing_orderby_partitionby(self):\n        self.assert_compile(\n            select(\n                func.rank()\n                .filter(table1.c.name > \"foo\")\n                .over(order_by=table1.c.name, partition_by=[\"description\"])\n            ),\n            \"SELECT rank() FILTER (WHERE mytable.name > :name_1) \"\n            \"OVER (PARTITION BY mytable.description ORDER BY mytable.name) \"\n            \"AS anon_1 FROM mytable\",\n        )\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "test_funcfilter_windowing_range",
      "md_content": [
        "**test_funcfilter_windowing_range**: The function of test_funcfilter_windowing_range is to test the compilation of a SQL query that uses the `rank()` window function with a filter and a specified range in the `OVER` clause.\n\n**parameters**: The parameters of this Function.\n· self: The instance of the test class, used to access assertion methods and other class attributes.\n\n**Code Description**: \nThe function `test_funcfilter_windowing_range` is a test case that verifies the correct compilation of a SQL query. The query involves the `rank()` window function, which is filtered using a condition (`table1.c.name > \"foo\"`) and applied over a specified range (`range_=(1, 5)`) within partitions defined by the `description` column. \n\nThe `assert_compile` method is used to compare the generated SQL query with the expected SQL string. The expected SQL string includes the `rank()` function with a `FILTER` clause, an `OVER` clause with a `RANGE BETWEEN` specification, and a `PARTITION BY` clause. The `checkparams` argument ensures that the parameters used in the query (such as `name_1`, `param_1`, and `param_2`) match the expected values.\n\n**Note**: \n- The test ensures that the SQL query is correctly compiled with the appropriate filter and range specifications.\n- The `checkparams` dictionary is used to validate that the parameters in the compiled SQL query match the expected values, ensuring the correctness of the query generation."
      ],
      "code_start_line": 805,
      "code_end_line": 817,
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def test_funcfilter_windowing_range(self):\n        self.assert_compile(\n            select(\n                func.rank()\n                .filter(table1.c.name > \"foo\")\n                .over(range_=(1, 5), partition_by=[\"description\"])\n            ),\n            \"SELECT rank() FILTER (WHERE mytable.name > :name_1) \"\n            \"OVER (PARTITION BY mytable.description RANGE BETWEEN :param_1 \"\n            \"FOLLOWING AND :param_2 FOLLOWING) \"\n            \"AS anon_1 FROM mytable\",\n            checkparams={\"name_1\": \"foo\", \"param_1\": 1, \"param_2\": 5},\n        )\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "test_funcfilter_windowing_range_positional",
      "md_content": [
        "**test_funcfilter_windowing_range_positional**: The function of `test_funcfilter_windowing_range_positional` is to test the SQL compilation of a window function with a filter and a range-based window specification using positional parameters.\n\n**parameters**: The function does not take any external parameters. It is a method within a test class and uses the `self` parameter to access class-level attributes and methods.\n\n**Code Description**: \nThe function `test_funcfilter_windowing_range_positional` is designed to verify the correct compilation of an SQL query that includes a window function with a filter and a range-based window specification. The function uses the `assert_compile` method to compare the generated SQL query with an expected SQL string. \n\nThe SQL query being tested is constructed using the `select` function, which includes a `rank()` window function. The `rank()` function is modified with a filter condition (`filter(table1.c.name > \"foo\")`) and a window specification (`over(range_=(1, 5), partition_by=[\"description\"])`). The filter condition specifies that only rows where the `name` column in `table1` is greater than \"foo\" should be considered. The window specification defines a range-based window that includes rows within a range of 1 to 5 following the current row, partitioned by the `description` column.\n\nThe expected SQL output is:\n```sql\nSELECT rank() FILTER (WHERE mytable.name > ?) \nOVER (PARTITION BY mytable.description RANGE BETWEEN ? \nFOLLOWING AND ? FOLLOWING) \nAS anon_1 FROM mytable\n```\nThe `checkpositional` parameter is used to specify the positional parameters (\"foo\", 1, 5) that should be used in the compiled SQL query. The `dialect` parameter is set to \"default_qmark\" to indicate that the SQL dialect should use question marks (`?`) as placeholders for positional parameters.\n\n**Note**: \n- The function is part of a test suite and is intended to ensure that the SQL compilation logic for window functions with filters and range-based windows works correctly.\n- The `assert_compile` method is used to compare the generated SQL with the expected SQL string, ensuring that the query is compiled as intended.\n- The positional parameters in the `checkpositional` argument must match the expected values in the SQL query to pass the test."
      ],
      "code_start_line": 819,
      "code_end_line": 832,
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def test_funcfilter_windowing_range_positional(self):\n        self.assert_compile(\n            select(\n                func.rank()\n                .filter(table1.c.name > \"foo\")\n                .over(range_=(1, 5), partition_by=[\"description\"])\n            ),\n            \"SELECT rank() FILTER (WHERE mytable.name > ?) \"\n            \"OVER (PARTITION BY mytable.description RANGE BETWEEN ? \"\n            \"FOLLOWING AND ? FOLLOWING) \"\n            \"AS anon_1 FROM mytable\",\n            checkpositional=(\"foo\", 1, 5),\n            dialect=\"default_qmark\",\n        )\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "test_funcfilter_windowing_rows",
      "md_content": [
        "**test_funcfilter_windowing_rows**: The function of test_funcfilter_windowing_rows is to test the compilation of a SQL query that uses the `rank()` window function with a filter condition and a specific windowing clause.\n\n**parameters**: The parameters of this Function.\n· self: The instance of the test class, used to access the `assert_compile` method and other class attributes.\n\n**Code Description**: The description of this Function.\nThe function `test_funcfilter_windowing_rows` is a test case that verifies the correct compilation of a SQL query. The query involves the `rank()` window function, which is applied with a filter condition (`table1.c.name > \"foo\"`) and a windowing clause (`rows=(1, 5)`). The `over` method is used to define the window, which includes partitioning by the `description` column and specifying a range of rows (from 1 to 5) within each partition.\n\nThe `assert_compile` method is called to compare the generated SQL query with the expected SQL string. The expected SQL string is:\n```\nSELECT rank() FILTER (WHERE mytable.name > :name_1) \nOVER (PARTITION BY mytable.description ROWS BETWEEN :param_1 \nFOLLOWING AND :param_2 FOLLOWING) \nAS anon_1 FROM mytable\n```\nThis string represents the SQL query that should be generated by the code. The `assert_compile` method ensures that the actual SQL query produced by the code matches this expected string.\n\n**Note**: Points to note about the use of the code\n- The `filter` method is used to apply a condition to the `rank()` function, which is a common use case in SQL for ranking rows based on specific criteria.\n- The `over` method with `rows=(1, 5)` specifies a window of rows relative to the current row, which is useful for calculations that depend on a specific range of rows.\n- The `partition_by` argument in the `over` method is used to divide the result set into partitions to which the `rank()` function is applied independently.\n- The `assert_compile` method is crucial for verifying that the SQL query is generated correctly, ensuring that the code behaves as expected."
      ],
      "code_start_line": 834,
      "code_end_line": 845,
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def test_funcfilter_windowing_rows(self):\n        self.assert_compile(\n            select(\n                func.rank()\n                .filter(table1.c.name > \"foo\")\n                .over(rows=(1, 5), partition_by=[\"description\"])\n            ),\n            \"SELECT rank() FILTER (WHERE mytable.name > :name_1) \"\n            \"OVER (PARTITION BY mytable.description ROWS BETWEEN :param_1 \"\n            \"FOLLOWING AND :param_2 FOLLOWING) \"\n            \"AS anon_1 FROM mytable\",\n        )\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "test_funcfilter_more_criteria",
      "md_content": [
        "**test_funcfilter_more_criteria**: The function of `test_funcfilter_more_criteria` is to test the functionality of applying multiple filter criteria to a SQL function using SQLAlchemy's `func.rank()` and `filter()` methods.\n\n**parameters**: This function does not take any external parameters. It operates within the context of the test class and uses predefined objects and methods.\n\n**Code Description**: \nThe function begins by creating a SQL function `rank()` using SQLAlchemy's `func.rank()` method. It then applies a filter condition to this function using the `filter()` method, specifying that the `name` column in `table1` should be greater than the string \"foo\". This filtered function is stored in the variable `ff`.\n\nNext, the function applies an additional filter condition to the previously filtered function `ff`. This second filter specifies that the `myid` column in `table1` should be equal to 1. The result of this second filter is stored in the variable `ff2`.\n\nThe function then uses the `assert_compile` method to verify that the SQL query generated by `select(ff, ff2)` matches the expected SQL string. The expected SQL string includes two instances of the `rank()` function, each with its own filter conditions. The first instance filters only by the `name` column, while the second instance filters by both the `name` and `myid` columns. The `assert_compile` method also checks that the parameters used in the query (`name_1` and `myid_1`) are correctly bound to the values \"foo\" and 1, respectively.\n\n**Note**: This function is part of a test suite and is designed to ensure that the SQLAlchemy ORM correctly handles the application of multiple filter criteria to SQL functions. It is important to ensure that the filter conditions are correctly combined in the generated SQL query."
      ],
      "code_start_line": 847,
      "code_end_line": 856,
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def test_funcfilter_more_criteria(self):\n        ff = func.rank().filter(table1.c.name > \"foo\")\n        ff2 = ff.filter(table1.c.myid == 1)\n        self.assert_compile(\n            select(ff, ff2),\n            \"SELECT rank() FILTER (WHERE mytable.name > :name_1) AS anon_1, \"\n            \"rank() FILTER (WHERE mytable.name > :name_1 AND \"\n            \"mytable.myid = :myid_1) AS anon_2 FROM mytable\",\n            {\"name_1\": \"foo\", \"myid_1\": 1},\n        )\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "test_funcfilter_within_group",
      "md_content": [
        "**test_funcfilter_within_group**: The function of test_funcfilter_within_group is to test the SQL compilation of a query that uses the `rank()` function with both `FILTER` and `WITHIN GROUP` clauses.\n\n**parameters**: The function does not take any external parameters. It operates within the context of the test class and uses the `self` parameter to access the class's methods and attributes.\n\n**Code Description**: \nThe function `test_funcfilter_within_group` is a test case that verifies the correct compilation of an SQL query. The query involves the `rank()` function, which is a window function in SQL. The `rank()` function is modified with two clauses:\n1. **FILTER**: This clause filters the rows to which the `rank()` function is applied. In this case, the filter condition is `table1.c.name > \"foo\"`, meaning the `rank()` function will only consider rows where the `name` column in `table1` is greater than the string \"foo\".\n2. **WITHIN GROUP**: This clause specifies the ordering of rows within the group for the `rank()` function. Here, the rows are ordered by the `name` column in `table1`.\n\nThe function uses the `assert_compile` method to compare the generated SQL query with the expected SQL string. The expected SQL string is:\n```sql\nSELECT rank() FILTER (WHERE mytable.name > :name_1) WITHIN GROUP (ORDER BY mytable.name) AS anon_1 FROM mytable\n```\nThis ensures that the SQLAlchemy query builder correctly translates the Python code into the appropriate SQL syntax.\n\n**Note**: \n- The function assumes the existence of a table named `mytable` with a column `name`.\n- The `assert_compile` method is used to validate the SQL compilation, which is a common practice in SQLAlchemy test cases.\n- The `:name_1` placeholder in the expected SQL string is a parameterized value that would be replaced with the actual value during execution."
      ],
      "code_start_line": 858,
      "code_end_line": 868,
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def test_funcfilter_within_group(self):\n        self.assert_compile(\n            select(\n                func.rank()\n                .filter(table1.c.name > \"foo\")\n                .within_group(table1.c.name)\n            ),\n            \"SELECT rank() FILTER (WHERE mytable.name > :name_1) \"\n            \"WITHIN GROUP (ORDER BY mytable.name) \"\n            \"AS anon_1 FROM mytable\",\n        )\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "test_within_group",
      "md_content": [
        "**test_within_group**: The function of test_within_group is to test the SQL compilation of a query that uses the `percentile_cont` function with the `WITHIN GROUP` clause.\n\n**parameters**: This function does not take any external parameters. It operates within the context of the class it belongs to, using the `self` reference to access class attributes and methods.\n\n**Code Description**: \nThe `test_within_group` function constructs a SQL query using SQLAlchemy's `select` statement. The query selects two columns: `myid` from `table1` and the result of the `percentile_cont` function applied to the `name` column of `table1`. The `percentile_cont` function is configured to calculate the median (50th percentile) using the `WITHIN GROUP` clause, which specifies the ordering of the data for the percentile calculation.\n\nThe `self.assert_compile` method is then used to verify that the constructed SQL statement matches the expected SQL string. The expected SQL string includes the `SELECT` statement with the `myid` column, the `percentile_cont` function with a placeholder for the percentile value, and the `WITHIN GROUP` clause ordering by the `name` column. The `assert_compile` method also checks that the placeholder value for the percentile is correctly set to `0.5`.\n\n**Note**: This test function is specifically designed to ensure that the SQLAlchemy query builder correctly compiles the `percentile_cont` function with the `WITHIN GROUP` clause into the appropriate SQL syntax. It is important to ensure that the table and column names used in the test match the actual database schema to avoid compilation errors."
      ],
      "code_start_line": 870,
      "code_end_line": 882,
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def test_within_group(self):\n        stmt = select(\n            table1.c.myid,\n            func.percentile_cont(0.5).within_group(table1.c.name),\n        )\n        self.assert_compile(\n            stmt,\n            \"SELECT mytable.myid, percentile_cont(:percentile_cont_1) \"\n            \"WITHIN GROUP (ORDER BY mytable.name) \"\n            \"AS anon_1 \"\n            \"FROM mytable\",\n            {\"percentile_cont_1\": 0.5},\n        )\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "test_within_group_multi",
      "md_content": [
        "**test_within_group_multi**: The function of test_within_group_multi is to test the SQL compilation of a query that uses the `percentile_cont` function with the `WITHIN GROUP` clause, ordering by multiple columns.\n\n**parameters**: This function does not take any parameters other than the implicit `self` parameter, which is a reference to the instance of the test class.\n\n**Code Description**: \nThe function `test_within_group_multi` constructs a SQL query using SQLAlchemy's `select` statement. The query selects two columns: `myid` from `table1` and the result of the `percentile_cont` function applied to the `name` and `description` columns of `table1`. The `percentile_cont` function is configured to calculate the median (50th percentile) by passing `0.5` as its argument. The `WITHIN GROUP` clause is used to specify the order in which the percentile calculation should be performed, ordering by both the `name` and `description` columns.\n\nThe function then uses `self.assert_compile` to verify that the constructed SQL statement is correctly compiled into the expected SQL string. The expected SQL string includes the `SELECT` statement with the `percentile_cont` function, the `WITHIN GROUP` clause, and the correct ordering of columns. The `assert_compile` method also checks that the parameters passed to the SQL function (in this case, `0.5` for the percentile) are correctly included in the compiled SQL.\n\n**Note**: \n- This test function is specifically designed to verify the correct compilation of SQL queries involving the `percentile_cont` function with multiple columns in the `WITHIN GROUP` clause.\n- The `assert_compile` method is a utility provided by SQLAlchemy's testing framework to ensure that the SQL statements generated by the ORM match the expected output.\n- The test assumes that `table1` is a predefined SQLAlchemy table object with columns `myid`, `name`, and `description`."
      ],
      "code_start_line": 884,
      "code_end_line": 898,
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def test_within_group_multi(self):\n        stmt = select(\n            table1.c.myid,\n            func.percentile_cont(0.5).within_group(\n                table1.c.name, table1.c.description\n            ),\n        )\n        self.assert_compile(\n            stmt,\n            \"SELECT mytable.myid, percentile_cont(:percentile_cont_1) \"\n            \"WITHIN GROUP (ORDER BY mytable.name, mytable.description) \"\n            \"AS anon_1 \"\n            \"FROM mytable\",\n            {\"percentile_cont_1\": 0.5},\n        )\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "test_within_group_desc",
      "md_content": [
        "**test_within_group_desc**: The function of test_within_group_desc is to test the SQL compilation of a query that uses the `percentile_cont` function with the `WITHIN GROUP` clause, specifically ordering the results in descending order.\n\n**parameters**: The parameters of this Function.\n· self: The instance of the test class, used to access assertion methods and other class attributes.\n\n**Code Description**: \nThe `test_within_group_desc` function constructs a SQL query using SQLAlchemy's `select` statement. The query selects two columns: `myid` from `table1` and the result of the `percentile_cont` function applied to the `name` column of `table1`. The `percentile_cont` function calculates the 50th percentile (median) of the `name` column, and the `WITHIN GROUP` clause specifies that the calculation should be performed with the `name` values ordered in descending order.\n\nThe `self.assert_compile` method is then used to verify that the constructed SQL query is correctly compiled into the expected SQL string. The expected SQL string includes the `SELECT` statement with the `myid` column, the `percentile_cont` function with a placeholder for the percentile value, and the `WITHIN GROUP` clause with the `ORDER BY` directive for the `name` column in descending order. The `assert_compile` method also checks that the placeholder value for the percentile is correctly set to 0.5.\n\n**Note**: \n- This test function is specifically designed to verify the correct compilation of SQL queries involving the `percentile_cont` function and the `WITHIN GROUP` clause with descending order.\n- Ensure that the `table1` object and its columns (`myid` and `name`) are properly defined in the SQLAlchemy model for this test to function correctly."
      ],
      "code_start_line": 900,
      "code_end_line": 912,
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def test_within_group_desc(self):\n        stmt = select(\n            table1.c.myid,\n            func.percentile_cont(0.5).within_group(table1.c.name.desc()),\n        )\n        self.assert_compile(\n            stmt,\n            \"SELECT mytable.myid, percentile_cont(:percentile_cont_1) \"\n            \"WITHIN GROUP (ORDER BY mytable.name DESC) \"\n            \"AS anon_1 \"\n            \"FROM mytable\",\n            {\"percentile_cont_1\": 0.5},\n        )\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "test_within_group_w_over",
      "md_content": [
        "**test_within_group_w_over**: The function of test_within_group_w_over is to test the SQL compilation of a query that uses the `percentile_cont` window function with a `WITHIN GROUP` clause and an `OVER` clause.\n\n**parameters**: This function does not take any parameters.\n\n**Code Description**: \nThe function constructs a SQL query using SQLAlchemy's `select` statement. The query selects two columns: `myid` from `table1` and a calculated column using the `percentile_cont` function. The `percentile_cont` function is configured to calculate the 50th percentile (median) of the `name` column in descending order, grouped within the `description` column. This is achieved using the `within_group` method with `table1.c.name.desc()` and the `over` method with `partition_by=table1.c.description`.\n\nThe `assert_compile` method is then used to verify that the constructed SQL statement matches the expected SQL string. The expected SQL string includes the `percentile_cont` function with a placeholder for the percentile value, the `WITHIN GROUP` clause specifying the ordering of the `name` column, and the `OVER` clause partitioning the data by the `description` column. The `assert_compile` method also checks that the placeholder value for `percentile_cont` is correctly set to 0.5.\n\n**Note**: This test function is specifically designed to ensure that the SQLAlchemy query builder correctly compiles complex SQL statements involving window functions and grouping. It is important to verify that the SQL syntax generated by SQLAlchemy matches the expected output, especially when using advanced SQL features like window functions."
      ],
      "code_start_line": 914,
      "code_end_line": 928,
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def test_within_group_w_over(self):\n        stmt = select(\n            table1.c.myid,\n            func.percentile_cont(0.5)\n            .within_group(table1.c.name.desc())\n            .over(partition_by=table1.c.description),\n        )\n        self.assert_compile(\n            stmt,\n            \"SELECT mytable.myid, percentile_cont(:percentile_cont_1) \"\n            \"WITHIN GROUP (ORDER BY mytable.name DESC) \"\n            \"OVER (PARTITION BY mytable.description) AS anon_1 \"\n            \"FROM mytable\",\n            {\"percentile_cont_1\": 0.5},\n        )\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "test_within_group_filter",
      "md_content": [
        "**test_within_group_filter**: The function of test_within_group_filter is to test the SQL compilation of a query that uses the `percentile_cont` function with a `WITHIN GROUP` clause and a `FILTER` condition.\n\n**parameters**: The parameters of this Function.\n· self: The instance of the test class, used to access assertion methods and other class attributes.\n\n**Code Description**: The description of this Function.\nThe `test_within_group_filter` function constructs a SQL query using SQLAlchemy's `select` statement. The query selects the `myid` column from `table1` and applies the `percentile_cont` function with a percentile value of 0.5. The `percentile_cont` function is used with the `WITHIN GROUP` clause, which orders the results by the `name` column of `table1`. Additionally, a `FILTER` condition is applied to the `percentile_cont` function, specifying that only rows where `myid` is greater than 42 should be considered.\n\nThe function then uses the `assert_compile` method to verify that the constructed SQL statement is correctly compiled into the expected SQL string. The expected SQL string includes the `percentile_cont` function with the `WITHIN GROUP` and `FILTER` clauses, and it also checks that the correct parameters (`percentile_cont_1` and `myid_1`) are passed to the query.\n\n**Note**: Points to note about the use of the code\n- This test function is specifically designed to verify the correct compilation of SQL queries that use advanced SQL features like `WITHIN GROUP` and `FILTER` clauses.\n- The `assert_compile` method is crucial for ensuring that the SQLAlchemy query is translated into the correct SQL syntax.\n- The test assumes that `table1` is a predefined SQLAlchemy table object with columns `myid` and `name`."
      ],
      "code_start_line": 930,
      "code_end_line": 945,
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def test_within_group_filter(self):\n        stmt = select(\n            table1.c.myid,\n            func.percentile_cont(0.5)\n            .within_group(table1.c.name)\n            .filter(table1.c.myid > 42),\n        )\n        self.assert_compile(\n            stmt,\n            \"SELECT mytable.myid, percentile_cont(:percentile_cont_1) \"\n            \"WITHIN GROUP (ORDER BY mytable.name) \"\n            \"FILTER (WHERE mytable.myid > :myid_1) \"\n            \"AS anon_1 \"\n            \"FROM mytable\",\n            {\"percentile_cont_1\": 0.5, \"myid_1\": 42},\n        )\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "test_incorrect_none_type",
      "md_content": [
        "**test_incorrect_none_type**: The function of test_incorrect_none_type is to verify that an error is raised when a SQLAlchemy FunctionElement is defined with an incorrect type attribute set to None.\n\n**parameters**: The parameters of this Function.\n· self: The instance of the test class. This parameter is implicitly passed when the method is called.\n\n**Code Description**: The description of this Function.\nThe `test_incorrect_none_type` function is a test case designed to ensure that a specific error is raised when a custom SQLAlchemy `FunctionElement` is improperly defined with a `type` attribute set to `None`. \n\n1. The function begins by importing the `FunctionElement` class from `sqlalchemy.sql.expression`. This class is used to define custom SQL functions in SQLAlchemy.\n\n2. A custom class `MissingType` is then defined, which inherits from `FunctionElement`. This class has two attributes:\n   - `name`: A string attribute set to `\"mt\"`, which represents the name of the function.\n   - `type`: An attribute set to `None`, which is intentionally incorrect to trigger the test case.\n\n3. The function uses the `assert_raises_message` utility to verify that a `TypeError` is raised with a specific error message when attempting to create a column using the `MissingType` class. The error message expected is: \"Object None associated with '.type' attribute is not a TypeEngine class or object\".\n\n4. The test case checks the behavior of the expression `column(\"x\", MissingType()) == 5`. This expression attempts to create a column with the `MissingType` function, which should fail due to the incorrect `type` attribute.\n\n**Note**: Points to note about the use of the code.\n- This test case is specifically designed to validate the error handling mechanism in SQLAlchemy when a `FunctionElement` is improperly defined. It ensures that the framework correctly identifies and raises an error when the `type` attribute is not a valid `TypeEngine` class or object.\n- The `assert_raises_message` function is used to assert both the type of the exception and the specific error message, making the test case more precise and reliable."
      ],
      "code_start_line": 947,
      "code_end_line": 959,
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def test_incorrect_none_type(self):\n        from sqlalchemy.sql.expression import FunctionElement\n\n        class MissingType(FunctionElement):\n            name = \"mt\"\n            type = None\n\n        assert_raises_message(\n            TypeError,\n            \"Object None associated with '.type' attribute is \"\n            \"not a TypeEngine class or object\",\n            lambda: column(\"x\", MissingType()) == 5,\n        )\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "ClassDef",
      "name": "MissingType",
      "md_content": [
        "**MissingType**: The function of MissingType is to represent a function element with a missing or undefined type.\n\n**attributes**: The attributes of this Class.\n· name: A string attribute set to \"mt\", which likely serves as an identifier or name for this function element.\n· type: An attribute set to `None`, indicating that the type of this function element is undefined or missing.\n\n**Code Description**: The `MissingType` class is a subclass of `FunctionElement`, which suggests it is part of a larger framework or system that deals with function elements. The class defines two attributes: `name` and `type`. The `name` attribute is a string set to \"mt\", which could be used to identify this specific function element within the system. The `type` attribute is set to `None`, indicating that the type of this function element is not specified or is intentionally left undefined. This could be useful in scenarios where the type of a function element is unknown or irrelevant, or where it needs to be dynamically determined at runtime.\n\n**Note**: When using the `MissingType` class, ensure that the absence of a type is intentional and aligns with the requirements of your application. The `name` attribute should be unique if it is used as an identifier within the system."
      ],
      "code_start_line": 950,
      "code_end_line": 952,
      "params": [],
      "have_return": false,
      "code_content": "        class MissingType(FunctionElement):\n            name = \"mt\"\n            type = None\n",
      "name_column": 14,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "test_as_comparison",
      "md_content": [
        "**test_as_comparison**: The function of test_as_comparison is to test the behavior of the `as_comparison` method when applied to a substring function, ensuring that the resulting object has the correct type affinity and compiles as expected.\n\n**parameters**: The parameters of this Function.\n· self: The instance of the test class, used to access assertion methods and other test utilities.\n\n**Code Description**: The description of this Function.\nThe `test_as_comparison` function begins by creating a substring function using `func.substring(\"foo\", \"foobar\")` and then applies the `as_comparison(1, 2)` method to it. This method is used to treat the substring function as a comparison operation, which is expected to return a Boolean result. The function then verifies that the type affinity of the resulting object is indeed Boolean using `is_(fn.type._type_affinity, Boolean)`.\n\nNext, the function performs a series of assertions to ensure that the left and right operands of the comparison are compiled correctly. Specifically, it checks that the left operand compiles to `:substring_1` with the parameter `\"foo\"`, and the right operand compiles to `:substring_1` with the parameter `\"foobar\"`. These checks are done using the `assert_compile` method, which verifies both the SQL string and the parameters.\n\nFinally, the function asserts that the entire comparison expression compiles to `\"substring(:substring_1, :substring_2)\"` with the parameters `{\"substring_1\": \"foo\", \"substring_2\": \"foobar\"}`. This ensures that the `as_comparison` method correctly transforms the substring function into a comparison operation that can be used in SQL queries.\n\n**Note**: Points to note about the use of the code.\n- The `as_comparison` method is used to treat a function as a comparison operation, which is particularly useful when constructing complex SQL expressions.\n- The `assert_compile` method is crucial for verifying both the SQL string and the parameters, ensuring that the generated SQL is correct and safe to use.\n- This test is essential for validating the behavior of the `as_comparison` method in the context of substring operations, ensuring that it produces the expected results when used in real-world scenarios."
      ],
      "code_start_line": 961,
      "code_end_line": 976,
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def test_as_comparison(self):\n        fn = func.substring(\"foo\", \"foobar\").as_comparison(1, 2)\n        is_(fn.type._type_affinity, Boolean)\n\n        self.assert_compile(\n            fn.left, \":substring_1\", checkparams={\"substring_1\": \"foo\"}\n        )\n        self.assert_compile(\n            fn.right, \":substring_1\", checkparams={\"substring_1\": \"foobar\"}\n        )\n\n        self.assert_compile(\n            fn,\n            \"substring(:substring_1, :substring_2)\",\n            checkparams={\"substring_1\": \"foo\", \"substring_2\": \"foobar\"},\n        )\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "test_as_comparison_annotate",
      "md_content": [
        "**test_as_comparison_annotate**: The function of test_as_comparison_annotate is to test the annotation functionality applied to a comparison function created using the `as_comparison` method.\n\n**parameters**: This function does not take any parameters.\n\n**Code Description**: \nThe function begins by creating a comparison function `fn` using the `func.foobar` method with arguments \"x\", \"y\", \"q\", \"p\", and \"r\". The `as_comparison` method is then applied to this function, specifying the indices 2 and 5 as the comparison points. This creates a comparison function where the elements at these indices are compared.\n\nNext, the function imports the `annotation` module from `sqlalchemy.sql`. The `_deep_annotate` method from this module is used to apply an annotation to the comparison function `fn`. The annotation is a dictionary with the key \"token\" and the value \"yes\". This results in a new annotated function `fn_annotated`.\n\nThe function then performs two assertions using the `eq_` method:\n1. It checks that the `_annotations` attribute of the `left` attribute of the original function `fn` is an empty dictionary, confirming that no annotations were applied to it.\n2. It verifies that the `_annotations` attribute of the `left` attribute of the annotated function `fn_annotated` contains the annotation {\"token\": \"yes\"}, confirming that the annotation was successfully applied.\n\n**Note**: This function is primarily used for testing purposes to ensure that annotations are correctly applied to comparison functions. It relies on the `sqlalchemy.sql.annotation` module, which is part of the SQLAlchemy library, and assumes familiarity with SQLAlchemy's annotation system."
      ],
      "code_start_line": 978,
      "code_end_line": 986,
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def test_as_comparison_annotate(self):\n        fn = func.foobar(\"x\", \"y\", \"q\", \"p\", \"r\").as_comparison(2, 5)\n\n        from sqlalchemy.sql import annotation\n\n        fn_annotated = annotation._deep_annotate(fn, {\"token\": \"yes\"})\n\n        eq_(fn.left._annotations, {})\n        eq_(fn_annotated.left._annotations, {\"token\": \"yes\"})\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "test_as_comparison_many_argument",
      "md_content": [
        "**test_as_comparison_many_argument**: The function of test_as_comparison_many_argument is to test the behavior of a comparison function with multiple arguments, ensuring that the generated SQL expressions and parameters are correctly compiled and validated.\n\n**parameters**: The parameters of this Function.\n· self: The instance of the test class, used to access class methods and attributes.\n\n**Code Description**: \nThe `test_as_comparison_many_argument` function is designed to verify the functionality of a comparison operation that involves multiple arguments. The function begins by creating a comparison function `fn` using `func.some_comparison` with six arguments (\"x\", \"y\", \"z\", \"p\", \"q\", \"r\"). The `as_comparison` method is then applied to this function, specifying the indices 2 and 5 to define the comparison behavior.\n\nThe function checks the type affinity of the generated comparison function to ensure it is of type `Boolean`. Following this, the function uses `assert_compile` to validate the SQL compilation of the left and right sides of the comparison. The left side is expected to compile to a parameterized SQL expression with the parameter \"y\", while the right side is expected to compile to a parameterized SQL expression with the parameter \"q\".\n\nNext, the function creates a cloned version of the comparison function using `visitors.cloned_traverse` and modifies the right side of the cloned function to a literal column \"ABC\". This modification is used to test the flexibility of the comparison function in handling different types of SQL expressions.\n\nFinally, the function performs two additional `assert_compile` checks. The first ensures that the original comparison function compiles correctly with all six parameters. The second ensures that the modified cloned function compiles correctly, with the right side replaced by the literal \"ABC\" and the remaining parameters intact.\n\n**Note**: \n- The function relies on the `assert_compile` method to validate SQL expressions, ensuring that the generated SQL matches the expected output and parameters.\n- The use of `visitors.cloned_traverse` demonstrates the ability to clone and modify SQL expressions, which is useful for testing dynamic SQL generation.\n- The function assumes familiarity with SQLAlchemy's expression compilation and parameter handling mechanisms."
      ],
      "code_start_line": 988,
      "code_end_line": 1037,
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def test_as_comparison_many_argument(self):\n        fn = func.some_comparison(\"x\", \"y\", \"z\", \"p\", \"q\", \"r\").as_comparison(\n            2, 5\n        )\n        is_(fn.type._type_affinity, Boolean)\n\n        self.assert_compile(\n            fn.left,\n            \":some_comparison_1\",\n            checkparams={\"some_comparison_1\": \"y\"},\n        )\n        self.assert_compile(\n            fn.right,\n            \":some_comparison_1\",\n            checkparams={\"some_comparison_1\": \"q\"},\n        )\n\n        from sqlalchemy.sql import visitors\n\n        fn_2 = visitors.cloned_traverse(fn, {}, {})\n        fn_2.right = literal_column(\"ABC\")\n\n        self.assert_compile(\n            fn,\n            \"some_comparison(:some_comparison_1, :some_comparison_2, \"\n            \":some_comparison_3, \"\n            \":some_comparison_4, :some_comparison_5, :some_comparison_6)\",\n            checkparams={\n                \"some_comparison_1\": \"x\",\n                \"some_comparison_2\": \"y\",\n                \"some_comparison_3\": \"z\",\n                \"some_comparison_4\": \"p\",\n                \"some_comparison_5\": \"q\",\n                \"some_comparison_6\": \"r\",\n            },\n        )\n\n        self.assert_compile(\n            fn_2,\n            \"some_comparison(:some_comparison_1, :some_comparison_2, \"\n            \":some_comparison_3, \"\n            \":some_comparison_4, ABC, :some_comparison_5)\",\n            checkparams={\n                \"some_comparison_1\": \"x\",\n                \"some_comparison_2\": \"y\",\n                \"some_comparison_3\": \"z\",\n                \"some_comparison_4\": \"p\",\n                \"some_comparison_5\": \"r\",\n            },\n        )\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "ClassDef",
      "name": "ReturnTypeTest",
      "md_content": [
        "**ReturnTypeTest**: The function of ReturnTypeTest is to validate the return types of various SQL functions and expressions, ensuring they match the expected data types and structures.\n\n**attributes**: The attributes of this Class.\n· Inherits from `AssertsCompiledSQL` and `fixtures.TestBase`: This class inherits functionality for asserting SQL compilation and test base utilities.\n\n**Code Description**: The description of this Class.\nThe `ReturnTypeTest` class is a test suite designed to verify the correctness of return types for SQL functions and expressions, particularly focusing on array aggregation and percentile-related functions. It ensures that the types returned by these functions align with the expected SQL data types, such as `ARRAY`, `Integer`, and `Numeric`.\n\n1. **test_array_agg**: This method tests the `array_agg` function with an integer column. It verifies that the return type is an `ARRAY` and that the item type within the array is `Integer`. It also checks that the array has one dimension.\n\n2. **test_array_agg_array_datatype**: This method tests the `array_agg` function with an array column. It ensures that the return type is an `ARRAY` and that the item type and dimensions match the input array column.\n\n3. **test_array_agg_array_literal_implicit_type**: This method tests the `array_agg` function with an implicitly typed array literal. It verifies that the return type is a PostgreSQL-specific `ARRAY` (`PG_ARRAY`) and that the item type is `Integer`. It also compiles the expression to ensure the SQL output is correct.\n\n4. **test_array_agg_array_literal_explicit_type**: This method tests the `array_agg` function with an explicitly typed array literal. It ensures that the return type is an `ARRAY` and that the item type is `Integer`. It also compiles the expression to validate the SQL output.\n\n5. **test_mode**: This method tests the `mode` function with a descending integer column. It verifies that the return type is `Integer`.\n\n6. **test_percentile_cont**: This method tests the `percentile_cont` function with an integer column. It ensures that the return type is `Integer`.\n\n7. **test_percentile_cont_array**: This method tests the `percentile_cont` function with multiple percentiles and an integer column. It verifies that the return type is an `ARRAY` and that the item type is `Integer`.\n\n8. **test_percentile_cont_array_desc**: This method tests the `percentile_cont` function with multiple percentiles and a descending integer column. It ensures that the return type is an `ARRAY` and that the item type is `Integer`.\n\n9. **test_cume_dist**: This method tests the `cume_dist` function with a descending integer column. It verifies that the return type is `Numeric`.\n\n10. **test_percent_rank**: This method tests the `percent_rank` function with an integer column. It ensures that the return type is `Numeric`.\n\n**Note**: Points to note about the use of the code\n- The tests rely on SQLAlchemy's type system and PostgreSQL-specific features, such as `PG_ARRAY`. Ensure the correct dialect is used when running these tests.\n- The `assert_compile` method is used to validate the SQL output, so the tests are tightly coupled with the SQL generation logic.\n- The `within_group` clause is used in percentile and ranking functions, which is a PostgreSQL-specific feature. Ensure compatibility with other databases if used outside PostgreSQL."
      ],
      "code_start_line": 1040,
      "code_end_line": 1109,
      "params": [],
      "have_return": false,
      "code_content": "class ReturnTypeTest(AssertsCompiledSQL, fixtures.TestBase):\n    def test_array_agg(self):\n        expr = func.array_agg(column(\"data\", Integer))\n        is_(expr.type._type_affinity, ARRAY)\n        is_(expr.type.item_type._type_affinity, Integer)\n        is_(expr.type.dimensions, 1)\n\n    def test_array_agg_array_datatype(self):\n        col = column(\"data\", ARRAY(Integer))\n        expr = func.array_agg(col)\n        is_(expr.type._type_affinity, ARRAY)\n        is_(expr.type.item_type._type_affinity, Integer)\n        eq_(expr.type.dimensions, col.type.dimensions)\n\n    def test_array_agg_array_literal_implicit_type(self):\n        expr = array([column(\"data\", Integer), column(\"d2\", Integer)])\n\n        assert isinstance(expr.type, PG_ARRAY)\n\n        agg_expr = func.array_agg(expr)\n        assert isinstance(agg_expr.type, PG_ARRAY)\n        is_(agg_expr.type._type_affinity, ARRAY)\n        is_(agg_expr.type.item_type._type_affinity, Integer)\n\n        self.assert_compile(\n            agg_expr, \"array_agg(ARRAY[data, d2])\", dialect=\"postgresql\"\n        )\n\n    def test_array_agg_array_literal_explicit_type(self):\n        from sqlalchemy.dialects.postgresql import array\n\n        expr = array([column(\"data\", Integer), column(\"d2\", Integer)])\n\n        agg_expr = func.array_agg(expr, type_=ARRAY(Integer))\n        is_(agg_expr.type._type_affinity, ARRAY)\n        is_(agg_expr.type.item_type._type_affinity, Integer)\n\n        self.assert_compile(\n            agg_expr, \"array_agg(ARRAY[data, d2])\", dialect=\"postgresql\"\n        )\n\n    def test_mode(self):\n        expr = func.mode(0.5).within_group(column(\"data\", Integer).desc())\n        is_(expr.type._type_affinity, Integer)\n\n    def test_percentile_cont(self):\n        expr = func.percentile_cont(0.5).within_group(column(\"data\", Integer))\n        is_(expr.type._type_affinity, Integer)\n\n    def test_percentile_cont_array(self):\n        expr = func.percentile_cont(0.5, 0.7).within_group(\n            column(\"data\", Integer)\n        )\n        is_(expr.type._type_affinity, ARRAY)\n        is_(expr.type.item_type._type_affinity, Integer)\n\n    def test_percentile_cont_array_desc(self):\n        expr = func.percentile_cont(0.5, 0.7).within_group(\n            column(\"data\", Integer).desc()\n        )\n        is_(expr.type._type_affinity, ARRAY)\n        is_(expr.type.item_type._type_affinity, Integer)\n\n    def test_cume_dist(self):\n        expr = func.cume_dist(0.5).within_group(column(\"data\", Integer).desc())\n        is_(expr.type._type_affinity, Numeric)\n\n    def test_percent_rank(self):\n        expr = func.percent_rank(0.5).within_group(column(\"data\", Integer))\n        is_(expr.type._type_affinity, Numeric)\n",
      "name_column": 6,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "test_array_agg",
      "md_content": [
        "**test_array_agg**: The function of test_array_agg is to test the functionality and type properties of the `array_agg` SQL function when applied to an integer column.\n\n**parameters**: This function does not take any external parameters. It operates within the context of the test class and uses internally defined expressions and assertions.\n\n**Code Description**: \nThe `test_array_agg` function performs the following steps:\n1. It creates an SQL expression using the `array_agg` function, which aggregates values from a column named \"data\" of type `Integer`. This is done using `func.array_agg(column(\"data\", Integer))`.\n2. It then checks the type affinity of the resulting expression using `is_(expr.type._type_affinity, ARRAY)`. This assertion ensures that the type of the expression is an array.\n3. Next, it verifies the type affinity of the array's item type using `is_(expr.type.item_type._type_affinity, Integer)`. This ensures that the items within the array are of type `Integer`.\n4. Finally, it checks the dimensionality of the array using `is_(expr.type.dimensions, 1)`. This assertion confirms that the array is one-dimensional.\n\n**Note**: This function is designed to validate the behavior of the `array_agg` function in SQL, specifically focusing on type properties and dimensionality. It is part of a test suite and should be used in conjunction with other tests to ensure the correctness of SQL function implementations."
      ],
      "code_start_line": 1041,
      "code_end_line": 1045,
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def test_array_agg(self):\n        expr = func.array_agg(column(\"data\", Integer))\n        is_(expr.type._type_affinity, ARRAY)\n        is_(expr.type.item_type._type_affinity, Integer)\n        is_(expr.type.dimensions, 1)\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "test_array_agg_array_datatype",
      "md_content": [
        "**test_array_agg_array_datatype**: The function of test_array_agg_array_datatype is to verify the behavior and type affinity of the `array_agg` function when applied to a column of array datatype with integer elements.\n\n**parameters**: This function does not take any external parameters. It operates within the context of the test class and uses internally defined variables.\n\n**Code Description**: \nThe function begins by defining a column named \"data\" with a datatype of `ARRAY(Integer)`. This column is intended to represent an array of integers. The `array_agg` function is then applied to this column, which aggregates the elements of the array into a single array. \n\nThe function proceeds to perform two type checks using the `is_` assertion:\n1. It verifies that the type affinity of the resulting expression (`expr.type._type_affinity`) is `ARRAY`, ensuring that the aggregation operation correctly maintains the array datatype.\n2. It checks that the type affinity of the array's item type (`expr.type.item_type._type_affinity`) is `Integer`, confirming that the elements within the array are of the expected integer type.\n\nFinally, the function uses the `eq_` assertion to compare the dimensions of the resulting array type (`expr.type.dimensions`) with the dimensions of the original column type (`col.type.dimensions`). This ensures that the aggregation operation preserves the dimensionality of the array.\n\n**Note**: This test function is specifically designed to validate the behavior of the `array_agg` function when working with array datatypes. It assumes familiarity with SQLAlchemy's type system and the `array_agg` function. Ensure that the necessary imports and dependencies are correctly set up before running this test."
      ],
      "code_start_line": 1047,
      "code_end_line": 1052,
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def test_array_agg_array_datatype(self):\n        col = column(\"data\", ARRAY(Integer))\n        expr = func.array_agg(col)\n        is_(expr.type._type_affinity, ARRAY)\n        is_(expr.type.item_type._type_affinity, Integer)\n        eq_(expr.type.dimensions, col.type.dimensions)\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "test_array_agg_array_literal_implicit_type",
      "md_content": [
        "**test_array_agg_array_literal_implicit_type**: The function of test_array_agg_array_literal_implicit_type is to test the behavior of the `array_agg` function when applied to an array literal with implicitly defined types, ensuring the resulting expression has the correct type and compiles to the expected SQL statement in PostgreSQL.\n\n**parameters**: This function does not take any parameters. It is a test method within a class, and it operates on the instance attributes and methods of the class.\n\n**Code Description**: \n1. The function begins by creating an array expression using the `array` function, which combines two columns, `data` and `d2`, both of type `Integer`. This array expression is stored in the variable `expr`.\n2. The function then asserts that the type of `expr` is `PG_ARRAY`, confirming that the expression is recognized as a PostgreSQL array type.\n3. Next, the function applies the `array_agg` function to the array expression `expr`, resulting in an aggregate expression `agg_expr`. The function asserts that the type of `agg_expr` is also `PG_ARRAY`.\n4. The function further verifies that the type affinity of `agg_expr` is `ARRAY` and that the item type within the array has an affinity of `Integer`, ensuring the correct type hierarchy is maintained.\n5. Finally, the function compiles the `agg_expr` into a SQL statement using the PostgreSQL dialect and asserts that the compiled SQL matches the expected output: `\"array_agg(ARRAY[data, d2])\"`.\n\n**Note**: This test function is specifically designed to validate the behavior of PostgreSQL's `array_agg` function when used with array literals. It ensures that the type system correctly interprets the array and its elements, and that the SQL generation aligns with PostgreSQL's syntax. This test is crucial for verifying the correctness of type inference and SQL compilation in the context of array aggregation."
      ],
      "code_start_line": 1054,
      "code_end_line": 1066,
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def test_array_agg_array_literal_implicit_type(self):\n        expr = array([column(\"data\", Integer), column(\"d2\", Integer)])\n\n        assert isinstance(expr.type, PG_ARRAY)\n\n        agg_expr = func.array_agg(expr)\n        assert isinstance(agg_expr.type, PG_ARRAY)\n        is_(agg_expr.type._type_affinity, ARRAY)\n        is_(agg_expr.type.item_type._type_affinity, Integer)\n\n        self.assert_compile(\n            agg_expr, \"array_agg(ARRAY[data, d2])\", dialect=\"postgresql\"\n        )\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "test_array_agg_array_literal_explicit_type",
      "md_content": [
        "**test_array_agg_array_literal_explicit_type**: The function of test_array_agg_array_literal_explicit_type is to test the functionality of the `array_agg` SQL function when used with an explicitly typed array literal in a PostgreSQL dialect.\n\n**parameters**: This function does not take any parameters as it is a test method within a class.\n\n**Code Description**: \nThe function begins by importing the `array` function from the `sqlalchemy.dialects.postgresql` module, which is used to create a PostgreSQL array literal. It then constructs an array literal using the `array` function, where the array contains two columns named \"data\" and \"d2\", both explicitly typed as `Integer`.\n\nNext, the function creates an aggregate expression using the `array_agg` function from SQLAlchemy's `func` module. The `array_agg` function is used to aggregate the array literal into a single array. The `type_` parameter is explicitly set to `ARRAY(Integer)`, ensuring that the resulting array has an explicit type of `ARRAY` with `Integer` as its item type.\n\nThe function then performs two assertions using the `is_` function to verify that the type affinity of the aggregate expression is `ARRAY` and that the item type affinity is `Integer`. These assertions ensure that the type information is correctly propagated through the SQL expression.\n\nFinally, the function uses `self.assert_compile` to verify that the SQL expression generated by the aggregate expression matches the expected SQL string `\"array_agg(ARRAY[data, d2])\"` when using the PostgreSQL dialect. This ensures that the SQLAlchemy expression is correctly translated into the appropriate SQL syntax for PostgreSQL.\n\n**Note**: This test function is specifically designed to work with the PostgreSQL dialect. If used with other SQL dialects, the SQL syntax or type handling may differ, potentially leading to errors or unexpected behavior. Ensure that the correct dialect is used when running this test."
      ],
      "code_start_line": 1068,
      "code_end_line": 1079,
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def test_array_agg_array_literal_explicit_type(self):\n        from sqlalchemy.dialects.postgresql import array\n\n        expr = array([column(\"data\", Integer), column(\"d2\", Integer)])\n\n        agg_expr = func.array_agg(expr, type_=ARRAY(Integer))\n        is_(agg_expr.type._type_affinity, ARRAY)\n        is_(agg_expr.type.item_type._type_affinity, Integer)\n\n        self.assert_compile(\n            agg_expr, \"array_agg(ARRAY[data, d2])\", dialect=\"postgresql\"\n        )\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "test_mode",
      "md_content": [
        "**test_mode**: The function of test_mode is to verify the behavior and type affinity of the `mode` function when used within a SQL expression.\n\n**parameters**: This function does not take any parameters.\n\n**Code Description**: \nThe `test_mode` function is designed to test the functionality of the `mode` function, which is typically used in SQL queries to calculate the mode (the most frequently occurring value) of a dataset. In this test, the `mode` function is applied with a value of `0.5`, and it is used within a `within_group` clause. The `within_group` clause specifies that the mode should be calculated within a group of data sorted in descending order based on the `data` column, which is of type `Integer`.\n\nThe expression `func.mode(0.5).within_group(column(\"data\", Integer).desc())` constructs a SQL expression that calculates the mode of the `data` column, sorted in descending order. The `is_` function is then used to assert that the type affinity of the resulting expression is `Integer`. This ensures that the `mode` function returns a value of the expected type.\n\n**Note**: \n- This test assumes that the `mode` function and the `within_group` clause are correctly implemented and that the `data` column is of type `Integer`.\n- The test does not validate the actual calculation of the mode but rather focuses on the type affinity of the resulting expression."
      ],
      "code_start_line": 1081,
      "code_end_line": 1083,
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def test_mode(self):\n        expr = func.mode(0.5).within_group(column(\"data\", Integer).desc())\n        is_(expr.type._type_affinity, Integer)\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "test_percentile_cont",
      "md_content": [
        "**test_percentile_cont**: The function of test_percentile_cont is to test the functionality of the `percentile_cont` function, specifically verifying that the type affinity of the resulting expression is correctly identified as `Integer`.\n\n**parameters**: This function does not take any parameters.\n\n**Code Description**: \nThe function `test_percentile_cont` performs a test on the `percentile_cont` function, which is a continuous percentile calculation function. The test constructs an expression using `func.percentile_cont(0.5)`, which calculates the median (50th percentile) of a dataset. The `within_group` method is then used to specify the column on which the percentile calculation should be performed, in this case, a column named \"data\" of type `Integer`. \n\nThe `is_` function is used to assert that the type affinity of the resulting expression (`expr.type._type_affinity`) is `Integer`. This ensures that the `percentile_cont` function correctly maintains the integer type affinity when performing the percentile calculation on an integer column.\n\n**Note**: This test is specifically designed to verify the type affinity of the result produced by the `percentile_cont` function. It assumes that the input column \"data\" is of type `Integer` and that the `percentile_cont` function should preserve this type affinity in its output."
      ],
      "code_start_line": 1085,
      "code_end_line": 1087,
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def test_percentile_cont(self):\n        expr = func.percentile_cont(0.5).within_group(column(\"data\", Integer))\n        is_(expr.type._type_affinity, Integer)\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "test_percentile_cont_array",
      "md_content": [
        "**test_percentile_cont_array**: The function of test_percentile_cont_array is to test the behavior and type affinity of the `percentile_cont` function when applied to an array of integers.\n\n**parameters**: This function does not take any explicit parameters. It operates within the context of the test class and uses predefined expressions and column references.\n\n**Code Description**: \nThe function begins by creating an expression using the `percentile_cont` function, which calculates the continuous percentile for the given values (0.5 and 0.7 in this case). The `within_group` method is then applied to this expression, specifying that the calculation should be performed within the group defined by the `data` column of type `Integer`. \n\nThe function then performs two assertions using the `is_` function:\n1. It checks that the type affinity of the resulting expression is `ARRAY`, confirming that the `percentile_cont` function returns an array.\n2. It verifies that the item type within the array has a type affinity of `Integer`, ensuring that the elements of the array are of the expected integer type.\n\n**Note**: This test function is specifically designed to validate the type behavior of the `percentile_cont` function when used with an array of integers. It assumes that the `data` column and the `percentile_cont` function are correctly implemented and available in the context where this test is run."
      ],
      "code_start_line": 1089,
      "code_end_line": 1094,
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def test_percentile_cont_array(self):\n        expr = func.percentile_cont(0.5, 0.7).within_group(\n            column(\"data\", Integer)\n        )\n        is_(expr.type._type_affinity, ARRAY)\n        is_(expr.type.item_type._type_affinity, Integer)\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "test_percentile_cont_array_desc",
      "md_content": [
        "**test_percentile_cont_array_desc**: The function of test_percentile_cont_array_desc is to test the behavior of the `percentile_cont` function when applied to an array of integers in descending order, ensuring the resulting expression has the correct type affinity.\n\n**parameters**: This function does not take any parameters. It operates within the context of the test case class it belongs to.\n\n**Code Description**: \nThe function begins by constructing an expression using the `percentile_cont` function, which calculates the continuous percentile for the given percentiles (0.5 and 0.7 in this case). The `within_group` method is then used to specify that the calculation should be performed on the `data` column of type `Integer`, sorted in descending order using `column(\"data\", Integer).desc()`.\n\nThe function then performs two assertions using the `is_` function:\n1. It checks that the type affinity of the resulting expression (`expr.type._type_affinity`) is `ARRAY`, confirming that the result is an array type.\n2. It verifies that the type affinity of the array's item type (`expr.type.item_type._type_affinity`) is `Integer`, ensuring that the elements within the array are of integer type.\n\nThese assertions validate that the `percentile_cont` function, when applied to a descending array of integers, produces an array of integers as expected.\n\n**Note**: This test function is specifically designed to verify the type affinity of the result when using the `percentile_cont` function with a descending sort order. It assumes the existence of a `data` column of type `Integer` in the context where the function is used. Ensure that the necessary database schema and data are set up correctly for this test to pass."
      ],
      "code_start_line": 1096,
      "code_end_line": 1101,
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def test_percentile_cont_array_desc(self):\n        expr = func.percentile_cont(0.5, 0.7).within_group(\n            column(\"data\", Integer).desc()\n        )\n        is_(expr.type._type_affinity, ARRAY)\n        is_(expr.type.item_type._type_affinity, Integer)\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "test_cume_dist",
      "md_content": [
        "**test_cume_dist**: The function of test_cume_dist is to test the behavior and type affinity of the `cume_dist` window function when applied to a descending ordered column of integer data.\n\n**parameters**: This function does not take any parameters.\n\n**Code Description**: \nThe `test_cume_dist` function is designed to verify the functionality of the `cume_dist` window function in a specific context. The function constructs an expression using `func.cume_dist(0.5)`, which calculates the cumulative distribution of the value `0.5` within a group. The `within_group` method is then used to specify the ordering of the group, where the column named \"data\" of type `Integer` is sorted in descending order using `column(\"data\", Integer).desc()`. \n\nThe function then checks the type affinity of the resulting expression using `is_(expr.type._type_affinity, Numeric)`. This assertion ensures that the type affinity of the expression is `Numeric`, which is expected for the result of a cumulative distribution calculation.\n\n**Note**: This test function is specifically designed to validate the type affinity of the `cume_dist` window function when applied to a descending ordered integer column. It does not test the actual computation of the cumulative distribution but rather ensures that the resulting expression has the correct type affinity."
      ],
      "code_start_line": 1103,
      "code_end_line": 1105,
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def test_cume_dist(self):\n        expr = func.cume_dist(0.5).within_group(column(\"data\", Integer).desc())\n        is_(expr.type._type_affinity, Numeric)\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "test_percent_rank",
      "md_content": [
        "**test_percent_rank**: The function of test_percent_rank is to test the behavior and type affinity of the `percent_rank` function when applied to a specific value within a group of integer data.\n\n**parameters**: This function does not take any parameters directly. It operates on the instance of the class it belongs to.\n\n**Code Description**: \nThe `test_percent_rank` function constructs an expression using the `percent_rank` function from the `func` module. The `percent_rank` function is applied to the value `0.5` and is configured to operate within a group defined by the `data` column, which is expected to contain integer values. The `within_group` method specifies that the `percent_rank` calculation should be performed within the context of the `data` column. \n\nThe function then checks the type affinity of the resulting expression using the `is_` assertion. Specifically, it verifies that the type affinity of the expression's type is `Numeric`. This ensures that the `percent_rank` function, when applied in this context, produces a result that is of a numeric type.\n\n**Note**: This test function is primarily used to validate the correct behavior and type handling of the `percent_rank` function within a specific grouping context. It assumes that the `data` column exists and contains integer values. Ensure that the necessary dependencies, such as the `func` module and the `column` function, are properly imported and configured in the environment where this test is executed."
      ],
      "code_start_line": 1107,
      "code_end_line": 1109,
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def test_percent_rank(self):\n        expr = func.percent_rank(0.5).within_group(column(\"data\", Integer))\n        is_(expr.type._type_affinity, Numeric)\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "ClassDef",
      "name": "ExecuteTest",
      "md_content": [
        "**ExecuteTest**: The function of ExecuteTest is to test the execution of SQL functions and expressions using SQLAlchemy, ensuring that database operations such as queries, updates, and function executions behave as expected.\n\n**attributes**: The attributes of this Class.\n· __backend__: A class-level attribute set to `True`, indicating that this test class requires a database backend to execute its tests.\n\n**Code Description**: The description of this Class.\nThe `ExecuteTest` class is a test class that inherits from `fixtures.TestBase`. It contains multiple test methods to validate the behavior of SQLAlchemy's execution of SQL functions, expressions, and database operations. Below is a detailed breakdown of its methods:\n\n1. **teardown_test**: This method is a placeholder for cleanup operations after each test. It currently does nothing (`pass`).\n\n2. **test_conn_execute**: Tests the execution of SQL functions using a database connection. It defines a custom SQL function `myfunc` and uses the `@compiles` decorator to compile it. The test verifies that the results of executing `func.current_date()` directly, via a select statement, and using the custom function are identical.\n\n3. **test_exec_options**: Tests the execution options for SQL functions. It verifies that execution options like `foo=\"bar\"` are correctly propagated to the SQL function and its select statement.\n\n4. **test_update**: Tests the insertion and updating of database records using SQL functions and expressions. It creates two tables (`t1` and `t2`) with columns that use SQL functions for default values and on-update behavior. The test verifies that the values inserted and updated using SQL functions are correct.\n\n5. **test_aggregate_strings_execute**: Tests the aggregation of string values using the `aggregate_strings` function. It creates a table with string and Unicode columns, inserts data, and verifies that the aggregation works correctly with both Unicode and non-Unicode separators.\n\n6. **test_as_from**: Tests the execution of SQL functions as a source for a `SELECT` statement. It verifies that the results of executing `func.current_date()` directly and as a `SELECT` source are identical. This test is limited to PostgreSQL.\n\n7. **test_extract_bind**: Tests the extraction of date parts (year, month, day) from a date or datetime object using the `extract` function. It verifies that the extracted values match the expected results.\n\n8. **test_extract_expression**: Tests the extraction of date parts from database columns using the `extract` function. It creates a table with `DateTime` and `Date` columns, inserts data, and verifies that the extracted values are correct.\n\n**Note**: \n- The `test_as_from` method is specifically designed to work with PostgreSQL and may not function correctly with other databases.\n- The `test_aggregate_strings_execute` method handles both Unicode and non-Unicode strings and separators, ensuring compatibility with different database configurations.\n- The `test_update` method demonstrates the use of SQL functions in `INSERT` and `UPDATE` statements, including column-level defaults and on-update behaviors.\n\n**Output Example**: \nFor the `test_conn_execute` method, the output might look like this:\n```\nTrue\n```\nThis indicates that the results of executing `func.current_date()` in different ways are identical.\n\nFor the `test_update` method, the output might look like this:\n```\n[(9, 'some stuff'), (9, 'some stuff'), (9, 'some stuff')]\n```\nThis shows the updated values in the `t2` table after executing the update statement with SQL functions."
      ],
      "code_start_line": 1112,
      "code_end_line": 1330,
      "params": [],
      "have_return": true,
      "code_content": "class ExecuteTest(fixtures.TestBase):\n    __backend__ = True\n\n    def teardown_test(self):\n        pass\n\n    def test_conn_execute(self, connection):\n        from sqlalchemy.sql.expression import FunctionElement\n        from sqlalchemy.ext.compiler import compiles\n\n        class myfunc(FunctionElement):\n            inherit_cache = True\n            type = Date()\n\n        @compiles(myfunc)\n        def compile_(elem, compiler, **kw):\n            return compiler.process(func.current_date())\n\n        x = connection.execute(func.current_date()).scalar()\n        y = connection.execute(func.current_date().select()).scalar()\n        z = connection.scalar(func.current_date())\n        q = connection.scalar(myfunc())\n\n        assert (x == y == z == q) is True\n\n    def test_exec_options(self, connection):\n        f = func.foo()\n        eq_(f._execution_options, {})\n\n        f = f.execution_options(foo=\"bar\")\n        eq_(f._execution_options, {\"foo\": \"bar\"})\n        s = f.select()\n        eq_(s._execution_options, {\"foo\": \"bar\"})\n\n        ret = connection.execute(func.now().execution_options(foo=\"bar\"))\n        eq_(ret.context.execution_options, {\"foo\": \"bar\"})\n        ret.close()\n\n    @testing.provide_metadata\n    def test_update(self, connection):\n        \"\"\"\n        Tests sending functions and SQL expressions to the VALUES and SET\n        clauses of INSERT/UPDATE instances, and that column-level defaults\n        get overridden.\n        \"\"\"\n\n        meta = self.metadata\n        t = Table(\n            \"t1\",\n            meta,\n            Column(\n                \"id\",\n                Integer,\n                normalize_sequence(config, Sequence(\"t1idseq\", optional=True)),\n                primary_key=True,\n            ),\n            Column(\"value\", Integer),\n        )\n        t2 = Table(\n            \"t2\",\n            meta,\n            Column(\n                \"id\",\n                Integer,\n                normalize_sequence(config, Sequence(\"t2idseq\", optional=True)),\n                primary_key=True,\n            ),\n            Column(\"value\", Integer, default=7),\n            Column(\"stuff\", String(20), onupdate=\"thisisstuff\"),\n        )\n        meta.create_all(connection)\n        connection.execute(t.insert().values(value=func.length(\"one\")))\n        eq_(connection.execute(t.select()).first().value, 3)\n        connection.execute(t.update().values(value=func.length(\"asfda\")))\n        eq_(connection.execute(t.select()).first().value, 5)\n\n        r = connection.execute(\n            t.insert().values(value=func.length(\"sfsaafsda\"))\n        )\n        id_ = r.inserted_primary_key[0]\n        eq_(\n            connection.execute(t.select().where(t.c.id == id_)).first().value,\n            9,\n        )\n        connection.execute(t.update().values({t.c.value: func.length(\"asdf\")}))\n        eq_(connection.execute(t.select()).first().value, 4)\n        connection.execute(t2.insert())\n        connection.execute(t2.insert().values(value=func.length(\"one\")))\n        connection.execute(\n            t2.insert().values(value=func.length(\"asfda\") + -19),\n            dict(stuff=\"hi\"),\n        )\n\n        res = sorted(connection.execute(select(t2.c.value, t2.c.stuff)))\n        eq_(res, [(-14, \"hi\"), (3, None), (7, None)])\n\n        connection.execute(\n            t2.update().values(value=func.length(\"asdsafasd\")),\n            dict(stuff=\"some stuff\"),\n        )\n        eq_(\n            connection.execute(select(t2.c.value, t2.c.stuff)).fetchall(),\n            [(9, \"some stuff\"), (9, \"some stuff\"), (9, \"some stuff\")],\n        )\n\n        connection.execute(t2.delete())\n\n        connection.execute(t2.insert().values(value=func.length(\"one\") + 8))\n        eq_(connection.execute(t2.select()).first().value, 11)\n\n        connection.execute(t2.update().values(value=func.length(\"asfda\")))\n        eq_(\n            connection.execute(select(t2.c.value, t2.c.stuff)).first(),\n            (5, \"thisisstuff\"),\n        )\n\n        connection.execute(\n            t2.update().values(\n                {t2.c.value: func.length(\"asfdaasdf\"), t2.c.stuff: \"foo\"}\n            )\n        )\n\n        eq_(\n            connection.execute(select(t2.c.value, t2.c.stuff)).first(),\n            (9, \"foo\"),\n        )\n\n    @testing.variation(\"unicode_value\", [True, False])\n    @testing.variation(\"unicode_separator\", [True, False])\n    def test_aggregate_strings_execute(\n        self, connection, metadata, unicode_value, unicode_separator\n    ):\n        values_t = Table(\n            \"values\",\n            metadata,\n            Column(\"value\", String(42)),\n            Column(\"unicode_value\", Unicode(42)),\n        )\n        metadata.create_all(connection)\n        connection.execute(\n            values_t.insert(),\n            [\n                {\"value\": \"a\", \"unicode_value\": \"測試\"},\n                {\"value\": \"b\", \"unicode_value\": \"téble2\"},\n                {\"value\": None, \"unicode_value\": None},  # ignored\n                {\"value\": \"c\", \"unicode_value\": \"🐍 su\"},\n            ],\n        )\n\n        if unicode_separator:\n            separator = \" 🐍試 \"\n        else:\n            separator = \" and \"\n\n        if unicode_value:\n            col = values_t.c.unicode_value\n            expected = separator.join([\"測試\", \"téble2\", \"🐍 su\"])\n        else:\n            col = values_t.c.value\n            expected = separator.join([\"a\", \"b\", \"c\"])\n\n            # to join on a unicode separator, source string has to be unicode,\n            # so cast().  SQL Server will raise otherwise\n            if unicode_separator:\n                col = cast(col, Unicode(42))\n\n        value = connection.execute(\n            select(func.aggregate_strings(col, separator))\n        ).scalar_one()\n\n        eq_(value, expected)\n\n    @testing.fails_on_everything_except(\"postgresql\")\n    def test_as_from(self, connection):\n        # TODO: shouldn't this work on oracle too ?\n        x = connection.execute(func.current_date()).scalar()\n        y = connection.execute(func.current_date().select()).scalar()\n        z = connection.scalar(func.current_date())\n        w = connection.scalar(select(\"*\").select_from(func.current_date()))\n\n        assert x == y == z == w\n\n    def test_extract_bind(self, connection):\n        \"\"\"Basic common denominator execution tests for extract()\"\"\"\n\n        date = datetime.date(2010, 5, 1)\n\n        def execute(field):\n            return connection.execute(select(extract(field, date))).scalar()\n\n        assert execute(\"year\") == 2010\n        assert execute(\"month\") == 5\n        assert execute(\"day\") == 1\n\n        date = datetime.datetime(2010, 5, 1, 12, 11, 10)\n\n        assert execute(\"year\") == 2010\n        assert execute(\"month\") == 5\n        assert execute(\"day\") == 1\n\n    @testing.provide_metadata\n    def test_extract_expression(self, connection):\n        meta = self.metadata\n        table = Table(\"test\", meta, Column(\"dt\", DateTime), Column(\"d\", Date))\n        meta.create_all(connection)\n        connection.execute(\n            table.insert(),\n            {\n                \"dt\": datetime.datetime(2010, 5, 1, 12, 11, 10),\n                \"d\": datetime.date(2010, 5, 1),\n            },\n        )\n        rs = connection.execute(\n            select(extract(\"year\", table.c.dt), extract(\"month\", table.c.d))\n        )\n        row = rs.first()\n        assert row[0] == 2010\n        assert row[1] == 5\n        rs.close()\n",
      "name_column": 6,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "teardown_test",
      "md_content": [
        "**teardown_test**: The function of teardown_test is to perform cleanup or teardown operations after a test execution.\n\n**parameters**: The function does not take any parameters.\n\n**Code Description**: The `teardown_test` function is a placeholder method intended to be overridden or implemented by subclasses or specific test cases. It is typically used in test frameworks to define cleanup actions that should be executed after a test case has completed. These actions might include releasing resources, resetting states, or closing connections to ensure that the environment is left in a consistent state for subsequent tests. In its current implementation, the function does nothing (`pass`), indicating that no specific teardown actions are defined by default.\n\n**Note**: Developers should override this function in their test classes to implement any necessary cleanup logic. If no cleanup is required, the function can be left as is. Ensure that any resources allocated during the test are properly released to avoid memory leaks or other issues in the test environment."
      ],
      "code_start_line": 1115,
      "code_end_line": 1116,
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def teardown_test(self):\n        pass\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "test_conn_execute",
      "md_content": [
        "**test_conn_execute**: The function of test_conn_execute is to test the execution of SQL functions and custom SQL functions using a database connection, ensuring that the results are consistent across different execution methods.\n\n**parameters**: The parameters of this Function.\n· connection: A database connection object that is used to execute SQL queries and functions.\n\n**Code Description**: \nThe `test_conn_execute` function is designed to verify the correctness and consistency of executing SQL functions and custom SQL functions through a database connection. The function performs the following steps:\n\n1. **Custom Function Definition**: \n   - A custom SQL function `myfunc` is defined as a subclass of `FunctionElement` from SQLAlchemy. This function is configured to return a `Date` type and has `inherit_cache` set to `True` for caching purposes.\n   - The `compile_` function is defined using the `@compiles` decorator to specify how `myfunc` should be compiled into SQL. In this case, it compiles `myfunc` to return the result of the `current_date()` SQL function.\n\n2. **Execution of SQL Functions**:\n   - The function executes the `current_date()` SQL function in three different ways:\n     - `x = connection.execute(func.current_date()).scalar()`: Executes the `current_date()` function directly and retrieves the scalar result.\n     - `y = connection.execute(func.current_date().select()).scalar()`: Executes the `current_date()` function within a `SELECT` statement and retrieves the scalar result.\n     - `z = connection.scalar(func.current_date())`: Directly retrieves the scalar result of the `current_date()` function using the connection's `scalar` method.\n   - Additionally, the custom function `myfunc` is executed using `q = connection.scalar(myfunc())`, and its result is stored in `q`.\n\n3. **Assertion**:\n   - The function asserts that the results of all four executions (`x`, `y`, `z`, and `q`) are equal, ensuring that the different methods of executing the SQL function and the custom function produce consistent results.\n\n**Note**: \n- The function assumes that the provided `connection` object is properly configured and connected to a database that supports the `current_date()` SQL function.\n- The custom function `myfunc` is designed to mimic the behavior of `current_date()`, so the assertion relies on this equivalence.\n\n**Output Example**: \nSince the function is primarily used for testing and does not return a value, there is no explicit output. However, if the assertion passes, it indicates that all executed SQL functions and the custom function returned the same date value. If the assertion fails, it would raise an `AssertionError`, indicating a discrepancy in the results."
      ],
      "code_start_line": 1118,
      "code_end_line": 1135,
      "params": [
        "self",
        "connection"
      ],
      "have_return": true,
      "code_content": "    def test_conn_execute(self, connection):\n        from sqlalchemy.sql.expression import FunctionElement\n        from sqlalchemy.ext.compiler import compiles\n\n        class myfunc(FunctionElement):\n            inherit_cache = True\n            type = Date()\n\n        @compiles(myfunc)\n        def compile_(elem, compiler, **kw):\n            return compiler.process(func.current_date())\n\n        x = connection.execute(func.current_date()).scalar()\n        y = connection.execute(func.current_date().select()).scalar()\n        z = connection.scalar(func.current_date())\n        q = connection.scalar(myfunc())\n\n        assert (x == y == z == q) is True\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "ClassDef",
      "name": "myfunc",
      "md_content": [
        "**myfunc**: The function of myfunc is to serve as a custom SQL function element that returns a Date type.\n\n**attributes**: The attributes of this Class.\n· inherit_cache: A boolean attribute set to True, indicating that the function's result can be cached to improve performance when the same inputs are provided repeatedly.\n· type: An attribute set to Date(), specifying that the return type of this function is a Date.\n\n**Code Description**: The myfunc class is a subclass of FunctionElement, which is typically used in SQLAlchemy to define custom SQL functions. By setting inherit_cache to True, this function leverages caching mechanisms to optimize performance, especially when the function is called multiple times with the same arguments. The type attribute is explicitly set to Date(), indicating that the output of this function will be of the Date data type. This is useful in scenarios where the function is expected to return date-related values, such as in database queries or calculations involving dates.\n\n**Note**: When using myfunc, ensure that the context in which it is applied aligns with the expected Date type output. Additionally, the caching mechanism (inherit_cache) should be considered carefully, as it may not be suitable for functions whose output depends on external factors that change over time."
      ],
      "code_start_line": 1122,
      "code_end_line": 1124,
      "params": [],
      "have_return": false,
      "code_content": "        class myfunc(FunctionElement):\n            inherit_cache = True\n            type = Date()\n",
      "name_column": 14,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "compile_",
      "md_content": [
        "**compile_**: The function of compile_ is to process the current date using a provided compiler.\n\n**parameters**: The parameters of this Function.\n· elem: This parameter is not directly used in the function but is passed to it. Its purpose is not explicitly defined within the function.\n· compiler: This is a required parameter that represents the compiler object. The function uses this object's `process` method to handle the current date.\n· **kw: This parameter allows for additional keyword arguments to be passed to the function, although they are not utilized within the function itself.\n\n**Code Description**: The `compile_` function takes in an element (`elem`), a compiler object (`compiler`), and optional keyword arguments (`**kw`). Inside the function, it calls the `process` method of the compiler object, passing the result of `func.current_date()` as an argument. The `func.current_date()` function is assumed to return the current date, which is then processed by the compiler. The function ultimately returns the result of this processing.\n\n**Note**: The function relies on the `func.current_date()` method to provide the current date. Ensure that `func.current_date()` is properly defined and accessible in the context where `compile_` is used. Additionally, the `compiler` object must have a `process` method that can handle the output of `func.current_date()`.\n\n**Output Example**: The return value of this function depends on the implementation of the `compiler.process` method. For example, if `compiler.process` formats the current date as a string, the output might look like this: `\"2023-10-05\"`."
      ],
      "code_start_line": 1127,
      "code_end_line": 1128,
      "params": [
        "elem",
        "compiler"
      ],
      "have_return": true,
      "code_content": "        def compile_(elem, compiler, **kw):\n            return compiler.process(func.current_date())\n",
      "name_column": 12,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "test_exec_options",
      "md_content": [
        "**test_exec_options**: The function of test_exec_options is to verify the behavior of execution options in a function and its associated SQL execution context.\n\n**parameters**: The parameters of this Function.\n· connection: A database connection object used to execute SQL statements and verify the execution options in the context.\n\n**Code Description**: \nThe `test_exec_options` function tests the functionality of setting and retrieving execution options for a function and its associated SQL execution context. The function begins by creating an instance of `func.foo()` and asserts that its `_execution_options` attribute is initially an empty dictionary. \n\nNext, the function sets an execution option `foo=\"bar\"` using the `execution_options` method and verifies that the `_execution_options` attribute is updated to `{\"foo\": \"bar\"}`. It then creates a SQL select statement from the function and confirms that the execution options are correctly propagated to the select statement.\n\nFinally, the function executes a SQL function `func.now()` with the same execution option `foo=\"bar\"` using the provided `connection` object. It checks that the execution options are correctly set in the execution context of the result object. The result object is then closed to release any associated resources.\n\n**Note**: Ensure that the `connection` object provided is valid and properly configured to execute SQL statements. The function assumes that the `func.foo()` and `func.now()` functions are correctly defined and available in the context."
      ],
      "code_start_line": 1137,
      "code_end_line": 1148,
      "params": [
        "self",
        "connection"
      ],
      "have_return": false,
      "code_content": "    def test_exec_options(self, connection):\n        f = func.foo()\n        eq_(f._execution_options, {})\n\n        f = f.execution_options(foo=\"bar\")\n        eq_(f._execution_options, {\"foo\": \"bar\"})\n        s = f.select()\n        eq_(s._execution_options, {\"foo\": \"bar\"})\n\n        ret = connection.execute(func.now().execution_options(foo=\"bar\"))\n        eq_(ret.context.execution_options, {\"foo\": \"bar\"})\n        ret.close()\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "test_update",
      "md_content": [
        "**test_update**: The function of test_update is to test the functionality of sending functions and SQL expressions to the VALUES and SET clauses of INSERT/UPDATE instances, and to verify that column-level defaults are overridden correctly.\n\n**parameters**: The parameters of this Function.\n· connection: A database connection object used to execute SQL statements and interact with the database.\n\n**Code Description**: \nThe `test_update` function is designed to test the behavior of SQL INSERT and UPDATE operations when using SQL functions and expressions in the VALUES and SET clauses. It also ensures that column-level defaults are properly overridden during these operations.\n\n1. **Table Creation**: The function begins by defining two tables, `t1` and `t2`, using SQLAlchemy's `Table` and `Column` constructs. Both tables have an `id` column with a sequence-based primary key and a `value` column. The `t2` table also includes a `stuff` column with an `onupdate` trigger and a default value for the `value` column.\n\n2. **Table Initialization**: The tables are created in the database using `meta.create_all(connection)`.\n\n3. **INSERT and UPDATE Operations**: The function performs a series of INSERT and UPDATE operations on the `t1` and `t2` tables:\n   - Inserts a row into `t1` with the `value` column set to the result of the SQL function `func.length(\"one\")`.\n   - Updates the `value` column in `t1` using the SQL function `func.length(\"asfda\")`.\n   - Inserts multiple rows into `t2` with varying values, including the use of SQL functions and overrides of column defaults.\n   - Updates the `value` and `stuff` columns in `t2` using SQL functions and explicit values.\n\n4. **Assertions**: The function uses `eq_` (an equality assertion function) to verify that the results of the SELECT queries match the expected values after each INSERT and UPDATE operation. This ensures that the SQL functions and expressions are correctly applied and that column defaults are overridden as expected.\n\n5. **Cleanup**: The function deletes all rows from `t2` and performs additional INSERT and UPDATE operations to further validate the behavior of the `onupdate` trigger and column defaults.\n\n**Note**: \n- This function assumes that the database connection (`connection`) is already established and that the necessary SQLAlchemy constructs (e.g., `Table`, `Column`, `func`) are available.\n- The function relies on the `eq_` assertion function to validate the results, which is typically provided by a testing framework like `unittest` or `pytest`.\n- The `onupdate` trigger in the `stuff` column of `t2` is tested to ensure it behaves as expected during UPDATE operations."
      ],
      "code_start_line": 1151,
      "code_end_line": 1237,
      "params": [
        "self",
        "connection"
      ],
      "have_return": false,
      "code_content": "    def test_update(self, connection):\n        \"\"\"\n        Tests sending functions and SQL expressions to the VALUES and SET\n        clauses of INSERT/UPDATE instances, and that column-level defaults\n        get overridden.\n        \"\"\"\n\n        meta = self.metadata\n        t = Table(\n            \"t1\",\n            meta,\n            Column(\n                \"id\",\n                Integer,\n                normalize_sequence(config, Sequence(\"t1idseq\", optional=True)),\n                primary_key=True,\n            ),\n            Column(\"value\", Integer),\n        )\n        t2 = Table(\n            \"t2\",\n            meta,\n            Column(\n                \"id\",\n                Integer,\n                normalize_sequence(config, Sequence(\"t2idseq\", optional=True)),\n                primary_key=True,\n            ),\n            Column(\"value\", Integer, default=7),\n            Column(\"stuff\", String(20), onupdate=\"thisisstuff\"),\n        )\n        meta.create_all(connection)\n        connection.execute(t.insert().values(value=func.length(\"one\")))\n        eq_(connection.execute(t.select()).first().value, 3)\n        connection.execute(t.update().values(value=func.length(\"asfda\")))\n        eq_(connection.execute(t.select()).first().value, 5)\n\n        r = connection.execute(\n            t.insert().values(value=func.length(\"sfsaafsda\"))\n        )\n        id_ = r.inserted_primary_key[0]\n        eq_(\n            connection.execute(t.select().where(t.c.id == id_)).first().value,\n            9,\n        )\n        connection.execute(t.update().values({t.c.value: func.length(\"asdf\")}))\n        eq_(connection.execute(t.select()).first().value, 4)\n        connection.execute(t2.insert())\n        connection.execute(t2.insert().values(value=func.length(\"one\")))\n        connection.execute(\n            t2.insert().values(value=func.length(\"asfda\") + -19),\n            dict(stuff=\"hi\"),\n        )\n\n        res = sorted(connection.execute(select(t2.c.value, t2.c.stuff)))\n        eq_(res, [(-14, \"hi\"), (3, None), (7, None)])\n\n        connection.execute(\n            t2.update().values(value=func.length(\"asdsafasd\")),\n            dict(stuff=\"some stuff\"),\n        )\n        eq_(\n            connection.execute(select(t2.c.value, t2.c.stuff)).fetchall(),\n            [(9, \"some stuff\"), (9, \"some stuff\"), (9, \"some stuff\")],\n        )\n\n        connection.execute(t2.delete())\n\n        connection.execute(t2.insert().values(value=func.length(\"one\") + 8))\n        eq_(connection.execute(t2.select()).first().value, 11)\n\n        connection.execute(t2.update().values(value=func.length(\"asfda\")))\n        eq_(\n            connection.execute(select(t2.c.value, t2.c.stuff)).first(),\n            (5, \"thisisstuff\"),\n        )\n\n        connection.execute(\n            t2.update().values(\n                {t2.c.value: func.length(\"asfdaasdf\"), t2.c.stuff: \"foo\"}\n            )\n        )\n\n        eq_(\n            connection.execute(select(t2.c.value, t2.c.stuff)).first(),\n            (9, \"foo\"),\n        )\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "test_aggregate_strings_execute",
      "md_content": [
        "**test_aggregate_strings_execute**: The function of test_aggregate_strings_execute is to test the functionality of aggregating strings from a database table using a specified separator, with support for both standard and Unicode strings.\n\n**parameters**: The parameters of this Function.\n· connection: A database connection object used to execute SQL commands.\n· metadata: A metadata object that holds the schema definition for the database tables.\n· unicode_value: A boolean flag indicating whether to use Unicode strings for the aggregation.\n· unicode_separator: A boolean flag indicating whether to use a Unicode separator for the aggregation.\n\n**Code Description**: The description of this Function.\nThe function begins by defining a table named \"values\" with two columns: \"value\" (a standard string column) and \"unicode_value\" (a Unicode string column). The table is then created in the database using the provided metadata and connection. Next, the function inserts four rows into the table, including standard strings, Unicode strings, and NULL values. The NULL values are ignored during the aggregation process.\n\nThe function then determines the separator to be used for aggregation based on the `unicode_separator` parameter. If `unicode_separator` is True, a Unicode separator (\" 🐍試 \") is used; otherwise, a standard separator (\" and \") is used.\n\nDepending on the `unicode_value` parameter, the function selects either the \"unicode_value\" column or the \"value\" column for aggregation. If `unicode_separator` is True and the selected column is not already a Unicode column, the function casts the column to a Unicode type to ensure compatibility with the Unicode separator.\n\nThe function then executes a SQL query to aggregate the selected column values using the `func.aggregate_strings` function and the determined separator. The result of this aggregation is compared to the expected result using the `eq_` assertion function to verify that the aggregation works as intended.\n\n**Note**: Points to note about the use of the code\n- Ensure that the database connection and metadata objects are properly initialized before calling this function.\n- The function assumes that the `func.aggregate_strings` function is available in the SQL environment and is capable of handling both standard and Unicode strings.\n- The function is designed to handle NULL values gracefully by ignoring them during the aggregation process."
      ],
      "code_start_line": 1241,
      "code_end_line": 1282,
      "params": [
        "self",
        "connection",
        "metadata",
        "unicode_value",
        "unicode_separator"
      ],
      "have_return": false,
      "code_content": "    def test_aggregate_strings_execute(\n        self, connection, metadata, unicode_value, unicode_separator\n    ):\n        values_t = Table(\n            \"values\",\n            metadata,\n            Column(\"value\", String(42)),\n            Column(\"unicode_value\", Unicode(42)),\n        )\n        metadata.create_all(connection)\n        connection.execute(\n            values_t.insert(),\n            [\n                {\"value\": \"a\", \"unicode_value\": \"測試\"},\n                {\"value\": \"b\", \"unicode_value\": \"téble2\"},\n                {\"value\": None, \"unicode_value\": None},  # ignored\n                {\"value\": \"c\", \"unicode_value\": \"🐍 su\"},\n            ],\n        )\n\n        if unicode_separator:\n            separator = \" 🐍試 \"\n        else:\n            separator = \" and \"\n\n        if unicode_value:\n            col = values_t.c.unicode_value\n            expected = separator.join([\"測試\", \"téble2\", \"🐍 su\"])\n        else:\n            col = values_t.c.value\n            expected = separator.join([\"a\", \"b\", \"c\"])\n\n            # to join on a unicode separator, source string has to be unicode,\n            # so cast().  SQL Server will raise otherwise\n            if unicode_separator:\n                col = cast(col, Unicode(42))\n\n        value = connection.execute(\n            select(func.aggregate_strings(col, separator))\n        ).scalar_one()\n\n        eq_(value, expected)\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "test_as_from",
      "md_content": [
        "**test_as_from**: The function of test_as_from is to verify the correctness of executing and retrieving the current date using different SQLAlchemy query methods.\n\n**parameters**: The parameters of this Function.\n· connection: A database connection object used to execute SQL queries and retrieve results.\n\n**Code Description**: The description of this Function.\nThe `test_as_from` function performs a series of operations to ensure that different methods of querying the current date from a database yield consistent results. It uses the `func.current_date()` function provided by SQLAlchemy to retrieve the current date. The function executes this query in four different ways:\n1. Directly executing `func.current_date()` and retrieving the scalar result.\n2. Executing a `select` statement on `func.current_date()` and retrieving the scalar result.\n3. Using the `scalar` method directly on the connection object with `func.current_date()`.\n4. Using the `scalar` method on a `select` statement that selects from `func.current_date()`.\n\nAfter executing these queries, the function asserts that all four results (`x`, `y`, `z`, and `w`) are equal, ensuring that the different methods of querying the current date produce the same output.\n\n**Note**: Points to note about the use of the code\n- The function assumes that the database connection (`connection`) is properly initialized and available for use.\n- The function currently includes a TODO comment indicating that it may not work on Oracle databases, suggesting that further testing or adjustments might be needed for Oracle compatibility."
      ],
      "code_start_line": 1285,
      "code_end_line": 1292,
      "params": [
        "self",
        "connection"
      ],
      "have_return": false,
      "code_content": "    def test_as_from(self, connection):\n        # TODO: shouldn't this work on oracle too ?\n        x = connection.execute(func.current_date()).scalar()\n        y = connection.execute(func.current_date().select()).scalar()\n        z = connection.scalar(func.current_date())\n        w = connection.scalar(select(\"*\").select_from(func.current_date()))\n\n        assert x == y == z == w\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "test_extract_bind",
      "md_content": [
        "**test_extract_bind**: The function of test_extract_bind is to perform basic execution tests for the `extract()` function, ensuring it correctly extracts year, month, and day components from date and datetime objects.\n\n**parameters**: The parameters of this Function.\n· connection: A database connection object used to execute SQL queries.\n\n**Code Description**: \nThe `test_extract_bind` function is designed to test the functionality of the `extract()` method, which is used to extract specific components (year, month, day) from date and datetime objects. The function begins by defining a date object (`datetime.date(2010, 5, 1)`) and a nested helper function `execute(field)`. This helper function constructs a SQL query using the `select()` and `extract()` methods, executes it using the provided `connection` object, and returns the scalar result.\n\nThe function then asserts that the extracted year, month, and day values match the expected values (2010, 5, and 1, respectively). Next, the function defines a datetime object (`datetime.datetime(2010, 5, 1, 12, 11, 10)`) and repeats the same assertions to ensure the `extract()` method works correctly with datetime objects as well.\n\n**Note**: \n- The `connection` parameter must be a valid database connection object capable of executing SQL queries.\n- This test assumes the `extract()` function is correctly implemented and available in the context where this test is run.\n\n**Output Example**: \nSince this is a test function, it does not return a value. Instead, it raises an assertion error if any of the extracted values do not match the expected results. If all assertions pass, the function completes without any output."
      ],
      "code_start_line": 1294,
      "code_end_line": 1310,
      "params": [
        "self",
        "connection"
      ],
      "have_return": true,
      "code_content": "    def test_extract_bind(self, connection):\n        \"\"\"Basic common denominator execution tests for extract()\"\"\"\n\n        date = datetime.date(2010, 5, 1)\n\n        def execute(field):\n            return connection.execute(select(extract(field, date))).scalar()\n\n        assert execute(\"year\") == 2010\n        assert execute(\"month\") == 5\n        assert execute(\"day\") == 1\n\n        date = datetime.datetime(2010, 5, 1, 12, 11, 10)\n\n        assert execute(\"year\") == 2010\n        assert execute(\"month\") == 5\n        assert execute(\"day\") == 1\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "execute",
      "md_content": [
        "**execute**: The function of execute is to extract a specific field from a date using a database connection and return the result as a scalar value.\n\n**parameters**: The parameters of this Function.\n· field: The field to be extracted from the date. This could be a part of the date such as year, month, day, etc.\n\n**Code Description**: The execute function takes a single parameter, `field`, which specifies the part of the date to be extracted. The function uses a database connection (`connection`) to execute a SQL query. The query is constructed using the `select` function, which selects the result of the `extract` function applied to the `field` and `date`. The `extract` function is typically used in SQL to retrieve a specific part of a date or timestamp. The result of the query is then returned as a scalar value using the `.scalar()` method, which fetches the first column of the first row of the result set.\n\n**Note**: Ensure that the `connection` object is properly initialized and connected to the database before calling this function. Additionally, the `date` variable should be defined and contain a valid date or timestamp value.\n\n**Output Example**: If the `field` parameter is set to 'year' and the `date` variable contains the date '2023-10-05', the function might return `2023` as the extracted year."
      ],
      "code_start_line": 1299,
      "code_end_line": 1300,
      "params": [
        "field"
      ],
      "have_return": true,
      "code_content": "        def execute(field):\n            return connection.execute(select(extract(field, date))).scalar()\n",
      "name_column": 12,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "test_extract_expression",
      "md_content": [
        "**test_extract_expression**: The function of test_extract_expression is to test the extraction of date and time components (year and month) from a database table using SQLAlchemy's `extract` function.\n\n**parameters**: The parameters of this Function.\n· self: The instance of the test class.\n· connection: A database connection object used to execute SQL statements and interact with the database.\n\n**Code Description**: \nThe `test_extract_expression` function performs the following steps:\n1. It initializes a metadata object (`meta`) and defines a table named \"test\" with two columns: `dt` (DateTime type) and `d` (Date type).\n2. The table is created in the database using `meta.create_all(connection)`.\n3. A new row is inserted into the \"test\" table with specific date and time values: `dt` is set to `datetime.datetime(2010, 5, 1, 12, 11, 10)` and `d` is set to `datetime.date(2010, 5, 1)`.\n4. The function then executes a SQL query using `connection.execute` to extract the \"year\" component from the `dt` column and the \"month\" component from the `d` column using the `extract` function.\n5. The result of the query is fetched, and the first row is retrieved.\n6. Assertions are made to verify that the extracted year is `2010` and the extracted month is `5`.\n7. Finally, the result set (`rs`) is closed to release database resources.\n\n**Note**: \n- Ensure that the database connection (`connection`) is properly established before calling this function.\n- The `extract` function is used to extract specific parts (e.g., year, month) from date or datetime columns, which is useful for date-based filtering or analysis.\n- The function assumes that the database supports the `extract` function and the provided date and time formats."
      ],
      "code_start_line": 1313,
      "code_end_line": 1330,
      "params": [
        "self",
        "connection"
      ],
      "have_return": false,
      "code_content": "    def test_extract_expression(self, connection):\n        meta = self.metadata\n        table = Table(\"test\", meta, Column(\"dt\", DateTime), Column(\"d\", Date))\n        meta.create_all(connection)\n        connection.execute(\n            table.insert(),\n            {\n                \"dt\": datetime.datetime(2010, 5, 1, 12, 11, 10),\n                \"d\": datetime.date(2010, 5, 1),\n            },\n        )\n        rs = connection.execute(\n            select(extract(\"year\", table.c.dt), extract(\"month\", table.c.d))\n        )\n        row = rs.first()\n        assert row[0] == 2010\n        assert row[1] == 5\n        rs.close()\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "ClassDef",
      "name": "RegisterTest",
      "md_content": [
        "**RegisterTest**: The function of RegisterTest is to test the registration and behavior of custom SQL functions within a registry system.\n\n**attributes**: The attributes of this Class.\n· `__dialect__`: Specifies the default dialect for the SQL function registry. It is set to \"default\" in this class.\n\n**Code Description**: The description of this Class.\nThe `RegisterTest` class is designed to test the registration and behavior of custom SQL functions. It inherits from `fixtures.TestBase` and `AssertsCompiledSQL`, which provide testing utilities and SQL compilation assertions, respectively. The class includes methods to set up and tear down the test environment, ensuring that the function registry is properly managed during testing.\n\n1. **setup_test**: This method initializes the test environment by creating a deep copy of the current function registry (`functions._registry`). This ensures that any modifications made during the test do not affect the global registry.\n\n2. **teardown_test**: This method restores the original function registry after the test is completed, ensuring that the global state remains unchanged.\n\n3. **test_GenericFunction_is_registered**: This test method checks that the `GenericFunction` class is not registered in the default dialect of the function registry. This is a baseline check to ensure that the registry is in the expected state before testing custom function registration.\n\n4. **test_register_function**: This method tests the registration of custom SQL functions. It defines two classes, `registered_func` and `registered_func_child`, which are registered in the function registry. The method then verifies that `registered_func` is present in the registry and that `registered_func_child` correctly inherits the `Integer` type. Additionally, it defines `not_registered_func` and `not_registered_func_child`, which are not registered in the registry, and verifies that they are not present in the registry while still correctly inheriting the `Integer` type.\n\n**Note**: Points to note about the use of the code.\n- The `setup_test` and `teardown_test` methods are crucial for maintaining the integrity of the function registry during testing. Ensure that these methods are called appropriately to avoid unintended side effects on the global registry.\n- The `test_register_function` method demonstrates how to register and test custom SQL functions. Pay attention to the `_register` attribute, which controls whether a function is registered in the registry.\n- The `__dialect__` attribute is set to \"default\", which means the tests are performed using the default SQL dialect. If testing with a different dialect, this attribute should be adjusted accordingly."
      ],
      "code_start_line": 1333,
      "code_end_line": 1369,
      "params": [],
      "have_return": false,
      "code_content": "class RegisterTest(fixtures.TestBase, AssertsCompiledSQL):\n    __dialect__ = \"default\"\n\n    def setup_test(self):\n        self._registry = deepcopy(functions._registry)\n\n    def teardown_test(self):\n        functions._registry = self._registry\n\n    def test_GenericFunction_is_registered(self):\n        assert \"GenericFunction\" not in functions._registry[\"_default\"]\n\n    def test_register_function(self):\n        # test generic function registering\n        class registered_func(GenericFunction):\n            _register = True\n\n            def __init__(self, *args, **kwargs):\n                GenericFunction.__init__(self, *args, **kwargs)\n\n        class registered_func_child(registered_func):\n            type = sqltypes.Integer\n\n        assert \"registered_func\" in functions._registry[\"_default\"]\n        assert isinstance(func.registered_func_child().type, Integer)\n\n        class not_registered_func(GenericFunction):\n            _register = False\n\n            def __init__(self, *args, **kwargs):\n                GenericFunction.__init__(self, *args, **kwargs)\n\n        class not_registered_func_child(not_registered_func):\n            type = sqltypes.Integer\n\n        assert \"not_registered_func\" not in functions._registry[\"_default\"]\n        assert isinstance(func.not_registered_func_child().type, Integer)\n",
      "name_column": 6,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "setup_test",
      "md_content": [
        "**setup_test**: The function of setup_test is to initialize the test environment by creating a deep copy of the function registry.\n\n**parameters**: This function does not take any parameters.\n\n**Code Description**: The `setup_test` function is responsible for preparing the test environment. It achieves this by creating a deep copy of the `_registry` attribute from the `functions` module and assigning it to the `_registry` attribute of the current instance. The `deepcopy` function from the `copy` module is used to ensure that the copied registry is independent of the original, preventing any unintended side effects during testing. This setup is crucial for isolating test cases and ensuring that modifications to the registry during testing do not affect other parts of the system.\n\n**Note**: Ensure that the `functions` module and its `_registry` attribute are properly initialized before calling this function. The use of `deepcopy` guarantees that the test environment is isolated, but it may introduce performance overhead if the registry is large."
      ],
      "code_start_line": 1336,
      "code_end_line": 1337,
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def setup_test(self):\n        self._registry = deepcopy(functions._registry)\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "teardown_test",
      "md_content": [
        "**teardown_test**: The function of teardown_test is to restore the registry to its original state after a test has been executed.\n\n**parameters**: This function does not take any parameters.\n\n**Code Description**: The `teardown_test` function is responsible for resetting the `_registry` attribute of the `functions` module to the value stored in the instance's `_registry` attribute. This is typically used in a testing context to ensure that any modifications made to the registry during the test are reverted, maintaining the integrity of the test environment. The function achieves this by directly assigning the instance's `_registry` attribute to the `_registry` attribute of the `functions` module.\n\n**Note**: This function is crucial for maintaining a clean state between tests, especially when the registry is modified during test execution. Ensure that the `_registry` attribute is properly initialized before calling this function to avoid unintended side effects."
      ],
      "code_start_line": 1339,
      "code_end_line": 1340,
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def teardown_test(self):\n        functions._registry = self._registry\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "test_GenericFunction_is_registered",
      "md_content": [
        "**test_GenericFunction_is_registered**: The function of test_GenericFunction_is_registered is to verify that the \"GenericFunction\" is not registered in the default registry of functions.\n\n**parameters**: The parameters of this Function.\n· self: Refers to the instance of the test class. This parameter is automatically passed when the method is called.\n\n**Code Description**: \nThe function `test_GenericFunction_is_registered` is a test method that checks whether the \"GenericFunction\" is present in the default registry of functions. It does this by asserting that the string \"GenericFunction\" is not a key in the `_registry[\"_default\"]` dictionary of the `functions` module. If the assertion fails, it means that \"GenericFunction\" is registered in the default registry, which would indicate a potential issue or unexpected behavior in the code being tested. This test ensures that the registry remains clean and does not contain unintended entries.\n\n**Note**: \n- This test assumes that the `functions` module and its `_registry` attribute are properly initialized and accessible.\n- The test is designed to fail if \"GenericFunction\" is found in the default registry, so it is crucial to ensure that the registry is in the expected state before running this test."
      ],
      "code_start_line": 1342,
      "code_end_line": 1343,
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def test_GenericFunction_is_registered(self):\n        assert \"GenericFunction\" not in functions._registry[\"_default\"]\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "test_register_function",
      "md_content": [
        "**test_register_function**: The function of test_register_function is to test the registration of generic functions and their child classes in a registry system, ensuring that only functions marked for registration are added to the registry.\n\n**parameters**: This function does not take any parameters.\n\n**Code Description**: \nThe `test_register_function` function is designed to verify the behavior of registering generic functions and their child classes within a registry system. The function performs the following steps:\n\n1. It defines a class `registered_func` that inherits from `GenericFunction`. This class is marked for registration by setting the `_register` attribute to `True`. The `__init__` method of this class initializes the parent class `GenericFunction` with the provided arguments.\n\n2. A child class `registered_func_child` is then defined, which inherits from `registered_func`. This child class specifies a `type` attribute as `sqltypes.Integer`.\n\n3. The function asserts that the name `\"registered_func\"` is present in the registry (`functions._registry[\"_default\"]`), confirming that the function was successfully registered. It also asserts that an instance of `registered_func_child` has a `type` attribute that is an instance of `Integer`.\n\n4. Next, the function defines another class `not_registered_func` that also inherits from `GenericFunction`. However, this class is marked as not to be registered by setting the `_register` attribute to `False`. The `__init__` method of this class initializes the parent class `GenericFunction` with the provided arguments.\n\n5. A child class `not_registered_func_child` is defined, which inherits from `not_registered_func`. This child class also specifies a `type` attribute as `sqltypes.Integer`.\n\n6. The function asserts that the name `\"not_registered_func\"` is not present in the registry (`functions._registry[\"_default\"]`), confirming that the function was not registered. It also asserts that an instance of `not_registered_func_child` has a `type` attribute that is an instance of `Integer`.\n\n**Note**: \n- The function relies on the existence of a registry system (`functions._registry`) and assumes that the `GenericFunction` class and `sqltypes.Integer` are properly defined elsewhere in the codebase.\n- The `_register` attribute is crucial for determining whether a function should be registered or not. Ensure that this attribute is correctly set in any custom function classes that inherit from `GenericFunction`."
      ],
      "code_start_line": 1345,
      "code_end_line": 1369,
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def test_register_function(self):\n        # test generic function registering\n        class registered_func(GenericFunction):\n            _register = True\n\n            def __init__(self, *args, **kwargs):\n                GenericFunction.__init__(self, *args, **kwargs)\n\n        class registered_func_child(registered_func):\n            type = sqltypes.Integer\n\n        assert \"registered_func\" in functions._registry[\"_default\"]\n        assert isinstance(func.registered_func_child().type, Integer)\n\n        class not_registered_func(GenericFunction):\n            _register = False\n\n            def __init__(self, *args, **kwargs):\n                GenericFunction.__init__(self, *args, **kwargs)\n\n        class not_registered_func_child(not_registered_func):\n            type = sqltypes.Integer\n\n        assert \"not_registered_func\" not in functions._registry[\"_default\"]\n        assert isinstance(func.not_registered_func_child().type, Integer)\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "ClassDef",
      "name": "registered_func",
      "md_content": [
        "**registered_func**: The function of registered_func is to serve as a base class for registering functions within the system, enabling inheritance and customization of functionality.\n\n**attributes**: The attributes of this Class.\n· _register: A class-level attribute set to `True`, indicating that this class is intended to be registered within the system.\n· *args and **kwargs: These are passed to the parent class `GenericFunction` during initialization, allowing flexibility in handling additional arguments and keyword arguments.\n\n**Code Description**: \nThe `registered_func` class is a subclass of `GenericFunction`, designed to be a base class for registering functions. It includes a class-level attribute `_register` set to `True`, which signifies that this class is meant to be registered within the system. The `__init__` method initializes the class by calling the `__init__` method of its parent class, `GenericFunction`, with any provided arguments and keyword arguments. This ensures that the initialization logic of the parent class is properly executed.\n\nIn the project, `registered_func` is inherited by `registered_func_child`, which extends its functionality by specifying a `type` attribute as `sqltypes.Integer`. This demonstrates how `registered_func` serves as a foundational class for creating specialized function classes with additional attributes or behaviors.\n\n**Note**: When using `registered_func`, ensure that any subclass properly initializes the parent class using `super().__init__()` or by explicitly calling `GenericFunction.__init__()` as shown in the code. This guarantees that the parent class's initialization logic is correctly applied. Additionally, the `_register` attribute should remain `True` if the class is intended to be registered within the system."
      ],
      "code_start_line": 1347,
      "code_end_line": 1351,
      "params": [],
      "have_return": false,
      "code_content": "        class registered_func(GenericFunction):\n            _register = True\n\n            def __init__(self, *args, **kwargs):\n                GenericFunction.__init__(self, *args, **kwargs)\n",
      "name_column": 14,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "dataset/test_functions.py/RegisterTest/test_register_function/registered_func_child"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [
        "**__init__**: The function of __init__ is to initialize an instance of the class by calling the initialization method of its parent class, GenericFunction.\n\n**parameters**: The parameters of this Function.\n· *args: A variable-length argument list that allows passing any number of positional arguments to the parent class's __init__ method.\n· **kwargs: A variable-length keyword argument dictionary that allows passing any number of keyword arguments to the parent class's __init__ method.\n\n**Code Description**: The __init__ method is the constructor for the class. When an instance of the class is created, this method is automatically called. In this implementation, the __init__ method takes any number of positional arguments (*args) and keyword arguments (**kwargs) and passes them directly to the __init__ method of the parent class, GenericFunction. This ensures that the initialization logic defined in the parent class is executed, allowing the instance to be properly set up with the provided arguments.\n\n**Note**: When using this __init__ method, ensure that the arguments passed to it are compatible with the initialization requirements of the GenericFunction class. Any additional initialization logic specific to this class should be added within this method if needed."
      ],
      "code_start_line": 1350,
      "code_end_line": 1351,
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "            def __init__(self, *args, **kwargs):\n                GenericFunction.__init__(self, *args, **kwargs)\n",
      "name_column": 16,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "ClassDef",
      "name": "registered_func_child",
      "md_content": [
        "**registered_func_child**: The function of registered_func_child is to extend the functionality of the `registered_func` class by specifying a custom `type` attribute, which is set to `sqltypes.Integer`.\n\n**attributes**: The attributes of this Class.\n· type: A class-level attribute set to `sqltypes.Integer`, which defines the data type associated with this class. This attribute is used to specify the type of data that the function will handle or return.\n\n**Code Description**: The `registered_func_child` class is a subclass of `registered_func`, inheriting its core functionality while adding a specific `type` attribute. By setting `type = sqltypes.Integer`, this class explicitly defines that it is associated with integer data types. This customization allows `registered_func_child` to be used in contexts where integer-specific functionality is required.\n\nThe relationship between `registered_func_child` and its parent class, `registered_func`, is one of inheritance. `registered_func` serves as a base class that provides the foundational structure for registering functions within the system, while `registered_func_child` extends this functionality by introducing a specialized `type` attribute. This demonstrates how the parent class can be used as a template for creating more specific function classes tailored to particular data types or use cases.\n\n**Note**: When using `registered_func_child`, ensure that the `type` attribute is appropriate for the intended functionality. Since `type` is set to `sqltypes.Integer`, this class is specifically designed for handling integer data. If a different data type is required, consider creating a new subclass of `registered_func` with the appropriate `type` attribute. Additionally, always ensure that the parent class's initialization logic is properly executed by calling `super().__init__()` or explicitly initializing the parent class, as demonstrated in the `registered_func` implementation."
      ],
      "code_start_line": 1353,
      "code_end_line": 1354,
      "params": [],
      "have_return": false,
      "code_content": "        class registered_func_child(registered_func):\n            type = sqltypes.Integer\n",
      "name_column": 14,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "dataset/test_functions.py/RegisterTest/test_register_function/registered_func"
      ],
      "special_reference_type": [
        false
      ]
    },
    {
      "type": "ClassDef",
      "name": "not_registered_func",
      "md_content": [
        "**not_registered_func**: The function of not_registered_func is to serve as a base class for functions that are explicitly not registered in the system, inheriting from GenericFunction.\n\n**attributes**: The attributes of this Class.\n· _register: A class-level attribute set to False, indicating that instances of this class should not be registered in the system.\n· *args: Variable-length positional arguments passed to the constructor.\n· **kwargs: Variable-length keyword arguments passed to the constructor.\n\n**Code Description**: The not_registered_func class is a subclass of GenericFunction, designed to create function objects that are explicitly excluded from registration in the system. This is achieved by setting the class-level attribute _register to False. The class overrides the __init__ method to ensure proper initialization by calling the parent class's __init__ method with the provided arguments (*args and **kwargs). This class is intended to be used as a base class for other functions that should not be registered, as demonstrated by its child class not_registered_func_child, which inherits from it and defines additional attributes like type.\n\n**Note**: When using not_registered_func or its subclasses, ensure that the _register attribute remains False to maintain the intended behavior of preventing registration. Any subclass should explicitly define its own attributes or behavior while inheriting the non-registration feature from this base class."
      ],
      "code_start_line": 1359,
      "code_end_line": 1363,
      "params": [],
      "have_return": false,
      "code_content": "        class not_registered_func(GenericFunction):\n            _register = False\n\n            def __init__(self, *args, **kwargs):\n                GenericFunction.__init__(self, *args, **kwargs)\n",
      "name_column": 14,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "dataset/test_functions.py/RegisterTest/test_register_function/not_registered_func_child"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [
        "**__init__**: The function of __init__ is to initialize an instance of the `not_registered_func` class by calling the initialization method of its parent class, `GenericFunction`.\n\n**parameters**: The parameters of this Function.\n· *args: Variable-length positional arguments that are passed to the parent class's `__init__` method.\n· **kwargs: Variable-length keyword arguments that are passed to the parent class's `__init__` method.\n\n**Code Description**: The `__init__` method is the constructor for the `not_registered_func` class. When an instance of this class is created, this method is automatically called. It takes any number of positional arguments (`*args`) and keyword arguments (`**kwargs`) and passes them directly to the `__init__` method of the parent class, `GenericFunction`. This ensures that the initialization logic defined in `GenericFunction` is executed, setting up the instance with the necessary attributes and configurations.\n\n**Note**: This method does not add any additional initialization logic specific to the `not_registered_func` class. It relies entirely on the parent class's initialization process. Therefore, any customization or additional setup for instances of this class should be handled in the parent class or through the arguments passed during instantiation."
      ],
      "code_start_line": 1362,
      "code_end_line": 1363,
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "            def __init__(self, *args, **kwargs):\n                GenericFunction.__init__(self, *args, **kwargs)\n",
      "name_column": 16,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "ClassDef",
      "name": "not_registered_func_child",
      "md_content": [
        "**not_registered_func_child**: The function of not_registered_func_child is to serve as a subclass of not_registered_func, specifically defining a type attribute for use in contexts where the function should not be registered in the system.\n\n**attributes**: The attributes of this Class.\n· type: A class-level attribute set to sqltypes.Integer, indicating the data type associated with this function. This attribute is used to define the expected return type or behavior of the function in database operations.\n\n**Code Description**: The not_registered_func_child class is a subclass of not_registered_func, inheriting its non-registration behavior by maintaining the _register attribute as False. This class extends the functionality of its parent by introducing a type attribute, which is set to sqltypes.Integer. This attribute is crucial for defining the data type associated with the function, particularly in database-related operations where type information is necessary for proper execution or validation. By inheriting from not_registered_func, this class ensures that instances of not_registered_func_child are not registered in the system, while also providing additional type-specific functionality.\n\n**Note**: When using not_registered_func_child, ensure that the type attribute is appropriately defined based on the intended use case. Since this class inherits the non-registration behavior from not_registered_func, it is suitable for scenarios where explicit registration of the function is undesirable. Any further customization or extension of this class should maintain the _register attribute as False to preserve its intended behavior."
      ],
      "code_start_line": 1365,
      "code_end_line": 1366,
      "params": [],
      "have_return": false,
      "code_content": "        class not_registered_func_child(not_registered_func):\n            type = sqltypes.Integer\n",
      "name_column": 14,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "dataset/test_functions.py/RegisterTest/test_register_function/not_registered_func"
      ],
      "special_reference_type": [
        false
      ]
    },
    {
      "type": "ClassDef",
      "name": "TableValuedCompileTest",
      "md_content": [
        "**TableValuedCompileTest**: The function of TableValuedCompileTest is to test the compilation and functionality of table-valued SQL functions and their usage in SQLAlchemy queries.\n\n**attributes**: The attributes of this Class.\n· __dialect__: Specifies the SQL dialect used for compiling SQL statements. In this case, it is set to \"default_enhanced\".\n\n**Code Description**: The TableValuedCompileTest class is a test class that inherits from fixtures.TestBase and AssertsCompiledSQL. It is designed to verify the correct compilation and behavior of table-valued functions in SQLAlchemy. The class contains multiple test methods, each focusing on a specific aspect of table-valued functions, such as scalar table-valued functions, table-valued functions with ordinality, and subqueries as table-valued functions.\n\n1. **test_aggregate_scalar_over_table_valued**: Tests the use of a table-valued function (json_array_elements_text) in combination with an aggregate function (max). It verifies the SQL compilation of a query that selects the maximum value from a JSON array.\n\n2. **test_scalar_table_valued**: Tests the use of scalar table-valued functions (jsonb_each) to extract key-value pairs from a JSON column. It checks the SQL compilation of a query that selects these key-value pairs.\n\n3. **test_table_valued_one**: Tests the use of a table-valued function (jsonb_each) in a JOIN operation. It verifies the SQL compilation of a query that joins a table with the result of a table-valued function.\n\n4. **test_table_valued_two**: Tests the use of a table-valued function (value_ids) in a JOIN operation with another table. It checks the SQL compilation of a query that joins the result of the table-valued function with another table.\n\n5. **test_table_as_table_valued**: Tests the use of a table as a table-valued function. It verifies the SQL compilation of a query that uses the row_to_json function on a table.\n\n6. **test_subquery_as_table_valued**: Tests the use of a subquery as a table-valued function. It checks the SQL compilation of a query that uses the row_to_json function on a subquery.\n\n7. **test_scalar_subquery**: Tests the use of a scalar subquery as a table-valued function. It verifies the SQL compilation of a query that uses the row_to_json function on a scalar subquery.\n\n8. **test_named_with_ordinality**: Tests the use of a table-valued function (unnest) with ordinality. It checks the SQL compilation of a query that uses the WITH ORDINALITY clause.\n\n9. **test_render_derived_maintains_tableval_type**: Tests that the render_derived method maintains the table-valued type of a function.\n\n10. **test_alias_maintains_tableval_type**: Tests that the alias method maintains the table-valued type of a function.\n\n11. **test_star_with_ordinality**: Tests the use of a table-valued function (generate_series) with ordinality in a SELECT * query.\n\n12. **test_json_object_keys_with_ordinality**: Tests the use of a table-valued function (json_object_keys) with ordinality. It checks the SQL compilation of a query that selects all columns from the result of the function.\n\n13. **test_alias_column**: Tests the use of aliased columns from table-valued functions (generate_series). It verifies the SQL compilation of a query that selects these aliased columns.\n\n14. **test_column_valued_one**: Tests the use of a column-valued function (unnest). It checks the SQL compilation of a query that selects the result of the function.\n\n15. **test_column_valued_two**: Tests the use of multiple column-valued functions (generate_series) in a single query. It verifies the SQL compilation of a query that selects the results of these functions.\n\n16. **test_column_valued_subquery**: Tests the use of column-valued functions in a subquery. It checks the SQL compilation of a query that selects from a subquery containing column-valued functions.\n\n17. **test_render_derived_with_lateral**: Tests the use of the render_derived method with the LATERAL keyword. It verifies the SQL compilation of a query that joins a table with a lateral derived table.\n\n18. **test_function_alias**: Tests the use of an aliased table-valued function (json_array_elements) in a query. It checks the SQL compilation of a query that selects from the aliased function.\n\n19. **test_named_table_valued**: Tests the use of a named table-valued function (json_to_recordset). It verifies the SQL compilation of a query that selects columns from the result of the function.\n\n20. **test_named_table_valued_w_quoting**: Tests the use of a named table-valued function with quoted column names. It checks the SQL compilation of a query that selects columns with special characters in their names.\n\n21. **test_named_table_valued_subquery**: Tests the use of a named table-valued function in a subquery. It verifies the SQL compilation of a query that selects from a subquery containing the named table-valued function.\n\n22. **test_named_table_valued_alias**: Tests the use of an aliased named table-valued function. It checks the SQL compilation of a query that selects columns from the aliased function.\n\n**Note**: When using table-valued functions, ensure that the SQL dialect supports the specific functions and features being tested. The tests in this class are designed to verify the correct compilation of SQL statements, so they are useful for ensuring compatibility with different SQL dialects. Additionally, the use of ordinality and lateral joins may require specific database support."
      ],
      "code_start_line": 1372,
      "code_end_line": 1899,
      "params": [],
      "have_return": false,
      "code_content": "class TableValuedCompileTest(fixtures.TestBase, AssertsCompiledSQL):\n    \"\"\"test the full set of functions as FROM developed in [ticket:3566]\"\"\"\n\n    __dialect__ = \"default_enhanced\"\n\n    def test_aggregate_scalar_over_table_valued(self):\n        test = table(\"test\", column(\"id\"), column(\"data\", JSON))\n\n        elem = (\n            func.json_array_elements_text(test.c.data[\"key\"])\n            .table_valued(\"value\")\n            .alias(\"elem\")\n        )\n\n        maxdepth = select(func.max(cast(elem.c.value, Float))).label(\n            \"maxdepth\"\n        )\n\n        stmt = select(test.c.id.label(\"test_id\"), maxdepth).order_by(\n            \"maxdepth\"\n        )\n\n        self.assert_compile(\n            stmt,\n            \"SELECT test.id AS test_id, \"\n            \"(SELECT max(CAST(elem.value AS FLOAT)) AS max_1 \"\n            \"FROM json_array_elements_text(test.data[:data_1]) AS elem) \"\n            \"AS maxdepth \"\n            \"FROM test ORDER BY maxdepth\",\n        )\n\n    def test_scalar_table_valued(self):\n        assets_transactions = table(\n            \"assets_transactions\", column(\"id\"), column(\"contents\", JSON)\n        )\n\n        stmt = select(\n            assets_transactions.c.id,\n            func.jsonb_each(\n                assets_transactions.c.contents\n            ).scalar_table_valued(\"key\"),\n            func.jsonb_each(\n                assets_transactions.c.contents\n            ).scalar_table_valued(\"value\"),\n        )\n        self.assert_compile(\n            stmt,\n            \"SELECT assets_transactions.id, \"\n            \"(jsonb_each(assets_transactions.contents)).key, \"\n            \"(jsonb_each(assets_transactions.contents)).value \"\n            \"FROM assets_transactions\",\n        )\n\n    def test_table_valued_one(self):\n        assets_transactions = table(\n            \"assets_transactions\", column(\"id\"), column(\"contents\", JSON)\n        )\n\n        jb = func.jsonb_each(assets_transactions.c.contents).table_valued(\n            \"key\", \"value\"\n        )\n\n        stmt = select(assets_transactions.c.id, jb.c.key, jb.c.value).join(\n            jb, true()\n        )\n\n        self.assert_compile(\n            stmt,\n            \"SELECT assets_transactions.id, anon_1.key, anon_1.value \"\n            \"FROM assets_transactions \"\n            \"JOIN jsonb_each(assets_transactions.contents) AS anon_1 ON true\",\n        )\n\n    def test_table_valued_two(self):\n        \"\"\"\n        SELECT vi.id, vv.value\n        FROM value_ids() AS vi JOIN values AS vv ON vv.id = vi.id\n\n        \"\"\"\n\n        values = table(\n            \"values\",\n            column(\n                \"id\",\n                Integer,\n            ),\n            column(\"value\", String),\n        )\n        vi = func.value_ids().table_valued(column(\"id\", Integer)).alias(\"vi\")\n        vv = values.alias(\"vv\")\n\n        stmt = select(vi.c.id, vv.c.value).select_from(  # noqa\n            vi.join(vv, vv.c.id == vi.c.id)\n        )\n        self.assert_compile(\n            stmt,\n            \"SELECT vi.id, vv.value FROM value_ids() AS vi \"\n            \"JOIN values AS vv ON vv.id = vi.id\",\n        )\n\n    def test_table_as_table_valued(self):\n        a = table(\n            \"a\",\n            column(\"id\"),\n            column(\"x\"),\n            column(\"y\"),\n        )\n\n        stmt = select(func.row_to_json(a.table_valued()))\n\n        self.assert_compile(\n            stmt, \"SELECT row_to_json(a) AS row_to_json_1 FROM a\"\n        )\n\n    def test_subquery_as_table_valued(self):\n        \"\"\"\n        SELECT row_to_json(anon_1) AS row_to_json_1\n        FROM (SELECT a.id AS id, a.x AS x, a.y AS y\n        FROM a) AS anon_1\n\n        \"\"\"\n\n        a = table(\n            \"a\",\n            column(\"id\"),\n            column(\"x\"),\n            column(\"y\"),\n        )\n\n        stmt = select(func.row_to_json(a.select().subquery().table_valued()))\n\n        self.assert_compile(\n            stmt,\n            \"SELECT row_to_json(anon_1) AS row_to_json_1 FROM \"\n            \"(SELECT a.id AS id, a.x AS x, a.y AS y FROM a) AS anon_1\",\n        )\n\n    def test_scalar_subquery(self):\n        a = table(\n            \"a\",\n            column(\"id\"),\n            column(\"x\"),\n            column(\"y\"),\n        )\n\n        stmt = select(func.row_to_json(a.select().scalar_subquery()))\n\n        self.assert_compile(\n            stmt,\n            \"SELECT row_to_json((SELECT a.id, a.x, a.y FROM a)) \"\n            \"AS row_to_json_1\",\n        )\n\n    def test_named_with_ordinality(self):\n        \"\"\"\n        SELECT a.id AS a_id, a.refs AS a_refs,\n        unnested.unnested AS unnested_unnested,\n        unnested.ordinality AS unnested_ordinality,\n        b.id AS b_id, b.ref AS b_ref\n        FROM a LEFT OUTER JOIN unnest(a.refs)\n        `WITH ORDINALITY AS unnested(unnested, ordinality) ON true\n        LEFT OUTER JOIN b ON unnested.unnested = b.ref\n\n        \"\"\"  # noqa: 501\n\n        a = table(\"a\", column(\"id\"), column(\"refs\"))\n        b = table(\"b\", column(\"id\"), column(\"ref\"))\n\n        unnested = (\n            func.unnest(a.c.refs)\n            .table_valued(\"unnested\", with_ordinality=\"ordinality\")\n            .render_derived()\n            .alias(\"unnested\")\n        )\n\n        stmt = (\n            select(\n                a.c.id, a.c.refs, unnested.c.unnested, unnested.c.ordinality\n            )\n            .outerjoin(unnested, true())\n            .outerjoin(\n                b,\n                unnested.c.unnested == b.c.ref,\n            )\n        )\n        self.assert_compile(\n            stmt,\n            \"SELECT a.id, a.refs, unnested.unnested, unnested.ordinality \"\n            \"FROM a \"\n            \"LEFT OUTER JOIN unnest(a.refs) \"\n            \"WITH ORDINALITY AS unnested(unnested, ordinality) ON true \"\n            \"LEFT OUTER JOIN b ON unnested.unnested = b.ref\",\n        )\n\n    def test_render_derived_maintains_tableval_type(self):\n        fn = func.json_something()\n\n        tv = fn.table_valued(column(\"x\", String))\n\n        eq_(tv.column.type, testing.eq_type_affinity(sqltypes.TableValueType))\n        eq_(tv.column.type._elements[0].type, testing.eq_type_affinity(String))\n\n        tv = tv.render_derived()\n        eq_(tv.column.type, testing.eq_type_affinity(sqltypes.TableValueType))\n        eq_(tv.column.type._elements[0].type, testing.eq_type_affinity(String))\n\n    def test_alias_maintains_tableval_type(self):\n        fn = func.json_something()\n\n        tv = fn.table_valued(column(\"x\", String))\n\n        eq_(tv.column.type, testing.eq_type_affinity(sqltypes.TableValueType))\n        eq_(tv.column.type._elements[0].type, testing.eq_type_affinity(String))\n\n        tv = tv.alias()\n        eq_(tv.column.type, testing.eq_type_affinity(sqltypes.TableValueType))\n        eq_(tv.column.type._elements[0].type, testing.eq_type_affinity(String))\n\n    def test_star_with_ordinality(self):\n        \"\"\"\n        SELECT * FROM generate_series(4,1,-1) WITH ORDINALITY;\n        \"\"\"\n\n        stmt = select(\"*\").select_from(  # noqa\n            func.generate_series(4, 1, -1).table_valued(\n                with_ordinality=\"ordinality\"\n            )\n        )\n        self.assert_compile(\n            stmt,\n            \"SELECT * FROM generate_series\"\n            \"(:generate_series_1, :generate_series_2, :generate_series_3) \"\n            \"WITH ORDINALITY AS anon_1\",\n        )\n\n    def test_json_object_keys_with_ordinality(self):\n        \"\"\"\n        SELECT * FROM json_object_keys('{\"a1\":\"1\",\"a2\":\"2\",\"a3\":\"3\"}')\n        WITH ORDINALITY AS t(keys, n);\n        \"\"\"\n        stmt = select(\"*\").select_from(\n            func.json_object_keys(\n                literal({\"a1\": \"1\", \"a2\": \"2\", \"a3\": \"3\"}, type_=JSON)\n            )\n            .table_valued(\"keys\", with_ordinality=\"n\")\n            .render_derived()\n            .alias(\"t\")\n        )\n\n        self.assert_compile(\n            stmt,\n            \"SELECT * FROM json_object_keys(:param_1) \"\n            \"WITH ORDINALITY AS t(keys, n)\",\n        )\n\n    def test_alias_column(self):\n        \"\"\"\n        .. sourcecode:: sql\n\n            SELECT x, y\n            FROM\n                generate_series(:generate_series_1, :generate_series_2) AS x,\n                generate_series(:generate_series_3, :generate_series_4) AS y\n\n        \"\"\"\n\n        x = func.generate_series(1, 2).alias(\"x\")\n        y = func.generate_series(3, 4).alias(\"y\")\n        stmt = select(x.column, y.column)\n\n        self.assert_compile(\n            stmt,\n            \"SELECT x, y FROM \"\n            \"generate_series(:generate_series_1, :generate_series_2) AS x, \"\n            \"generate_series(:generate_series_3, :generate_series_4) AS y\",\n        )\n\n    def test_column_valued_one(self):\n        fn = func.unnest([\"one\", \"two\", \"three\", \"four\"]).column_valued()\n\n        stmt = select(fn)\n\n        self.assert_compile(\n            stmt, \"SELECT anon_1 FROM unnest(:unnest_1) AS anon_1\"\n        )\n\n    def test_column_valued_two(self):\n        \"\"\"\n        .. sourcecode:: sql\n\n            SELECT x, y\n            FROM\n                generate_series(:generate_series_1, :generate_series_2) AS x,\n                generate_series(:generate_series_3, :generate_series_4) AS y\n\n        \"\"\"\n\n        x = func.generate_series(1, 2).column_valued(\"x\")\n        y = func.generate_series(3, 4).column_valued(\"y\")\n        stmt = select(x, y)\n\n        self.assert_compile(\n            stmt,\n            \"SELECT x, y FROM \"\n            \"generate_series(:generate_series_1, :generate_series_2) AS x, \"\n            \"generate_series(:generate_series_3, :generate_series_4) AS y\",\n        )\n\n    def test_column_valued_subquery(self):\n        x = func.generate_series(1, 2).column_valued(\"x\")\n        y = func.generate_series(3, 4).column_valued(\"y\")\n        subq = select(x, y).subquery()\n        stmt = select(subq).where(subq.c.x > 2)\n\n        self.assert_compile(\n            stmt,\n            \"SELECT anon_1.x, anon_1.y FROM \"\n            \"(SELECT x, y FROM \"\n            \"generate_series(:generate_series_1, :generate_series_2) AS x, \"\n            \"generate_series(:generate_series_3, :generate_series_4) AS y\"\n            \") AS anon_1 \"\n            \"WHERE anon_1.x > :x_1\",\n        )\n\n    @testing.combinations((True,), (False,))\n    def test_render_derived_with_lateral(self, apply_alias_after_lateral):\n        \"\"\"\n        # this is the \"record\" type\n\n        SELECT\n            table1.user_id AS table1_user_id,\n            table2.name AS table2_name,\n            jsonb_table.name AS jsonb_table_name,\n            count(jsonb_table.time) AS count_1\n            FROM table1\n            JOIN table2 ON table1.user_id = table2.id\n            JOIN LATERAL jsonb_to_recordset(table1.jsonb)\n            AS jsonb_table(name TEXT, time FLOAT) ON true\n            WHERE table2.route_id = %(route_id_1)s\n            AND jsonb_table.name IN (%(name_1)s, %(name_2)s, %(name_3)s)\n            GROUP BY table1.user_id, table2.name, jsonb_table.name\n            ORDER BY table2.name\n\n        \"\"\"  # noqa\n\n        table1 = table(\"table1\", column(\"user_id\"), column(\"jsonb\"))\n        table2 = table(\n            \"table2\", column(\"id\"), column(\"name\"), column(\"route_id\")\n        )\n        jsonb_table = func.jsonb_to_recordset(table1.c.jsonb).table_valued(\n            column(\"name\", Text), column(\"time\", Float)\n        )\n\n        # I'm a little concerned about the naming, that lateral() and\n        # alias() both make a new name unconditionally.  lateral() already\n        # works this way, so try to just make sure .alias() after the\n        # fact works too\n        if apply_alias_after_lateral:\n            jsonb_table = (\n                jsonb_table.render_derived(with_types=True)\n                .lateral()\n                .alias(\"jsonb_table\")\n            )\n        else:\n            jsonb_table = jsonb_table.render_derived(with_types=True).lateral(\n                \"jsonb_table\"\n            )\n\n        stmt = (\n            select(\n                table1.c.user_id,\n                table2.c.name,\n                jsonb_table.c.name.label(\"jsonb_table_name\"),\n                func.count(jsonb_table.c.time),\n            )\n            .select_from(table1)\n            .join(table2, table1.c.user_id == table2.c.id)\n            .join(jsonb_table, true())\n            .where(table2.c.route_id == 5)\n            .where(jsonb_table.c.name.in_([\"n1\", \"n2\", \"n3\"]))\n            .group_by(table1.c.user_id, table2.c.name, jsonb_table.c.name)\n            .order_by(table2.c.name)\n        )\n\n        self.assert_compile(\n            stmt,\n            \"SELECT table1.user_id, table2.name, \"\n            \"jsonb_table.name AS jsonb_table_name, \"\n            \"count(jsonb_table.time) AS count_1 \"\n            \"FROM table1 \"\n            \"JOIN table2 ON table1.user_id = table2.id \"\n            \"JOIN LATERAL jsonb_to_recordset(table1.jsonb) \"\n            \"AS jsonb_table(name TEXT, time FLOAT) ON true \"\n            \"WHERE table2.route_id = 5 \"\n            \"AND jsonb_table.name IN ('n1', 'n2', 'n3') \"\n            \"GROUP BY table1.user_id, table2.name, jsonb_table.name \"\n            \"ORDER BY table2.name\",\n            literal_binds=True,\n            render_postcompile=True,\n        )\n\n    def test_function_alias(self):\n        \"\"\"\n        .. sourcecode:: sql\n\n            SELECT result_elem -> 'Field' as field\n            FROM \"check\" AS check_, json_array_elements(\n            (\n                SELECT check_inside.response -> 'Results'\n                FROM \"check\" as check_inside\n                WHERE check_inside.id = check_.id\n            )\n            ) AS result_elem\n            WHERE result_elem ->> 'Name' = 'FooBar'\n\n        \"\"\"\n        check = table(\"check\", column(\"id\"), column(\"response\", JSON))\n\n        check_inside = check.alias(\"check_inside\")\n        check_outside = check.alias(\"_check\")\n\n        subq = (\n            select(check_inside.c.response[\"Results\"])\n            .where(check_inside.c.id == check_outside.c.id)\n            .scalar_subquery()\n        )\n\n        fn = func.json_array_elements(subq, type_=JSON).alias(\"result_elem\")\n\n        stmt = (\n            select(fn.column[\"Field\"].label(\"field\"))\n            .where(fn.column[\"Name\"] == \"FooBar\")\n            .select_from(check_outside)\n        )\n\n        self.assert_compile(\n            stmt,\n            \"SELECT result_elem[:result_elem_1] AS field \"\n            'FROM \"check\" AS _check, json_array_elements('\n            \"(SELECT check_inside.response[:response_1] AS anon_1 \"\n            'FROM \"check\" AS check_inside '\n            \"WHERE check_inside.id = _check.id)\"\n            \") AS result_elem \"\n            \"WHERE result_elem[:result_elem_2] = :param_1\",\n        )\n\n    def test_named_table_valued(self):\n        fn = (\n            func.json_to_recordset(  # noqa\n                '[{\"a\":1,\"b\":\"foo\"},{\"a\":\"2\",\"c\":\"bar\"}]'\n            )\n            .table_valued(column(\"a\", Integer), column(\"b\", String))\n            .render_derived(with_types=True)\n        )\n\n        stmt = select(fn.c.a, fn.c.b)\n\n        self.assert_compile(\n            stmt,\n            \"SELECT anon_1.a, anon_1.b \"\n            \"FROM json_to_recordset(:json_to_recordset_1) \"\n            \"AS anon_1(a INTEGER, b VARCHAR)\",\n        )\n\n    def test_named_table_valued_w_quoting(self):\n        fn = (\n            func.json_to_recordset(  # noqa\n                '[{\"CaseSensitive\":1,\"the % value\":\"foo\"}, '\n                '{\"CaseSensitive\":\"2\",\"the % value\":\"bar\"}]'\n            )\n            .table_valued(\n                column(\"CaseSensitive\", Integer), column(\"the % value\", String)\n            )\n            .render_derived(with_types=True)\n        )\n\n        stmt = select(fn.c.CaseSensitive, fn.c[\"the % value\"])\n\n        self.assert_compile(\n            stmt,\n            'SELECT anon_1.\"CaseSensitive\", anon_1.\"the % value\" '\n            \"FROM json_to_recordset(:json_to_recordset_1) \"\n            'AS anon_1(\"CaseSensitive\" INTEGER, \"the % value\" VARCHAR)',\n        )\n\n    def test_named_table_valued_subquery(self):\n        fn = (\n            func.json_to_recordset(  # noqa\n                '[{\"a\":1,\"b\":\"foo\"},{\"a\":\"2\",\"c\":\"bar\"}]'\n            )\n            .table_valued(column(\"a\", Integer), column(\"b\", String))\n            .render_derived(with_types=True)\n        )\n\n        stmt = select(fn.c.a, fn.c.b).subquery()\n\n        stmt = select(stmt)\n\n        self.assert_compile(\n            stmt,\n            \"SELECT anon_1.a, anon_1.b FROM \"\n            \"(SELECT anon_2.a AS a, anon_2.b AS b \"\n            \"FROM json_to_recordset(:json_to_recordset_1) \"\n            \"AS anon_2(a INTEGER, b VARCHAR)\"\n            \") AS anon_1\",\n        )\n\n    def test_named_table_valued_alias(self):\n        \"\"\"select * from json_to_recordset\n        ('[{\"a\":1,\"b\":\"foo\"},{\"a\":\"2\",\"c\":\"bar\"}]') as x(a int, b text);\"\"\"\n\n        fn = (\n            func.json_to_recordset(  # noqa\n                '[{\"a\":1,\"b\":\"foo\"},{\"a\":\"2\",\"c\":\"bar\"}]'\n            )\n            .table_valued(column(\"a\", Integer), column(\"b\", String))\n            .render_derived(with_types=True)\n            .alias(\"jbr\")\n        )\n\n        stmt = select(fn.c.a, fn.c.b)\n\n        self.assert_compile(\n            stmt,\n            \"SELECT jbr.a, jbr.b \"\n            \"FROM json_to_recordset(:json_to_recordset_1) \"\n            \"AS jbr(a INTEGER, b VARCHAR)\",\n        )\n",
      "name_column": 6,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "test_aggregate_scalar_over_table_valued",
      "md_content": [
        "**test_aggregate_scalar_over_table_valued**: The function of test_aggregate_scalar_over_table_valued is to test the compilation of an SQL query that aggregates a scalar value over a table-valued function.\n\n**parameters**: This function does not take any parameters.\n\n**Code Description**: \nThe function begins by defining a table named \"test\" with two columns: \"id\" and \"data\" (where \"data\" is of type JSON). It then creates a table-valued function using `func.json_array_elements_text`, which extracts elements from the JSON array located at the key \"key\" in the \"data\" column. This table-valued function is aliased as \"elem\" and has a single column named \"value\".\n\nNext, the function constructs a subquery to calculate the maximum value of the extracted elements, casting them to the `Float` type. This subquery is labeled as \"maxdepth\". The main query then selects the \"id\" column from the \"test\" table (aliased as \"test_id\") and the \"maxdepth\" value, ordering the results by \"maxdepth\".\n\nFinally, the function uses `self.assert_compile` to verify that the generated SQL query matches the expected SQL string. The expected SQL string includes the selection of \"test.id\" as \"test_id\", a subquery to calculate the maximum value of the casted elements, and an ordering clause based on \"maxdepth\".\n\n**Note**: This function is primarily used for testing purposes to ensure that the SQL query generation logic works as expected when aggregating scalar values over table-valued functions. It is important to ensure that the JSON structure and key names used in the test match the expected format to avoid errors."
      ],
      "code_start_line": 1377,
      "code_end_line": 1401,
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def test_aggregate_scalar_over_table_valued(self):\n        test = table(\"test\", column(\"id\"), column(\"data\", JSON))\n\n        elem = (\n            func.json_array_elements_text(test.c.data[\"key\"])\n            .table_valued(\"value\")\n            .alias(\"elem\")\n        )\n\n        maxdepth = select(func.max(cast(elem.c.value, Float))).label(\n            \"maxdepth\"\n        )\n\n        stmt = select(test.c.id.label(\"test_id\"), maxdepth).order_by(\n            \"maxdepth\"\n        )\n\n        self.assert_compile(\n            stmt,\n            \"SELECT test.id AS test_id, \"\n            \"(SELECT max(CAST(elem.value AS FLOAT)) AS max_1 \"\n            \"FROM json_array_elements_text(test.data[:data_1]) AS elem) \"\n            \"AS maxdepth \"\n            \"FROM test ORDER BY maxdepth\",\n        )\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "test_scalar_table_valued",
      "md_content": [
        "**test_scalar_table_valued**: The function of test_scalar_table_valued is to test the compilation of a SQL query that uses a scalar table-valued function to extract key-value pairs from a JSON column in a database table.\n\n**parameters**: The parameters of this Function.\n· self: The instance of the test class, used to access assertion methods and other class attributes.\n\n**Code Description**: \nThe `test_scalar_table_valued` function is designed to verify the correct compilation of a SQL query that interacts with a JSON column in a database table. The function begins by defining a table named `assets_transactions` with two columns: `id` and `contents`, where `contents` is of type JSON. \n\nThe SQL query is constructed using the `select` statement, which selects the `id` column from the `assets_transactions` table. Additionally, it uses the `jsonb_each` function, a PostgreSQL function that expands a JSON object into a set of key-value pairs. The `scalar_table_valued` method is applied to the result of `jsonb_each` to extract the `key` and `value` fields from the JSON object stored in the `contents` column.\n\nThe `assert_compile` method is then used to compare the compiled SQL statement with the expected SQL string. The expected SQL string includes the `SELECT` statement that retrieves the `id` column and the `key` and `value` fields from the JSON object in the `contents` column. The test ensures that the SQL query is correctly compiled to match the expected output.\n\n**Note**: \n- This test assumes the use of PostgreSQL, as `jsonb_each` is a PostgreSQL-specific function.\n- The `scalar_table_valued` method is used to extract specific fields from the result of a table-valued function, which is a common operation when working with JSON data in SQL.\n- Ensure that the database being tested supports the `jsonb_each` function and the `scalar_table_valued` method to avoid runtime errors."
      ],
      "code_start_line": 1403,
      "code_end_line": 1423,
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def test_scalar_table_valued(self):\n        assets_transactions = table(\n            \"assets_transactions\", column(\"id\"), column(\"contents\", JSON)\n        )\n\n        stmt = select(\n            assets_transactions.c.id,\n            func.jsonb_each(\n                assets_transactions.c.contents\n            ).scalar_table_valued(\"key\"),\n            func.jsonb_each(\n                assets_transactions.c.contents\n            ).scalar_table_valued(\"value\"),\n        )\n        self.assert_compile(\n            stmt,\n            \"SELECT assets_transactions.id, \"\n            \"(jsonb_each(assets_transactions.contents)).key, \"\n            \"(jsonb_each(assets_transactions.contents)).value \"\n            \"FROM assets_transactions\",\n        )\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "test_table_valued_one",
      "md_content": [
        "**test_table_valued_one**: The function of test_table_valued_one is to test the compilation of a SQL query that involves a table-valued function and a join operation.\n\n**parameters**: The parameters of this Function.\n· self: The instance of the test class, used to access assertion methods and other class attributes.\n\n**Code Description**: The description of this Function.\nThe function `test_table_valued_one` is designed to test the SQL compilation of a query that uses a table-valued function in conjunction with a join operation. The test begins by defining a table named `assets_transactions` with two columns: `id` and `contents`, where `contents` is of type JSON. \n\nNext, the function `jsonb_each` is applied to the `contents` column of the `assets_transactions` table. This function is a PostgreSQL-specific function that expands a JSON object into a set of key-value pairs. The `table_valued` method is then used to define the structure of the resulting table, specifying two columns: `key` and `value`.\n\nA SQL `SELECT` statement is constructed to retrieve the `id` column from `assets_transactions`, along with the `key` and `value` columns from the table-valued function result. The `join` operation is performed between `assets_transactions` and the table-valued function result, using a `true()` condition to ensure that all rows are joined.\n\nFinally, the `assert_compile` method is used to verify that the compiled SQL query matches the expected SQL string. The expected SQL string includes the `SELECT` statement with the appropriate columns and the `JOIN` operation with the `jsonb_each` function applied to the `contents` column.\n\n**Note**: Points to note about the use of the code\n- This test function is specific to PostgreSQL due to the use of the `jsonb_each` function, which is a PostgreSQL extension.\n- The `true()` condition in the join operation ensures that all rows from both tables are included in the result, which may not be typical for most join operations but is used here for testing purposes.\n- The `assert_compile` method is crucial for verifying that the SQL query is correctly compiled, which is essential for ensuring the correctness of the SQL generation logic in the codebase."
      ],
      "code_start_line": 1425,
      "code_end_line": 1443,
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def test_table_valued_one(self):\n        assets_transactions = table(\n            \"assets_transactions\", column(\"id\"), column(\"contents\", JSON)\n        )\n\n        jb = func.jsonb_each(assets_transactions.c.contents).table_valued(\n            \"key\", \"value\"\n        )\n\n        stmt = select(assets_transactions.c.id, jb.c.key, jb.c.value).join(\n            jb, true()\n        )\n\n        self.assert_compile(\n            stmt,\n            \"SELECT assets_transactions.id, anon_1.key, anon_1.value \"\n            \"FROM assets_transactions \"\n            \"JOIN jsonb_each(assets_transactions.contents) AS anon_1 ON true\",\n        )\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "test_table_valued_two",
      "md_content": [
        "**test_table_valued_two**: The function of test_table_valued_two is to test the compilation of a SQL query that involves a table-valued function joined with another table.\n\n**parameters**: The parameters of this Function.\n· self: The instance of the test class, used to access methods and attributes within the class.\n\n**Code Description**: The description of this Function.\nThe function `test_table_valued_two` is designed to verify the correct compilation of a SQL query that uses a table-valued function (`value_ids()`) and joins it with another table (`values`). The query selects specific columns from the joined tables and ensures that the SQL statement is correctly generated.\n\n1. **Table Definitions**:\n   - The `values` table is defined with two columns: `id` (of type `Integer`) and `value` (of type `String`).\n   - The table-valued function `value_ids()` is defined to return a table with a single column `id` (of type `Integer`). This function is aliased as `vi`.\n\n2. **Table Aliases**:\n   - The `values` table is aliased as `vv`.\n\n3. **SQL Query Construction**:\n   - A SQL `SELECT` statement is constructed to retrieve the `id` column from the table-valued function (`vi`) and the `value` column from the `values` table (`vv`).\n   - The `SELECT` statement includes a `JOIN` clause that joins the table-valued function (`vi`) with the `values` table (`vv`) on the condition that their `id` columns match.\n\n4. **Assertion**:\n   - The `assert_compile` method is used to verify that the constructed SQL statement matches the expected SQL string:\n     ```sql\n     SELECT vi.id, vv.value FROM value_ids() AS vi JOIN values AS vv ON vv.id = vi.id\n     ```\n\n**Note**: Points to note about the use of the code.\n- This function is part of a test suite and is used to ensure that the SQL query generation logic works as expected. It does not execute the query against a database but rather checks the correctness of the SQL string produced by the query builder.\n- The function relies on the `table`, `column`, `func`, and `select` constructs from SQLAlchemy to define and build the SQL query. Familiarity with SQLAlchemy's ORM and Core components is necessary to understand and modify this code."
      ],
      "code_start_line": 1445,
      "code_end_line": 1470,
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def test_table_valued_two(self):\n        \"\"\"\n        SELECT vi.id, vv.value\n        FROM value_ids() AS vi JOIN values AS vv ON vv.id = vi.id\n\n        \"\"\"\n\n        values = table(\n            \"values\",\n            column(\n                \"id\",\n                Integer,\n            ),\n            column(\"value\", String),\n        )\n        vi = func.value_ids().table_valued(column(\"id\", Integer)).alias(\"vi\")\n        vv = values.alias(\"vv\")\n\n        stmt = select(vi.c.id, vv.c.value).select_from(  # noqa\n            vi.join(vv, vv.c.id == vi.c.id)\n        )\n        self.assert_compile(\n            stmt,\n            \"SELECT vi.id, vv.value FROM value_ids() AS vi \"\n            \"JOIN values AS vv ON vv.id = vi.id\",\n        )\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "test_table_as_table_valued",
      "md_content": [
        "**test_table_as_table_valued**: The function of test_table_as_table_valued is to test the functionality of using a table as a table-valued function in a SQL query and verify the correctness of the generated SQL statement.\n\n**parameters**: The parameters of this Function.\n· self: The instance of the test class, used to access assertion methods and other class attributes.\n\n**Code Description**: The description of this Function.\nThe function `test_table_as_table_valued` begins by defining a table named \"a\" with three columns: \"id\", \"x\", and \"y\". This table is created using the `table` function, which is typically used to represent a database table in SQLAlchemy. The `column` function is used to define each column within the table.\n\nNext, a SQL query is constructed using the `select` function, which selects the result of the `row_to_json` function applied to the table-valued representation of the table \"a\". The `table_valued` method is used to treat the table \"a\" as a table-valued function, which allows it to be used in a context where a function returning a table is expected. The `row_to_json` function is a PostgreSQL function that converts a row into a JSON object.\n\nFinally, the `assert_compile` method is used to verify that the generated SQL statement matches the expected SQL string. The expected SQL statement is `\"SELECT row_to_json(a) AS row_to_json_1 FROM a\"`, which selects the JSON representation of the table \"a\".\n\n**Note**: Points to note about the use of the code\n- This test function is specific to PostgreSQL, as it uses the `row_to_json` function, which is a PostgreSQL-specific feature.\n- The `table_valued` method is used to treat a table as a table-valued function, which is a powerful feature in SQLAlchemy for working with complex SQL queries.\n- Ensure that the database being tested supports the `row_to_json` function if you are adapting this code for other databases."
      ],
      "code_start_line": 1472,
      "code_end_line": 1484,
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def test_table_as_table_valued(self):\n        a = table(\n            \"a\",\n            column(\"id\"),\n            column(\"x\"),\n            column(\"y\"),\n        )\n\n        stmt = select(func.row_to_json(a.table_valued()))\n\n        self.assert_compile(\n            stmt, \"SELECT row_to_json(a) AS row_to_json_1 FROM a\"\n        )\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "test_subquery_as_table_valued",
      "md_content": [
        "**test_subquery_as_table_valued**: The function of test_subquery_as_table_valued is to test the compilation of a SQL query that uses a subquery as a table-valued function, specifically verifying that the SQLAlchemy query is correctly translated into the expected SQL statement.\n\n**parameters**: This function does not take any parameters.\n\n**Code Description**: \nThe function `test_subquery_as_table_valued` is designed to test the SQLAlchemy query compilation process for a specific use case where a subquery is treated as a table-valued function. The test involves the following steps:\n\n1. **Table Definition**: A table named `a` is defined with three columns: `id`, `x`, and `y`. This is done using the `table` and `column` functions provided by SQLAlchemy.\n\n2. **Query Construction**: A SQL query is constructed using SQLAlchemy's `select` function. The query selects the result of the `row_to_json` function applied to a subquery. The subquery is created by selecting all columns from the table `a` and then converting it into a table-valued function using the `subquery().table_valued()` method.\n\n3. **Assertion**: The `assert_compile` method is used to verify that the constructed SQLAlchemy query is correctly compiled into the expected SQL statement. The expected SQL statement is:\n   ```sql\n   SELECT row_to_json(anon_1) AS row_to_json_1 \n   FROM (SELECT a.id AS id, a.x AS x, a.y AS y FROM a) AS anon_1\n   ```\n   This ensures that the SQLAlchemy query builder correctly translates the high-level query into the appropriate SQL syntax.\n\n**Note**: This test is crucial for ensuring that SQLAlchemy's query compilation process works as expected when dealing with subqueries treated as table-valued functions. It is important to verify that the generated SQL matches the expected output to avoid runtime errors or incorrect query results."
      ],
      "code_start_line": 1486,
      "code_end_line": 1507,
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def test_subquery_as_table_valued(self):\n        \"\"\"\n        SELECT row_to_json(anon_1) AS row_to_json_1\n        FROM (SELECT a.id AS id, a.x AS x, a.y AS y\n        FROM a) AS anon_1\n\n        \"\"\"\n\n        a = table(\n            \"a\",\n            column(\"id\"),\n            column(\"x\"),\n            column(\"y\"),\n        )\n\n        stmt = select(func.row_to_json(a.select().subquery().table_valued()))\n\n        self.assert_compile(\n            stmt,\n            \"SELECT row_to_json(anon_1) AS row_to_json_1 FROM \"\n            \"(SELECT a.id AS id, a.x AS x, a.y AS y FROM a) AS anon_1\",\n        )\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "test_scalar_subquery",
      "md_content": [
        "**test_scalar_subquery**: The function of test_scalar_subquery is to test the compilation of a SQL query that includes a scalar subquery wrapped in a `row_to_json` function.\n\n**parameters**: The function does not take any parameters as it is a test method within a class.\n\n**Code Description**: \nThe `test_scalar_subquery` function begins by defining a table named \"a\" with three columns: `id`, `x`, and `y`. This table is created using the `table` function, which is typically used to represent a database table in SQLAlchemy. \n\nNext, a SQL query is constructed using the `select` function. The query selects the result of the `row_to_json` function, which is applied to a scalar subquery. The scalar subquery is generated by calling the `scalar_subquery` method on a `select` statement that retrieves all columns (`id`, `x`, and `y`) from the table \"a\". The `scalar_subquery` method ensures that the subquery returns a single row and column, which is then wrapped by the `row_to_json` function to convert the row into a JSON object.\n\nFinally, the `assert_compile` method is used to verify that the generated SQL query matches the expected SQL string. The expected SQL string is:\n```sql\nSELECT row_to_json((SELECT a.id, a.x, a.y FROM a)) AS row_to_json_1\n```\nThis ensures that the SQL query is correctly compiled and formatted as intended.\n\n**Note**: This test function is specifically designed to verify the correct compilation of a SQL query involving a scalar subquery and the `row_to_json` function. It is important to ensure that the expected SQL string accurately reflects the intended query structure, as any discrepancies would indicate an issue with the query compilation process."
      ],
      "code_start_line": 1509,
      "code_end_line": 1523,
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def test_scalar_subquery(self):\n        a = table(\n            \"a\",\n            column(\"id\"),\n            column(\"x\"),\n            column(\"y\"),\n        )\n\n        stmt = select(func.row_to_json(a.select().scalar_subquery()))\n\n        self.assert_compile(\n            stmt,\n            \"SELECT row_to_json((SELECT a.id, a.x, a.y FROM a)) \"\n            \"AS row_to_json_1\",\n        )\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "test_named_with_ordinality",
      "md_content": [
        "**test_named_with_ordinality**: The function of `test_named_with_ordinality` is to test the SQL query generation for a table-valued function with ordinality, specifically using the `unnest` function, and to verify the correctness of the compiled SQL statement.\n\n**parameters**: This function does not take any parameters as it is a test method within a class.\n\n**Code Description**: \nThe `test_named_with_ordinality` function is designed to test the generation of a SQL query that involves a table-valued function (`unnest`) with ordinality. The function begins by defining two tables, `a` and `b`, with their respective columns. The `a` table has columns `id` and `refs`, while the `b` table has columns `id` and `ref`.\n\nThe `unnest` function is applied to the `refs` column of table `a`, and the result is treated as a table-valued function with an additional ordinality column. This is achieved using the `table_valued` method, which specifies the name of the unnested column (`unnested`) and the ordinality column (`ordinality`). The `render_derived` method is then called to render the derived table, and the result is aliased as `unnested`.\n\nThe SQL query is constructed using the `select` statement, which selects columns from table `a` and the derived `unnested` table. The query includes two `LEFT OUTER JOIN` operations: the first joins the `unnested` table with table `a` using a `true` condition, and the second joins the `unnested` table with table `b` based on the condition that the `unnested` column matches the `ref` column of table `b`.\n\nFinally, the `assert_compile` method is used to verify that the generated SQL statement matches the expected SQL query string. The expected SQL query includes the `SELECT` clause, the `FROM` clause, and the `LEFT OUTER JOIN` clauses with the `WITH ORDINALITY` syntax.\n\n**Note**: \n- The `WITH ORDINALITY` clause is used to generate a row number for each element in the unnested array, which is useful for maintaining the order of elements.\n- The `assert_compile` method is crucial for ensuring that the SQL query generated by the code matches the expected SQL syntax, which is essential for testing the correctness of the query generation logic."
      ],
      "code_start_line": 1525,
      "code_end_line": 1564,
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def test_named_with_ordinality(self):\n        \"\"\"\n        SELECT a.id AS a_id, a.refs AS a_refs,\n        unnested.unnested AS unnested_unnested,\n        unnested.ordinality AS unnested_ordinality,\n        b.id AS b_id, b.ref AS b_ref\n        FROM a LEFT OUTER JOIN unnest(a.refs)\n        `WITH ORDINALITY AS unnested(unnested, ordinality) ON true\n        LEFT OUTER JOIN b ON unnested.unnested = b.ref\n\n        \"\"\"  # noqa: 501\n\n        a = table(\"a\", column(\"id\"), column(\"refs\"))\n        b = table(\"b\", column(\"id\"), column(\"ref\"))\n\n        unnested = (\n            func.unnest(a.c.refs)\n            .table_valued(\"unnested\", with_ordinality=\"ordinality\")\n            .render_derived()\n            .alias(\"unnested\")\n        )\n\n        stmt = (\n            select(\n                a.c.id, a.c.refs, unnested.c.unnested, unnested.c.ordinality\n            )\n            .outerjoin(unnested, true())\n            .outerjoin(\n                b,\n                unnested.c.unnested == b.c.ref,\n            )\n        )\n        self.assert_compile(\n            stmt,\n            \"SELECT a.id, a.refs, unnested.unnested, unnested.ordinality \"\n            \"FROM a \"\n            \"LEFT OUTER JOIN unnest(a.refs) \"\n            \"WITH ORDINALITY AS unnested(unnested, ordinality) ON true \"\n            \"LEFT OUTER JOIN b ON unnested.unnested = b.ref\",\n        )\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "test_render_derived_maintains_tableval_type",
      "md_content": [
        "**test_render_derived_maintains_tableval_type**: The function of test_render_derived_maintains_tableval_type is to verify that the table-valued type is maintained after rendering a derived table-valued function.\n\n**parameters**: This function does not take any parameters.\n\n**Code Description**: \nThe function begins by creating a table-valued function `fn` using `func.json_something()`. This function is then converted into a table-valued object `tv` with a column named \"x\" of type `String`. \n\nThe function then performs two assertions using `eq_` to verify the type affinity of the column in the table-valued object. The first assertion checks that the column type is of `sqltypes.TableValueType`, and the second assertion ensures that the type of the first element within the column type is `String`.\n\nNext, the function calls `render_derived()` on the table-valued object `tv`, which generates a derived table-valued object. The function then repeats the same two assertions to confirm that the type affinity of the column and its elements remains unchanged after rendering the derived object. This ensures that the table-valued type is preserved throughout the transformation.\n\n**Note**: This test is crucial for ensuring that the type integrity of table-valued functions is maintained when they are rendered as derived objects. It is particularly important in scenarios where table-valued functions are used in complex SQL queries or transformations."
      ],
      "code_start_line": 1566,
      "code_end_line": 1576,
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def test_render_derived_maintains_tableval_type(self):\n        fn = func.json_something()\n\n        tv = fn.table_valued(column(\"x\", String))\n\n        eq_(tv.column.type, testing.eq_type_affinity(sqltypes.TableValueType))\n        eq_(tv.column.type._elements[0].type, testing.eq_type_affinity(String))\n\n        tv = tv.render_derived()\n        eq_(tv.column.type, testing.eq_type_affinity(sqltypes.TableValueType))\n        eq_(tv.column.type._elements[0].type, testing.eq_type_affinity(String))\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "test_alias_maintains_tableval_type",
      "md_content": [
        "**test_alias_maintains_tableval_type**: The function of test_alias_maintains_tableval_type is to verify that the table-valued type is maintained when an alias is applied to a table-valued function.\n\n**parameters**: The parameters of this Function.\n· self: The instance of the test class, used to access test utilities and assertions.\n\n**Code Description**: \nThe function begins by creating a table-valued function `fn` using `func.json_something()`. This function is then converted into a table-valued object `tv` using the `table_valued` method, with a column named \"x\" of type `String`. \n\nThe function then performs two assertions using `eq_` to verify the type of the column in the table-valued object. The first assertion checks that the column type is of `sqltypes.TableValueType`, and the second assertion ensures that the type of the first element in the column is `String`.\n\nNext, the function applies an alias to the table-valued object `tv` using the `alias()` method. After applying the alias, the function repeats the same two assertions to confirm that the column type and the type of the first element remain unchanged. This ensures that the table-valued type is preserved even after the alias operation.\n\n**Note**: This test is crucial for ensuring that the type integrity of table-valued functions is maintained when aliases are applied, which is important for maintaining consistency in SQL operations involving table-valued functions."
      ],
      "code_start_line": 1578,
      "code_end_line": 1588,
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def test_alias_maintains_tableval_type(self):\n        fn = func.json_something()\n\n        tv = fn.table_valued(column(\"x\", String))\n\n        eq_(tv.column.type, testing.eq_type_affinity(sqltypes.TableValueType))\n        eq_(tv.column.type._elements[0].type, testing.eq_type_affinity(String))\n\n        tv = tv.alias()\n        eq_(tv.column.type, testing.eq_type_affinity(sqltypes.TableValueType))\n        eq_(tv.column.type._elements[0].type, testing.eq_type_affinity(String))\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "test_star_with_ordinality",
      "md_content": [
        "**test_star_with_ordinality**: The function of test_star_with_ordinality is to test the SQL compilation of a query that uses the `WITH ORDINALITY` clause with the `generate_series` function.\n\n**parameters**: This function does not take any parameters. It is a method within a test class and uses the `self` parameter to access class-level attributes and methods.\n\n**Code Description**: \nThe function `test_star_with_ordinality` is designed to verify the correct compilation of an SQL query that selects all columns from a table-valued function `generate_series` with the `WITH ORDINALITY` clause. The `generate_series` function generates a series of numbers starting from 4, ending at 1, with a step of -1. The `WITH ORDINALITY` clause adds a column named \"ordinality\" to the result set, which provides the row number for each generated value.\n\nThe SQL query is constructed using SQLAlchemy's `select` function, where `\"*\"` is used to select all columns. The `select_from` method specifies the source of the data, which is the `generate_series` function. The `table_valued` method is used to treat the result of `generate_series` as a table, and the `with_ordinality` parameter is set to \"ordinality\" to include the row number column.\n\nThe `assert_compile` method is then used to compare the compiled SQL statement with the expected SQL string. The expected SQL string includes placeholders for the parameters of the `generate_series` function and the `WITH ORDINALITY` clause.\n\n**Note**: \n- This function is part of a test suite and is intended to ensure that the SQLAlchemy query builder correctly compiles the SQL statement with the `WITH ORDINALITY` clause.\n- The `generate_series` function and the `WITH ORDINALITY` clause are PostgreSQL-specific features, so this test is relevant for PostgreSQL databases.\n- The `assert_compile` method is used to verify that the SQL statement generated by SQLAlchemy matches the expected SQL string, ensuring the correctness of the query construction."
      ],
      "code_start_line": 1590,
      "code_end_line": 1605,
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def test_star_with_ordinality(self):\n        \"\"\"\n        SELECT * FROM generate_series(4,1,-1) WITH ORDINALITY;\n        \"\"\"\n\n        stmt = select(\"*\").select_from(  # noqa\n            func.generate_series(4, 1, -1).table_valued(\n                with_ordinality=\"ordinality\"\n            )\n        )\n        self.assert_compile(\n            stmt,\n            \"SELECT * FROM generate_series\"\n            \"(:generate_series_1, :generate_series_2, :generate_series_3) \"\n            \"WITH ORDINALITY AS anon_1\",\n        )\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "test_json_object_keys_with_ordinality",
      "md_content": [
        "**test_json_object_keys_with_ordinality**: The function of test_json_object_keys_with_ordinality is to test the SQL compilation of a query that extracts keys from a JSON object along with their ordinal positions using the `json_object_keys` function with the `WITH ORDINALITY` clause.\n\n**parameters**: This function does not take any parameters as it is a test method within a class.\n\n**Code Description**: \nThe function `test_json_object_keys_with_ordinality` is designed to verify the correct compilation of an SQL query that utilizes the `json_object_keys` function in combination with the `WITH ORDINALITY` clause. The `json_object_keys` function is used to extract the keys from a JSON object, and the `WITH ORDINALITY` clause adds a column that indicates the position of each key in the JSON object.\n\nIn the code, a JSON object `{\"a1\": \"1\", \"a2\": \"2\", \"a3\": \"3\"}` is passed to the `json_object_keys` function. The function is then configured to return a table-valued result with two columns: `keys` (the keys from the JSON object) and `n` (the ordinal position of each key). The result is aliased as `t`.\n\nThe SQL query is constructed using the `select` function to select all columns (`*`) from the derived table generated by the `json_object_keys` function. The `assert_compile` method is used to compare the compiled SQL statement with the expected SQL string:\n```sql\nSELECT * FROM json_object_keys(:param_1) WITH ORDINALITY AS t(keys, n)\n```\nThis ensures that the SQL query is correctly compiled and matches the expected output.\n\n**Note**: \n- The `json_object_keys` function is specific to PostgreSQL and may not be available in other SQL databases.\n- The `WITH ORDINALITY` clause is used to generate a sequence number for each row in the result set, which is useful when the order of keys matters.\n- The `assert_compile` method is typically used in unit tests to verify that the SQL generated by an ORM or query builder matches the expected SQL string."
      ],
      "code_start_line": 1607,
      "code_end_line": 1625,
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def test_json_object_keys_with_ordinality(self):\n        \"\"\"\n        SELECT * FROM json_object_keys('{\"a1\":\"1\",\"a2\":\"2\",\"a3\":\"3\"}')\n        WITH ORDINALITY AS t(keys, n);\n        \"\"\"\n        stmt = select(\"*\").select_from(\n            func.json_object_keys(\n                literal({\"a1\": \"1\", \"a2\": \"2\", \"a3\": \"3\"}, type_=JSON)\n            )\n            .table_valued(\"keys\", with_ordinality=\"n\")\n            .render_derived()\n            .alias(\"t\")\n        )\n\n        self.assert_compile(\n            stmt,\n            \"SELECT * FROM json_object_keys(:param_1) \"\n            \"WITH ORDINALITY AS t(keys, n)\",\n        )\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "test_alias_column",
      "md_content": [
        "**test_alias_column**: The function of test_alias_column is to test the SQL compilation of a query that selects columns from aliased table-valued functions.\n\n**parameters**: This function does not take any parameters.\n\n**Code Description**: \nThe `test_alias_column` function is designed to verify the correct compilation of an SQL query that involves aliased table-valued functions. The function begins by defining two table-valued functions using `func.generate_series`. The first function, `x`, generates a series of numbers from 1 to 2 and is aliased as \"x\". The second function, `y`, generates a series of numbers from 3 to 4 and is aliased as \"y\". \n\nA SQL `SELECT` statement is then constructed using the `select` function, which selects the columns from the aliased functions `x` and `y`. The `assert_compile` method is used to compare the compiled SQL statement with the expected SQL string. The expected SQL string is a query that selects the columns `x` and `y` from the aliased `generate_series` functions.\n\nThe function ensures that the SQL query is correctly compiled and matches the expected output, verifying that the aliasing of table-valued functions works as intended.\n\n**Note**: This test function is specifically designed to validate the SQL compilation process for aliased table-valued functions. It does not execute the query against a database but rather checks the correctness of the SQL string generation."
      ],
      "code_start_line": 1627,
      "code_end_line": 1647,
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def test_alias_column(self):\n        \"\"\"\n        .. sourcecode:: sql\n\n            SELECT x, y\n            FROM\n                generate_series(:generate_series_1, :generate_series_2) AS x,\n                generate_series(:generate_series_3, :generate_series_4) AS y\n\n        \"\"\"\n\n        x = func.generate_series(1, 2).alias(\"x\")\n        y = func.generate_series(3, 4).alias(\"y\")\n        stmt = select(x.column, y.column)\n\n        self.assert_compile(\n            stmt,\n            \"SELECT x, y FROM \"\n            \"generate_series(:generate_series_1, :generate_series_2) AS x, \"\n            \"generate_series(:generate_series_3, :generate_series_4) AS y\",\n        )\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "test_column_valued_one",
      "md_content": [
        "**test_column_valued_one**: The function of test_column_valued_one is to test the functionality of the `column_valued` method when used with the `unnest` function in a SQL query.\n\n**parameters**: The parameters of this Function.\n· self: The instance of the test class, used to access methods and assertions within the test case.\n\n**Code Description**: The function `test_column_valued_one` is a test case that verifies the behavior of the `column_valued` method when applied to the result of the `unnest` function. The `unnest` function is used to expand an array into a set of rows, and the `column_valued` method is applied to treat the result as a single column in a SQL query. \n\nIn this test, the `unnest` function is called with an array of strings `[\"one\", \"two\", \"three\", \"four\"]`, and the `column_valued` method is invoked on the result. This creates a SQL expression that treats the unnest result as a single column. The `select` statement is then used to create a SQL query that selects from this column-valued expression.\n\nThe `assert_compile` method is used to verify that the generated SQL query matches the expected SQL string `\"SELECT anon_1 FROM unnest(:unnest_1) AS anon_1\"`. This ensures that the `column_valued` method correctly transforms the `unnest` result into a single-column table expression in the SQL query.\n\n**Note**: This test case is specifically designed to validate the SQL compilation process when using the `column_valued` method with the `unnest` function. It is important to ensure that the SQL syntax generated by the ORM matches the expected output for proper database interaction."
      ],
      "code_start_line": 1649,
      "code_end_line": 1656,
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def test_column_valued_one(self):\n        fn = func.unnest([\"one\", \"two\", \"three\", \"four\"]).column_valued()\n\n        stmt = select(fn)\n\n        self.assert_compile(\n            stmt, \"SELECT anon_1 FROM unnest(:unnest_1) AS anon_1\"\n        )\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "test_column_valued_two",
      "md_content": [
        "**test_column_valued_two**: The function of test_column_valued_two is to test the compilation of a SQL query that selects columns from two column-valued functions, specifically `generate_series`, and ensures the generated SQL matches the expected output.\n\n**parameters**: This function does not take any parameters as it is a test method within a class.\n\n**Code Description**: \nThe function `test_column_valued_two` is designed to verify the correct compilation of a SQL query that involves two column-valued functions. The SQL query is constructed using the `generate_series` function, which generates a series of values between specified start and end points. \n\n1. The `generate_series` function is called twice:\n   - The first call generates a series from 1 to 2 and assigns it the alias \"x\".\n   - The second call generates a series from 3 to 4 and assigns it the alias \"y\".\n\n2. These column-valued functions are then used in a `select` statement to create a SQL query that selects both \"x\" and \"y\".\n\n3. The `assert_compile` method is used to compare the compiled SQL statement with the expected SQL string. The expected SQL string is:\n   ```sql\n   SELECT x, y FROM \n   generate_series(:generate_series_1, :generate_series_2) AS x, \n   generate_series(:generate_series_3, :generate_series_4) AS y\n   ```\n   This ensures that the SQL query generated by the code matches the expected format.\n\n**Note**: \n- The function is part of a test suite, so it is primarily used for verifying the correctness of SQL query compilation.\n- The `generate_series` function is a common PostgreSQL function used to generate a series of numbers, and its usage here is to simulate column-valued data for testing purposes.\n- The `assert_compile` method is crucial for ensuring that the SQL query generated by the code matches the expected SQL syntax, which is essential for maintaining the integrity of the SQL generation logic."
      ],
      "code_start_line": 1658,
      "code_end_line": 1678,
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def test_column_valued_two(self):\n        \"\"\"\n        .. sourcecode:: sql\n\n            SELECT x, y\n            FROM\n                generate_series(:generate_series_1, :generate_series_2) AS x,\n                generate_series(:generate_series_3, :generate_series_4) AS y\n\n        \"\"\"\n\n        x = func.generate_series(1, 2).column_valued(\"x\")\n        y = func.generate_series(3, 4).column_valued(\"y\")\n        stmt = select(x, y)\n\n        self.assert_compile(\n            stmt,\n            \"SELECT x, y FROM \"\n            \"generate_series(:generate_series_1, :generate_series_2) AS x, \"\n            \"generate_series(:generate_series_3, :generate_series_4) AS y\",\n        )\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "test_column_valued_subquery",
      "md_content": [
        "**test_column_valued_subquery**: The function of test_column_valued_subquery is to test the compilation of a SQL query that involves a column-valued subquery using SQLAlchemy's functional API.\n\n**parameters**: This function does not take any parameters.\n\n**Code Description**: \nThe function `test_column_valued_subquery` is designed to test the SQL compilation of a query that uses a column-valued subquery. Here is a detailed breakdown of the code:\n\n1. **Column-Valued Series Generation**:\n   - Two column-valued series are generated using `func.generate_series`. The first series, `x`, is generated with values from 1 to 2, and the second series, `y`, is generated with values from 3 to 4. Both series are labeled as \"x\" and \"y\" respectively using the `column_valued` method.\n\n2. **Subquery Creation**:\n   - A subquery is created using the `select` statement, which selects the columns `x` and `y` from the generated series. This subquery is then aliased using the `subquery` method, resulting in an anonymous subquery (`anon_1`).\n\n3. **Main Query Construction**:\n   - A main query is constructed using the `select` statement, which selects all columns from the subquery (`anon_1`). A condition is added to the `WHERE` clause to filter rows where the value of `x` is greater than 2.\n\n4. **Assertion**:\n   - The `assert_compile` method is used to verify that the compiled SQL statement matches the expected SQL string. The expected SQL string includes the selection of columns `x` and `y` from the subquery, with the condition that `x` must be greater than 2.\n\n**Note**: \n- This test function is specifically designed to ensure that the SQLAlchemy ORM correctly compiles a query involving column-valued subqueries. It is important to verify that the generated SQL matches the expected output to ensure the correctness of the ORM's query compilation process."
      ],
      "code_start_line": 1680,
      "code_end_line": 1694,
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def test_column_valued_subquery(self):\n        x = func.generate_series(1, 2).column_valued(\"x\")\n        y = func.generate_series(3, 4).column_valued(\"y\")\n        subq = select(x, y).subquery()\n        stmt = select(subq).where(subq.c.x > 2)\n\n        self.assert_compile(\n            stmt,\n            \"SELECT anon_1.x, anon_1.y FROM \"\n            \"(SELECT x, y FROM \"\n            \"generate_series(:generate_series_1, :generate_series_2) AS x, \"\n            \"generate_series(:generate_series_3, :generate_series_4) AS y\"\n            \") AS anon_1 \"\n            \"WHERE anon_1.x > :x_1\",\n        )\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "test_render_derived_with_lateral",
      "md_content": [
        "**test_render_derived_with_lateral**: The function of `test_render_derived_with_lateral` is to test the rendering of a SQL query that involves a LATERAL join with a derived table, ensuring the correct SQL syntax is generated and compiled.\n\n**parameters**: The parameters of this Function.\n· `apply_alias_after_lateral`: A boolean parameter that determines whether the alias for the derived table is applied after the LATERAL join or directly within the LATERAL join.\n\n**Code Description**: The description of this Function.\nThe `test_render_derived_with_lateral` function is designed to test the generation of a SQL query that includes a LATERAL join with a derived table. The function constructs a SQL query that selects specific columns from two tables (`table1` and `table2`) and a derived table (`jsonb_table`) created using the `jsonb_to_recordset` function. The derived table is generated from a JSONB column in `table1` and includes columns for `name` (of type `Text`) and `time` (of type `Float`).\n\nThe function allows for two different ways of applying the alias to the derived table:\n1. If `apply_alias_after_lateral` is `True`, the alias is applied after the derived table is rendered and the LATERAL join is created.\n2. If `apply_alias_after_lateral` is `False`, the alias is applied directly within the LATERAL join.\n\nThe SQL query is constructed using the `select` function, which specifies the columns to be selected, including the `user_id` from `table1`, the `name` from `table2`, the `name` from the derived table (`jsonb_table`), and a count of the `time` column from the derived table. The query includes joins between `table1` and `table2` on the `user_id` and `id` columns, and a LATERAL join with the derived table. The query also includes a `WHERE` clause to filter results based on the `route_id` in `table2` and the `name` in the derived table. The results are grouped by `user_id`, `name` from `table2`, and `name` from the derived table, and ordered by the `name` from `table2`.\n\nThe function then uses `self.assert_compile` to verify that the generated SQL query matches the expected SQL syntax, including the correct use of LATERAL joins, aliases, and filtering conditions.\n\n**Note**: Points to note about the use of the code\n- The function is primarily used for testing the SQL query generation and compilation process, particularly focusing on the correct handling of LATERAL joins and derived tables.\n- The `apply_alias_after_lateral` parameter allows for testing different ways of applying aliases to derived tables, which can be useful for ensuring compatibility with different SQL dialects or query optimization strategies.\n- The `literal_binds=True` and `render_postcompile=True` options in `self.assert_compile` ensure that the SQL query is rendered with literal values and post-compilation rendering, which is important for accurate testing of the query generation process."
      ],
      "code_start_line": 1697,
      "code_end_line": 1771,
      "params": [
        "self",
        "apply_alias_after_lateral"
      ],
      "have_return": false,
      "code_content": "    def test_render_derived_with_lateral(self, apply_alias_after_lateral):\n        \"\"\"\n        # this is the \"record\" type\n\n        SELECT\n            table1.user_id AS table1_user_id,\n            table2.name AS table2_name,\n            jsonb_table.name AS jsonb_table_name,\n            count(jsonb_table.time) AS count_1\n            FROM table1\n            JOIN table2 ON table1.user_id = table2.id\n            JOIN LATERAL jsonb_to_recordset(table1.jsonb)\n            AS jsonb_table(name TEXT, time FLOAT) ON true\n            WHERE table2.route_id = %(route_id_1)s\n            AND jsonb_table.name IN (%(name_1)s, %(name_2)s, %(name_3)s)\n            GROUP BY table1.user_id, table2.name, jsonb_table.name\n            ORDER BY table2.name\n\n        \"\"\"  # noqa\n\n        table1 = table(\"table1\", column(\"user_id\"), column(\"jsonb\"))\n        table2 = table(\n            \"table2\", column(\"id\"), column(\"name\"), column(\"route_id\")\n        )\n        jsonb_table = func.jsonb_to_recordset(table1.c.jsonb).table_valued(\n            column(\"name\", Text), column(\"time\", Float)\n        )\n\n        # I'm a little concerned about the naming, that lateral() and\n        # alias() both make a new name unconditionally.  lateral() already\n        # works this way, so try to just make sure .alias() after the\n        # fact works too\n        if apply_alias_after_lateral:\n            jsonb_table = (\n                jsonb_table.render_derived(with_types=True)\n                .lateral()\n                .alias(\"jsonb_table\")\n            )\n        else:\n            jsonb_table = jsonb_table.render_derived(with_types=True).lateral(\n                \"jsonb_table\"\n            )\n\n        stmt = (\n            select(\n                table1.c.user_id,\n                table2.c.name,\n                jsonb_table.c.name.label(\"jsonb_table_name\"),\n                func.count(jsonb_table.c.time),\n            )\n            .select_from(table1)\n            .join(table2, table1.c.user_id == table2.c.id)\n            .join(jsonb_table, true())\n            .where(table2.c.route_id == 5)\n            .where(jsonb_table.c.name.in_([\"n1\", \"n2\", \"n3\"]))\n            .group_by(table1.c.user_id, table2.c.name, jsonb_table.c.name)\n            .order_by(table2.c.name)\n        )\n\n        self.assert_compile(\n            stmt,\n            \"SELECT table1.user_id, table2.name, \"\n            \"jsonb_table.name AS jsonb_table_name, \"\n            \"count(jsonb_table.time) AS count_1 \"\n            \"FROM table1 \"\n            \"JOIN table2 ON table1.user_id = table2.id \"\n            \"JOIN LATERAL jsonb_to_recordset(table1.jsonb) \"\n            \"AS jsonb_table(name TEXT, time FLOAT) ON true \"\n            \"WHERE table2.route_id = 5 \"\n            \"AND jsonb_table.name IN ('n1', 'n2', 'n3') \"\n            \"GROUP BY table1.user_id, table2.name, jsonb_table.name \"\n            \"ORDER BY table2.name\",\n            literal_binds=True,\n            render_postcompile=True,\n        )\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "test_function_alias",
      "md_content": [
        "**test_function_alias**: The function of test_function_alias is to test the SQL query generation involving table aliases and JSON functions in SQLAlchemy.\n\n**parameters**: This function does not take any external parameters. It operates within the context of the test class and uses internally defined variables and constructs.\n\n**Code Description**: \nThe function `test_function_alias` is designed to test the compilation of a SQL query that involves table aliases and JSON functions. The query is constructed using SQLAlchemy's ORM and core components. Here is a detailed breakdown of the code:\n\n1. **Table Definition**: The function starts by defining a table named \"check\" with columns `id` and `response` (of type JSON). This table is aliased twice as `check_inside` and `check_outside` to simulate a self-referential query scenario.\n\n2. **Subquery Construction**: A subquery is created using the `select` statement. This subquery selects the `response[\"Results\"]` field from the `check_inside` table where the `id` matches the `id` of the `check_outside` table. The result of this subquery is then used as a scalar subquery.\n\n3. **JSON Function Application**: The `json_array_elements` function is applied to the scalar subquery, and the result is aliased as `result_elem`. This function is used to extract elements from a JSON array.\n\n4. **Main Query Construction**: The main query is constructed using the `select` statement. It selects the `Field` attribute from the `result_elem` JSON object and labels it as `field`. The query includes a `where` clause to filter the results where the `Name` attribute of the `result_elem` JSON object equals `'FooBar'`.\n\n5. **Assertion**: The function concludes by asserting that the compiled SQL statement matches the expected SQL string. This ensures that the SQLAlchemy query construction behaves as intended.\n\n**Note**: \n- The function is part of a test suite and is used to verify the correctness of SQL query generation involving JSON functions and table aliases.\n- The use of `json_array_elements` and JSON field extraction (`->` and `->>`) is specific to PostgreSQL. Ensure that the database being used supports these JSON functions.\n- The aliasing of tables (`check_inside` and `check_outside`) is crucial for self-referential queries and should be handled carefully to avoid confusion in more complex scenarios."
      ],
      "code_start_line": 1773,
      "code_end_line": 1816,
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def test_function_alias(self):\n        \"\"\"\n        .. sourcecode:: sql\n\n            SELECT result_elem -> 'Field' as field\n            FROM \"check\" AS check_, json_array_elements(\n            (\n                SELECT check_inside.response -> 'Results'\n                FROM \"check\" as check_inside\n                WHERE check_inside.id = check_.id\n            )\n            ) AS result_elem\n            WHERE result_elem ->> 'Name' = 'FooBar'\n\n        \"\"\"\n        check = table(\"check\", column(\"id\"), column(\"response\", JSON))\n\n        check_inside = check.alias(\"check_inside\")\n        check_outside = check.alias(\"_check\")\n\n        subq = (\n            select(check_inside.c.response[\"Results\"])\n            .where(check_inside.c.id == check_outside.c.id)\n            .scalar_subquery()\n        )\n\n        fn = func.json_array_elements(subq, type_=JSON).alias(\"result_elem\")\n\n        stmt = (\n            select(fn.column[\"Field\"].label(\"field\"))\n            .where(fn.column[\"Name\"] == \"FooBar\")\n            .select_from(check_outside)\n        )\n\n        self.assert_compile(\n            stmt,\n            \"SELECT result_elem[:result_elem_1] AS field \"\n            'FROM \"check\" AS _check, json_array_elements('\n            \"(SELECT check_inside.response[:response_1] AS anon_1 \"\n            'FROM \"check\" AS check_inside '\n            \"WHERE check_inside.id = _check.id)\"\n            \") AS result_elem \"\n            \"WHERE result_elem[:result_elem_2] = :param_1\",\n        )\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "test_named_table_valued",
      "md_content": [
        "**test_named_table_valued**: The function of test_named_table_valued is to test the compilation of a SQL statement that uses a named table-valued function with specific column definitions.\n\n**parameters**: This function does not take any parameters.\n\n**Code Description**: \nThe function `test_named_table_valued` is designed to verify the correct compilation of a SQL statement that utilizes a table-valued function. The function begins by defining a table-valued function using `func.json_to_recordset`, which converts a JSON string into a set of records. The JSON string provided is `'[{\"a\":1,\"b\":\"foo\"},{\"a\":\"2\",\"c\":\"bar\"}]'`. \n\nThe `table_valued` method is then called on the result of `func.json_to_recordset`, specifying the columns `a` and `b` with their respective data types (`Integer` for `a` and `String` for `b`). The `render_derived` method is used to render the derived table with the specified column types.\n\nNext, a SQL `select` statement is constructed to select columns `a` and `b` from the derived table. The `assert_compile` method is used to compare the compiled SQL statement with the expected SQL string. The expected SQL string is:\n```\nSELECT anon_1.a, anon_1.b \nFROM json_to_recordset(:json_to_recordset_1) \nAS anon_1(a INTEGER, b VARCHAR)\n```\nThis ensures that the SQL statement is correctly compiled with the appropriate column definitions and table alias.\n\n**Note**: \n- The function is primarily used for testing purposes to ensure that the SQL compilation process works as expected when using table-valued functions with specific column definitions.\n- The JSON string used in the function is hardcoded for testing and should be replaced with dynamic data in a real-world scenario.\n- The `assert_compile` method is crucial for verifying the correctness of the SQL compilation, and any changes to the SQL structure should be reflected in the expected output string."
      ],
      "code_start_line": 1818,
      "code_end_line": 1834,
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def test_named_table_valued(self):\n        fn = (\n            func.json_to_recordset(  # noqa\n                '[{\"a\":1,\"b\":\"foo\"},{\"a\":\"2\",\"c\":\"bar\"}]'\n            )\n            .table_valued(column(\"a\", Integer), column(\"b\", String))\n            .render_derived(with_types=True)\n        )\n\n        stmt = select(fn.c.a, fn.c.b)\n\n        self.assert_compile(\n            stmt,\n            \"SELECT anon_1.a, anon_1.b \"\n            \"FROM json_to_recordset(:json_to_recordset_1) \"\n            \"AS anon_1(a INTEGER, b VARCHAR)\",\n        )\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "test_named_table_valued_w_quoting",
      "md_content": [
        "**test_named_table_valued_w_quoting**: The function of `test_named_table_valued_w_quoting` is to test the functionality of creating and querying a named table-valued function with quoted column names in SQLAlchemy.\n\n**parameters**: This function does not take any parameters.\n\n**Code Description**: \nThe function begins by defining a table-valued function using `func.json_to_recordset`, which converts a JSON array into a set of records. The JSON array provided contains two objects, each with two fields: `\"CaseSensitive\"` and `\"the % value\"`. The `table_valued` method is then used to define the structure of the resulting table, specifying the column names and their respective data types (`Integer` for `\"CaseSensitive\"` and `String` for `\"the % value\"`). The `render_derived` method is called with `with_types=True` to ensure that the column types are included in the generated SQL.\n\nNext, a SQL `select` statement is constructed to query the table-valued function, specifically selecting the `\"CaseSensitive\"` and `\"the % value\"` columns. The `assert_compile` method is used to verify that the generated SQL matches the expected output. The expected SQL includes the correct quoting of column names and the proper structure of the table-valued function call.\n\n**Note**: \n- The function is designed to test the correct handling of quoted column names, especially those containing special characters like `%`.\n- The `render_derived` method with `with_types=True` ensures that the column types are explicitly included in the SQL, which is important for type safety and correctness in the generated SQL.\n- This test is crucial for ensuring that SQLAlchemy correctly handles and compiles table-valued functions with complex column names."
      ],
      "code_start_line": 1836,
      "code_end_line": 1855,
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def test_named_table_valued_w_quoting(self):\n        fn = (\n            func.json_to_recordset(  # noqa\n                '[{\"CaseSensitive\":1,\"the % value\":\"foo\"}, '\n                '{\"CaseSensitive\":\"2\",\"the % value\":\"bar\"}]'\n            )\n            .table_valued(\n                column(\"CaseSensitive\", Integer), column(\"the % value\", String)\n            )\n            .render_derived(with_types=True)\n        )\n\n        stmt = select(fn.c.CaseSensitive, fn.c[\"the % value\"])\n\n        self.assert_compile(\n            stmt,\n            'SELECT anon_1.\"CaseSensitive\", anon_1.\"the % value\" '\n            \"FROM json_to_recordset(:json_to_recordset_1) \"\n            'AS anon_1(\"CaseSensitive\" INTEGER, \"the % value\" VARCHAR)',\n        )\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "test_named_table_valued_subquery",
      "md_content": [
        "**test_named_table_valued_subquery**: The function of test_named_table_valued_subquery is to test the compilation of a SQL query that uses a named table-valued subquery derived from a JSON-to-recordset function.\n\n**parameters**: This function does not take any parameters.\n\n**Code Description**: \nThe function `test_named_table_valued_subquery` is designed to test the SQL compilation of a query that involves a named table-valued subquery. The process begins by using the `func.json_to_recordset` function to convert a JSON string into a set of records. The JSON string provided is `'[{\"a\":1,\"b\":\"foo\"},{\"a\":\"2\",\"c\":\"bar\"}]'`, which contains two objects with different key-value pairs.\n\nThe `table_valued` method is then applied to the result of `func.json_to_recordset`, specifying the columns `a` and `b` with their respective data types (`Integer` for `a` and `String` for `b`). The `render_derived` method is called with the `with_types=True` argument, which ensures that the derived table includes the specified column types in the resulting SQL.\n\nNext, a `select` statement is created to query the columns `a` and `b` from the derived table. This query is then wrapped in another `select` statement, effectively creating a subquery. The final SQL query is expected to select columns `a` and `b` from the derived table, which is itself a result of the `json_to_recordset` function.\n\nThe `self.assert_compile` method is used to verify that the compiled SQL matches the expected output. The expected SQL string includes the selection of columns `a` and `b` from the derived table, which is nested within another subquery. The derived table is named `anon_2`, and the outer subquery is named `anon_1`.\n\n**Note**: \n- The function is part of a test suite, so it is primarily used for verifying the correctness of SQL query compilation rather than for production use.\n- The JSON string used in the function is hardcoded, and the function assumes that the JSON structure will always match the specified columns and types.\n- The `with_types=True` argument in `render_derived` ensures that the column types are explicitly included in the SQL, which is important for type safety and correctness in the generated query."
      ],
      "code_start_line": 1857,
      "code_end_line": 1877,
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def test_named_table_valued_subquery(self):\n        fn = (\n            func.json_to_recordset(  # noqa\n                '[{\"a\":1,\"b\":\"foo\"},{\"a\":\"2\",\"c\":\"bar\"}]'\n            )\n            .table_valued(column(\"a\", Integer), column(\"b\", String))\n            .render_derived(with_types=True)\n        )\n\n        stmt = select(fn.c.a, fn.c.b).subquery()\n\n        stmt = select(stmt)\n\n        self.assert_compile(\n            stmt,\n            \"SELECT anon_1.a, anon_1.b FROM \"\n            \"(SELECT anon_2.a AS a, anon_2.b AS b \"\n            \"FROM json_to_recordset(:json_to_recordset_1) \"\n            \"AS anon_2(a INTEGER, b VARCHAR)\"\n            \") AS anon_1\",\n        )\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "test_named_table_valued_alias",
      "md_content": [
        "**test_named_table_valued_alias**: The function of `test_named_table_valued_alias` is to test the compilation of a SQL statement that uses a named table-valued function with an alias, specifically focusing on the `json_to_recordset` function and its derived table structure.\n\n**parameters**: This function does not take any parameters as it is a test method within a class.\n\n**Code Description**: \nThe function begins by defining a SQL-like comment that describes the intended query: selecting data from a JSON array converted into a table-like structure using the `json_to_recordset` function. The JSON array contains objects with keys `\"a\"` and `\"b\"`, and the query specifies the data types for these columns.\n\nThe code then constructs a table-valued function using `func.json_to_recordset`, which takes a JSON string as input. The JSON string represents an array of objects, each containing key-value pairs. The `table_valued` method is used to define the structure of the resulting table, specifying the columns `a` (of type `Integer`) and `b` (of type `String`). The `render_derived` method is called with `with_types=True` to ensure that the column types are included in the generated SQL. Finally, the `alias` method assigns the alias `\"jbr\"` to the derived table.\n\nA SQL `SELECT` statement is then created using the `select` function, targeting the columns `a` and `b` from the aliased table `jbr`. The `assert_compile` method is used to verify that the generated SQL matches the expected output. The expected SQL includes the alias `jbr` and explicitly specifies the column types (`INTEGER` and `VARCHAR`) in the derived table definition.\n\n**Note**: \n- The `json_to_recordset` function is used to convert a JSON array into a table-like structure, which is particularly useful for querying JSON data directly in SQL.\n- The `table_valued` method allows defining the structure of the resulting table, including column names and types.\n- The `render_derived` method with `with_types=True` ensures that the column types are included in the generated SQL, which is important for type safety and clarity.\n- The `alias` method is used to assign a name to the derived table, making it easier to reference in the SQL query.\n- This test ensures that the SQL compilation process correctly handles named table-valued functions with aliases and properly includes column types in the output."
      ],
      "code_start_line": 1879,
      "code_end_line": 1899,
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def test_named_table_valued_alias(self):\n        \"\"\"select * from json_to_recordset\n        ('[{\"a\":1,\"b\":\"foo\"},{\"a\":\"2\",\"c\":\"bar\"}]') as x(a int, b text);\"\"\"\n\n        fn = (\n            func.json_to_recordset(  # noqa\n                '[{\"a\":1,\"b\":\"foo\"},{\"a\":\"2\",\"c\":\"bar\"}]'\n            )\n            .table_valued(column(\"a\", Integer), column(\"b\", String))\n            .render_derived(with_types=True)\n            .alias(\"jbr\")\n        )\n\n        stmt = select(fn.c.a, fn.c.b)\n\n        self.assert_compile(\n            stmt,\n            \"SELECT jbr.a, jbr.b \"\n            \"FROM json_to_recordset(:json_to_recordset_1) \"\n            \"AS jbr(a INTEGER, b VARCHAR)\",\n        )\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    }
  ],
  "dataset/bulk_inserts.py": [
    {
      "type": "ClassDef",
      "name": "Customer",
      "md_content": [
        "**Customer**: The function of Customer is to represent a customer entity in the database, storing essential details such as the customer's name and description.\n\n**attributes**: The attributes of this Class.\n· id: An integer value that serves as the primary key for the customer record. It is auto-generated using the `Identity()` function.\n· name: A string value with a maximum length of 255 characters, representing the name of the customer.\n· description: A string value with a maximum length of 255 characters, providing additional details about the customer.\n\n**Code Description**: The `Customer` class is a SQLAlchemy model that maps to the `customer` table in the database. It inherits from the `Base` class, which is typically the declarative base for SQLAlchemy models. The class defines three columns: `id`, `name`, and `description`. The `id` column is the primary key and is automatically generated using the `Identity()` function, ensuring unique identification for each customer record. The `name` and `description` columns store textual information about the customer.\n\nIn the project, the `Customer` class is used extensively in various test functions to demonstrate different methods of inserting data into the database. For example:\n- In `test_flush_no_pk`, the `Customer` objects are created without specifying the `id`, and the database generates the primary key automatically.\n- In `test_flush_pk_given`, the `id` is explicitly provided when creating `Customer` objects, simulating scenarios where primary keys are predefined.\n- In `test_orm_bulk_insert`, `Customer` objects are inserted in bulk using the ORM without returning the inserted rows.\n- In `test_orm_insert_returning`, bulk insertion is performed, and the newly created `Customer` objects are returned.\n- In `test_core_insert`, a single Core INSERT statement is used to insert multiple `Customer` records in bulk.\n- In `test_dbapi_raw`, the DBAPI's raw cursor is used to insert `Customer` records in bulk, demonstrating a lower-level approach to database operations.\n\nThese test functions highlight the flexibility of the `Customer` class in handling different database insertion strategies, making it a versatile component in the project.\n\n**Note**: When using the `Customer` class, ensure that the `name` and `description` fields do not exceed the maximum length of 255 characters. Additionally, when inserting records, be mindful of whether the `id` should be auto-generated or explicitly provided, as this affects the behavior of the database operations."
      ],
      "code_start_line": 23,
      "code_end_line": 27,
      "params": [],
      "have_return": false,
      "code_content": "class Customer(Base):\n    __tablename__ = \"customer\"\n    id = Column(Integer, Identity(), primary_key=True)\n    name = Column(String(255))\n    description = Column(String(255))\n",
      "name_column": 6,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "dataset/bulk_inserts.py/test_flush_no_pk",
        "dataset/bulk_inserts.py/test_flush_pk_given",
        "dataset/bulk_inserts.py/test_orm_bulk_insert",
        "dataset/bulk_inserts.py/test_orm_insert_returning",
        "dataset/bulk_inserts.py/test_core_insert",
        "dataset/bulk_inserts.py/test_dbapi_raw"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "setup_database",
      "md_content": [
        "**setup_database**: The function of setup_database is to initialize and configure a database by creating or recreating its schema based on the provided database URL and settings.\n\n**parameters**: The parameters of this Function.\n· dburl: A string representing the database URL, which specifies the connection details for the database (e.g., dialect, driver, username, password, host, port, and database name).\n· echo: A boolean flag that determines whether SQL statements executed by the engine should be logged to the console. If set to True, SQL statements will be printed; if False, they will not.\n· num: This parameter is defined but not used in the function. It may be intended for future extensions or modifications to the function.\n\n**Code Description**: The description of this Function.\nThe `setup_database` function performs the following steps:\n1. It creates a global SQLAlchemy engine object using the `create_engine` function, which is initialized with the provided `dburl` and `echo` parameters. The engine is responsible for managing database connections and executing SQL statements.\n2. It drops all existing tables in the database associated with the engine using `Base.metadata.drop_all(engine)`. This ensures that any previous schema is removed before creating a new one.\n3. It creates all tables defined in the SQLAlchemy `Base` metadata using `Base.metadata.create_all(engine)`. This step establishes the database schema based on the ORM models associated with `Base`.\n\nThe function is typically used to reset or initialize a database schema, ensuring that the database is in a clean state with the latest schema definitions.\n\n**Note**: Points to note about the use of the code\n- The `num` parameter is currently unused, so it can be omitted or repurposed in future updates.\n- Dropping all tables with `Base.metadata.drop_all(engine)` will result in the loss of all data in the database. Use this function with caution in production environments.\n- Ensure that the `Base` object is properly defined and includes all necessary ORM models before calling this function."
      ],
      "code_start_line": 34,
      "code_end_line": 38,
      "params": [
        "dburl",
        "echo",
        "num"
      ],
      "have_return": false,
      "code_content": "def setup_database(dburl, echo, num):\n    global engine\n    engine = create_engine(dburl, echo=echo)\n    Base.metadata.drop_all(engine)\n    Base.metadata.create_all(engine)\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "test_flush_no_pk",
      "md_content": [
        "**test_flush_no_pk**: The function of test_flush_no_pk is to demonstrate the insertion of multiple customer records into a database using the ORM (Object-Relational Mapping) in batches, without specifying primary keys, and fetching generated row IDs if available.\n\n**parameters**: The parameters of this Function.\n· n: An integer representing the total number of customer records to be inserted into the database.\n\n**Code Description**: The `test_flush_no_pk` function is designed to insert a large number of customer records into the database in a batched manner. It achieves this by creating a session connected to the database engine and iterating over the range of records in chunks of 1000. For each chunk, the function creates a list of `Customer` objects, each initialized with a unique name and description. These objects are then added to the session using `session.add_all()`. After adding each batch, the function calls `session.flush()` to synchronize the session with the database, ensuring that the records are temporarily written to the database and that any generated primary keys (if applicable) are fetched. Finally, the function commits the transaction using `session.commit()` to permanently save the records to the database.\n\nThe function leverages the `Customer` class, which represents a customer entity in the database. The `Customer` class includes attributes such as `id` (auto-generated primary key), `name`, and `description`. In this function, the `id` is not explicitly provided, allowing the database to generate it automatically. This approach is useful for scenarios where primary keys are managed by the database, and the application does not need to specify them.\n\n**Note**: When using this function, ensure that the database supports auto-generated primary keys (e.g., using `Identity()` or similar mechanisms). Additionally, the function assumes that the database can handle large batch inserts efficiently. If the database or ORM configuration does not support batch operations or RETURNING clauses for fetching generated IDs, the function may need to be adjusted accordingly."
      ],
      "code_start_line": 42,
      "code_end_line": 57,
      "params": [
        "n"
      ],
      "have_return": false,
      "code_content": "def test_flush_no_pk(n):\n    \"\"\"INSERT statements via the ORM (batched with RETURNING if available),\n    fetching generated row id\"\"\"\n    session = Session(bind=engine)\n    for chunk in range(0, n, 1000):\n        session.add_all(\n            [\n                Customer(\n                    name=\"customer name %d\" % i,\n                    description=\"customer description %d\" % i,\n                )\n                for i in range(chunk, chunk + 1000)\n            ]\n        )\n        session.flush()\n    session.commit()\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "dataset/bulk_inserts.py/Customer"
      ],
      "special_reference_type": [
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "test_flush_pk_given",
      "md_content": [
        "**test_flush_pk_given**: The function of test_flush_pk_given is to demonstrate batched INSERT operations using the ORM (Object-Relational Mapping) with predefined primary keys (PKs) for the `Customer` records.\n\n**parameters**: The parameters of this Function.\n· n: An integer representing the total number of `Customer` records to be inserted into the database.\n\n**Code Description**: The `test_flush_pk_given` function performs batched INSERT operations for `Customer` records using SQLAlchemy's ORM. The function takes an integer `n` as input, which specifies the total number of records to be inserted. The records are inserted in batches of 1000 to optimize performance and manage memory usage effectively.\n\nThe function begins by creating a new SQLAlchemy session connected to the database engine. It then iterates over the range of `n` in increments of 1000, creating a list of `Customer` objects for each batch. Each `Customer` object is instantiated with a predefined `id` (primary key), `name`, and `description`. The `id` is explicitly set to `i + 1`, where `i` is the current iteration index, ensuring that each record has a unique primary key. The `name` and `description` fields are populated with dynamically generated strings.\n\nAfter creating the batch of `Customer` objects, the function adds them to the session using `session.add_all()` and immediately flushes the session with `session.flush()`. This ensures that the records are sent to the database but not yet committed, allowing for further operations within the same transaction. Once all batches are processed, the function commits the transaction using `session.commit()`, finalizing the insertion of all records into the database.\n\nThis function is particularly useful for scenarios where primary keys are predefined and need to be explicitly set during the insertion process. It showcases how to efficiently insert large volumes of data in batches while maintaining control over primary key assignment.\n\n**Note**: When using this function, ensure that the `id` values provided for the `Customer` records are unique to avoid primary key conflicts. Additionally, the batch size of 1000 can be adjusted based on system performance and memory constraints. This function assumes that the `Customer` table and its corresponding ORM model are properly configured in the database."
      ],
      "code_start_line": 61,
      "code_end_line": 76,
      "params": [
        "n"
      ],
      "have_return": false,
      "code_content": "def test_flush_pk_given(n):\n    \"\"\"Batched INSERT statements via the ORM, PKs already defined\"\"\"\n    session = Session(bind=engine)\n    for chunk in range(0, n, 1000):\n        session.add_all(\n            [\n                Customer(\n                    id=i + 1,\n                    name=\"customer name %d\" % i,\n                    description=\"customer description %d\" % i,\n                )\n                for i in range(chunk, chunk + 1000)\n            ]\n        )\n        session.flush()\n    session.commit()\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "dataset/bulk_inserts.py/Customer"
      ],
      "special_reference_type": [
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "test_orm_bulk_insert",
      "md_content": [
        "**test_orm_bulk_insert**: The function of test_orm_bulk_insert is to perform batched INSERT statements via the ORM in bulk without returning the inserted rows.\n\n**parameters**: The parameters of this Function.\n· n: An integer representing the number of customer records to be inserted into the database.\n\n**Code Description**: The `test_orm_bulk_insert` function demonstrates how to insert multiple customer records into the database using SQLAlchemy's ORM in a bulk operation. The function begins by creating a session object bound to the database engine. It then executes a bulk insert operation using the `session.execute` method, which takes an `insert` statement for the `Customer` table and a list of dictionaries. Each dictionary in the list represents a customer record, containing the `name` and `description` fields. The `name` and `description` values are dynamically generated using a loop that runs `n` times, where `n` is the parameter passed to the function. After executing the bulk insert, the function commits the transaction using `session.commit()` to save the changes to the database.\n\nThe function leverages the `Customer` class, which is a SQLAlchemy model representing the `customer` table in the database. The `Customer` class defines the structure of the table, including the `id`, `name`, and `description` columns. In this function, the `id` column is not explicitly provided, as it is auto-generated by the database using the `Identity()` function.\n\nThis function is particularly useful for scenarios where a large number of records need to be inserted efficiently without the need to retrieve the inserted rows. It showcases the ORM's capability to handle bulk operations, which can significantly improve performance compared to inserting records one by one.\n\n**Note**: When using this function, ensure that the `n` parameter is a positive integer, as it determines the number of records to be inserted. Additionally, be aware that the function does not return any data, as it is designed for bulk insertion without returning rows.\n\n**Output Example**: Since the function does not return any value, there is no output to display. However, after execution, the `customer` table in the database will contain `n` new records with dynamically generated `name` and `description` values. For example, if `n` is 3, the table will include records with names like \"customer name 0\", \"customer name 1\", and \"customer name 2\", along with corresponding descriptions."
      ],
      "code_start_line": 80,
      "code_end_line": 93,
      "params": [
        "n"
      ],
      "have_return": true,
      "code_content": "def test_orm_bulk_insert(n):\n    \"\"\"Batched INSERT statements via the ORM in \"bulk\", not returning rows\"\"\"\n    session = Session(bind=engine)\n    session.execute(\n        insert(Customer),\n        [\n            {\n                \"name\": \"customer name %d\" % i,\n                \"description\": \"customer description %d\" % i,\n            }\n            for i in range(n)\n        ],\n    )\n    session.commit()\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "dataset/bulk_inserts.py/Customer"
      ],
      "special_reference_type": [
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "test_orm_insert_returning",
      "md_content": [
        "**test_orm_insert_returning**: The function of test_orm_insert_returning is to perform batched INSERT statements via the ORM in bulk, returning newly created Customer objects.\n\n**parameters**: The parameters of this Function.\n· n: An integer representing the number of Customer records to be inserted into the database.\n\n**Code Description**: The `test_orm_insert_returning` function demonstrates how to insert multiple `Customer` records into the database using SQLAlchemy's ORM in a bulk operation, while also returning the newly created `Customer` objects. The function begins by creating a new session using `Session(bind=engine)`, which establishes a connection to the database.\n\nThe core operation is performed using `session.scalars()`, which executes a bulk INSERT statement. The `insert(Customer).returning(Customer)` construct specifies that the INSERT operation should return the newly created `Customer` objects. The data to be inserted is provided as a list of dictionaries, where each dictionary represents a `Customer` record with `name` and `description` fields. The list comprehension generates `n` such dictionaries, each with unique values for `name` and `description`.\n\nAfter executing the INSERT statement, the function retrieves the newly created `Customer` objects using `customer_result.all()`. This step ensures that the inserted rows are converted into `Customer` objects, which can then be used for further operations if needed. Finally, the function commits the transaction using `session.commit()`, ensuring that the changes are persisted in the database.\n\nThis function is closely related to the `Customer` class, which represents the `customer` table in the database. The `Customer` class defines the structure of the records being inserted, including the `id`, `name`, and `description` fields. The `id` field is auto-generated by the database, while `name` and `description` are provided in the input data.\n\n**Note**: When using this function, ensure that the `n` parameter is a positive integer, as it determines the number of records to be inserted. Additionally, the `name` and `description` fields in the input data should not exceed 255 characters, as defined by the `Customer` class.\n\n**Output Example**: The function does not explicitly return a value, but the newly created `Customer` objects are stored in the `customers` variable. These objects would have attributes such as `id`, `name`, and `description`, with `id` being auto-generated by the database. For example, after inserting 3 records, the `customers` variable might contain:\n```\n[<Customer(id=1, name='customer name 0', description='customer description 0')>,\n <Customer(id=2, name='customer name 1', description='customer description 1')>,\n <Customer(id=3, name='customer name 2', description='customer description 2')>]\n```"
      ],
      "code_start_line": 97,
      "code_end_line": 116,
      "params": [
        "n"
      ],
      "have_return": true,
      "code_content": "def test_orm_insert_returning(n):\n    \"\"\"Batched INSERT statements via the ORM in \"bulk\", returning new Customer\n    objects\"\"\"\n    session = Session(bind=engine)\n\n    customer_result = session.scalars(\n        insert(Customer).returning(Customer),\n        [\n            {\n                \"name\": \"customer name %d\" % i,\n                \"description\": \"customer description %d\" % i,\n            }\n            for i in range(n)\n        ],\n    )\n\n    # this step is where the rows actually become objects\n    customers = customer_result.all()  # noqa: F841\n\n    session.commit()\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "dataset/bulk_inserts.py/Customer"
      ],
      "special_reference_type": [
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "test_core_insert",
      "md_content": [
        "**test_core_insert**: The function of test_core_insert is to demonstrate a single Core INSERT operation that inserts multiple customer records in bulk into the database.\n\n**parameters**: The parameters of this Function.\n· n: An integer representing the number of customer records to be inserted into the database.\n\n**Code Description**: The `test_core_insert` function performs a bulk insertion of customer records into the `customer` table using SQLAlchemy's Core INSERT construct. The function begins by establishing a database connection using the `engine.begin()` context manager, which ensures that the transaction is automatically committed upon successful execution or rolled back in case of an error. \n\nInside the transaction, the `conn.execute()` method is called with two arguments:\n1. The first argument is the INSERT statement for the `Customer` table, which is generated using `Customer.__table__.insert()`. This constructs a SQL INSERT statement targeting the `customer` table.\n2. The second argument is a list of dictionaries, where each dictionary represents a customer record to be inserted. The list comprehension generates `n` dictionaries, each containing the `name` and `description` fields for a customer. The `name` and `description` values are dynamically generated using the loop index `i`, ensuring unique values for each record.\n\nThis approach leverages SQLAlchemy's Core API to perform a bulk insert, which is efficient for inserting large numbers of records in a single operation. The function is designed to test and demonstrate the capability of SQLAlchemy's Core API to handle bulk inserts without relying on the ORM layer.\n\n**Note**: When using this function, ensure that the `n` parameter is a positive integer, as it determines the number of records to be inserted. Additionally, the `name` and `description` fields in the generated records are limited to 255 characters, as defined by the `Customer` model. This function is primarily intended for testing and demonstration purposes, showcasing the use of SQLAlchemy's Core API for bulk database operations."
      ],
      "code_start_line": 120,
      "code_end_line": 132,
      "params": [
        "n"
      ],
      "have_return": false,
      "code_content": "def test_core_insert(n):\n    \"\"\"A single Core INSERT construct inserting mappings in bulk.\"\"\"\n    with engine.begin() as conn:\n        conn.execute(\n            Customer.__table__.insert(),\n            [\n                dict(\n                    name=\"customer name %d\" % i,\n                    description=\"customer description %d\" % i,\n                )\n                for i in range(n)\n            ],\n        )\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "dataset/bulk_inserts.py/Customer"
      ],
      "special_reference_type": [
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "test_dbapi_raw",
      "md_content": [
        "**test_dbapi_raw**: The function of test_dbapi_raw is to demonstrate the DBAPI's API for inserting rows in bulk into the database using a raw cursor.\n\n**parameters**: The parameters of this Function.\n· n: An integer representing the number of rows to be inserted into the database.\n\n**Code Description**: The `test_dbapi_raw` function is designed to showcase how to perform bulk inserts into a database using the DBAPI's raw cursor. The function begins by establishing a connection to the database using `engine.pool._creator()`, which creates a new database connection. A cursor object is then created from this connection to execute SQL commands.\n\nThe function prepares an SQL INSERT statement for the `Customer` table using SQLAlchemy's `insert()` method. The `values()` method is used to specify the columns to be inserted, with `bindparam` placeholders for the `name` and `description` fields. The SQL statement is then compiled into a format suitable for the database dialect using the `compile()` method.\n\nDepending on whether the compiled SQL statement uses positional parameters or named parameters, the function generates a list of arguments (`args`) to be inserted. If positional parameters are used, `args` is a generator of tuples, where each tuple contains the `name` and `description` values for a single row. If named parameters are used, `args` is a generator of dictionaries, where each dictionary maps the column names to their respective values.\n\nThe `executemany()` method of the cursor is then called with the compiled SQL statement and the list of arguments, which inserts all the rows into the `Customer` table in a single operation. After the insertion, the transaction is committed using `conn.commit()`, ensuring that the changes are saved to the database. Finally, the connection is closed using `conn.close()` to release the database resources.\n\nThis function is particularly useful for demonstrating low-level database operations, bypassing the ORM layer and directly using the DBAPI for bulk inserts. It highlights the flexibility and efficiency of using raw SQL for large-scale data operations.\n\n**Note**: When using this function, ensure that the `Customer` table exists in the database and that the `name` and `description` fields do not exceed their maximum length of 255 characters. Additionally, be cautious with the value of `n`, as inserting a large number of rows in a single operation can impact database performance."
      ],
      "code_start_line": 136,
      "code_end_line": 163,
      "params": [
        "n"
      ],
      "have_return": false,
      "code_content": "def test_dbapi_raw(n):\n    \"\"\"The DBAPI's API inserting rows in bulk.\"\"\"\n\n    conn = engine.pool._creator()\n    cursor = conn.cursor()\n    compiled = (\n        Customer.__table__.insert()\n        .values(name=bindparam(\"name\"), description=bindparam(\"description\"))\n        .compile(dialect=engine.dialect)\n    )\n\n    if compiled.positional:\n        args = (\n            (\"customer name %d\" % i, \"customer description %d\" % i)\n            for i in range(n)\n        )\n    else:\n        args = (\n            dict(\n                name=\"customer name %d\" % i,\n                description=\"customer description %d\" % i,\n            )\n            for i in range(n)\n        )\n\n    cursor.executemany(str(compiled), list(args))\n    conn.commit()\n    conn.close()\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "dataset/bulk_inserts.py/Customer"
      ],
      "special_reference_type": [
        false
      ]
    }
  ],
  "dataset/dict_of_sets_with_default.py": [
    {
      "type": "ClassDef",
      "name": "Base",
      "md_content": [
        "**Base**: The function of Base is to serve as a foundational class for database models, providing a primary key attribute for derived classes.\n\n**attributes**: The attributes of this Class.\n· id: A primary key column of type Integer, uniquely identifying each instance of the class in the database.\n\n**Code Description**: The `Base` class is a simple yet essential component in the database modeling hierarchy. It defines a single attribute, `id`, which is a primary key column of type `Integer`. This attribute ensures that each instance of any class derived from `Base` can be uniquely identified in the database. \n\nIn the project, `Base` is inherited by three other classes: `A`, `B`, and `C`. Each of these classes extends `Base` to include additional attributes and relationships specific to their roles in the database schema. For example:\n- Class `A` uses `Base` as its parent class and adds a relationship to class `B` through the `associations` attribute.\n- Class `B` extends `Base` and introduces a foreign key relationship to class `A` (`a_id`), a relationship to class `C` (`elements`), and a `key` attribute.\n- Class `C` also inherits from `Base` and includes a foreign key relationship to class `B` (`b_id`) and a `value` attribute.\n\nBy providing the `id` attribute, `Base` ensures that all derived classes have a consistent and unique identifier, which is crucial for database operations such as querying, updating, and deleting records.\n\n**Note**: When using the `Base` class, ensure that any derived class properly defines its table name using the `__tablename__` attribute, as seen in classes `A`, `B`, and `C`. This is necessary for SQLAlchemy to correctly map the class to a database table. Additionally, the `id` attribute should not be manually set, as it is typically managed by the database system to ensure uniqueness."
      ],
      "code_start_line": 29,
      "code_end_line": 30,
      "params": [],
      "have_return": false,
      "code_content": "class Base:\n    id = Column(Integer, primary_key=True)\n",
      "name_column": 6,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "dataset/dict_of_sets_with_default.py/A",
        "dataset/dict_of_sets_with_default.py/B",
        "dataset/dict_of_sets_with_default.py/C"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "ClassDef",
      "name": "GenDefaultCollection",
      "md_content": [
        "**GenDefaultCollection**: The function of GenDefaultCollection is to automatically create and return a default value for a missing key in a dictionary-like collection.\n\n**attributes**: The attributes of this Class.\n· key: The key for which a default value is generated when it is missing in the collection.\n\n**Code Description**: The GenDefaultCollection class inherits from KeyFuncDict, which is a dictionary-like class that likely uses a key function to manage its keys. The primary functionality of GenDefaultCollection is implemented in the `__missing__` method. This method is automatically called when a key is not found in the dictionary. When a missing key is accessed, the `__missing__` method creates a new instance of a class `B` (presumably defined elsewhere in the project) using the missing key as an argument. This new instance is then stored in the dictionary under the missing key and returned. This behavior ensures that accessing a non-existent key does not raise a KeyError but instead returns a newly created object of type `B`.\n\nIn the project, GenDefaultCollection is used in the context of SQLAlchemy relationships within the class `A`. Specifically, it is used as the `collection_class` for the `associations` relationship. This means that when accessing an association in `A` that does not exist, a new instance of `B` will be automatically created and associated with the missing key. The `collections` attribute in `A` is an association proxy that bridges the `associations` relationship to the `values` attribute of `B`, allowing for easier access to the values stored in `B`.\n\n**Note**: When using GenDefaultCollection, ensure that the class `B` is properly defined and that it accepts a key as an argument in its constructor. This class is particularly useful in scenarios where you want to avoid KeyError exceptions and prefer to have default values automatically created for missing keys.\n\n**Output Example**: If you access a non-existent key `'example_key'` in an instance of GenDefaultCollection, the output might look like this:\n```python\ncollection = GenDefaultCollection()\nvalue = collection['example_key']  # This will create a new instance of B with key 'example_key'\nprint(value)  # Output: <B object at 0x...>\n```"
      ],
      "code_start_line": 36,
      "code_end_line": 39,
      "params": [],
      "have_return": true,
      "code_content": "class GenDefaultCollection(KeyFuncDict):\n    def __missing__(self, key):\n        self[key] = b = B(key)\n        return b\n",
      "name_column": 6,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "dataset/dict_of_sets_with_default.py/A"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "__missing__",
      "md_content": [
        "**__missing__**: The function of __missing__ is to handle missing keys in a dictionary-like collection by dynamically creating and returning a new instance of the `B` class.\n\n**parameters**: The parameters of this Function.\n· key: The key that was not found in the collection. This key is used to initialize a new instance of the `B` class.\n\n**Code Description**: The `__missing__` method is invoked when a key is not found in the collection. It dynamically creates a new instance of the `B` class using the missing key as an argument. The newly created instance is then stored in the collection under the same key, ensuring that subsequent accesses to this key will return the same instance. Finally, the method returns the newly created instance of `B`.\n\nThis method is particularly useful in scenarios where the collection needs to automatically generate and manage instances of `B` on-demand, rather than requiring pre-population. By leveraging the `__missing__` method, the collection ensures that missing keys are handled gracefully, and the associated `B` instances are created and stored for future use.\n\nThe relationship with the `B` class is central to this method's functionality. When a key is missing, the method initializes a `B` instance with the key, which can then be used to establish relationships or store additional data as defined by the `B` class. This dynamic behavior is essential for maintaining the integrity and usability of the collection in the context of the broader project.\n\n**Note**: Ensure that the keys used in the collection are meaningful and unique, as they directly influence the creation and management of `B` instances. Additionally, be cautious when modifying the collection, as the `__missing__` method will automatically create new instances for any missing keys, which may lead to unintended side effects if not managed properly.\n\n**Output Example**: If the key \"example_key\" is not found in the collection, the method will create a new instance of `B` with \"example_key\" as its key and return it. The returned object will be an instance of the `B` class, initialized with the provided key."
      ],
      "code_start_line": 37,
      "code_end_line": 39,
      "params": [
        "self",
        "key"
      ],
      "have_return": true,
      "code_content": "    def __missing__(self, key):\n        self[key] = b = B(key)\n        return b\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "dataset/dict_of_sets_with_default.py/B"
      ],
      "special_reference_type": [
        false
      ]
    },
    {
      "type": "ClassDef",
      "name": "A",
      "md_content": [
        "**A**: The function of A is to represent a database model with a relationship to another model (B) and provide a proxy for accessing associated values.\n\n**attributes**: The attributes of this Class.\n· __tablename__: Specifies the name of the database table associated with this class, which is \"a\".\n· associations: A relationship attribute that links instances of A to instances of B. It uses a custom collection class, GenDefaultCollection, which automatically creates a default value for a missing key.\n· collections: An association proxy that bridges the `associations` relationship to the `values` attribute of B, allowing direct access to the values stored in B.\n\n**Code Description**: The class A inherits from the Base class, which provides a primary key (`id`) for database operations. The `__tablename__` attribute defines the name of the database table as \"a\". The `associations` attribute establishes a relationship with the class B. This relationship uses a custom collection class, GenDefaultCollection, which ensures that accessing a non-existent key in the relationship automatically creates a new instance of B with the missing key. This behavior is implemented through the `__missing__` method in GenDefaultCollection.\n\nThe `collections` attribute is an association proxy that simplifies access to the `values` attribute of B. By bridging the `associations` relationship to the `values` attribute, it allows for easier manipulation and retrieval of the values stored in B. This design pattern is particularly useful in scenarios where you want to manage related objects in a database while maintaining a clean and intuitive interface for accessing their attributes.\n\n**Note**: When using class A, ensure that the class B is properly defined and that it includes a `values` attribute. The GenDefaultCollection class relies on the existence of B and its constructor, which should accept a key as an argument. This setup is essential for the automatic creation of default values when accessing missing keys in the `associations` relationship. Additionally, the `collections` proxy should be used to interact with the `values` attribute of B, as it provides a more straightforward and consistent interface for accessing these values."
      ],
      "code_start_line": 42,
      "code_end_line": 54,
      "params": [],
      "have_return": false,
      "code_content": "class A(Base):\n    __tablename__ = \"a\"\n    associations = relationship(\n        \"B\",\n        collection_class=lambda: GenDefaultCollection(\n            operator.attrgetter(\"key\")\n        ),\n    )\n\n    collections = association_proxy(\"associations\", \"values\")\n    \"\"\"Bridge the association from 'associations' over to the 'values'\n    association proxy of B.\n    \"\"\"\n",
      "name_column": 6,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "dataset/dict_of_sets_with_default.py/Base",
        "dataset/dict_of_sets_with_default.py/GenDefaultCollection"
      ],
      "special_reference_type": [
        false,
        false
      ]
    },
    {
      "type": "ClassDef",
      "name": "B",
      "md_content": [
        "**B**: The function of B is to represent a database model that establishes a relationship between class A and class C, while also providing a bridge to access the values of associated C instances.\n\n**attributes**: The attributes of this Class.\n· a_id: A foreign key column of type Integer, referencing the `id` column of class A. This attribute ensures a relationship between instances of B and A, and it is non-nullable.\n· elements: A relationship attribute that links instances of B to a collection of instances of class C. The collection is implemented as a set, ensuring uniqueness of associated C instances.\n· key: A column of type String, representing a unique identifier or label for instances of B.\n· values: An association proxy that bridges the relationship from `elements` to the `value` attribute of class C. This allows direct access to the `value` attributes of associated C instances.\n\n**Code Description**: The `B` class is a database model that extends the `Base` class, inheriting its primary key attribute `id`. It defines a foreign key relationship to class A through the `a_id` attribute, ensuring that each instance of B is associated with an instance of A. The `elements` attribute establishes a one-to-many relationship with class C, using a set as the collection type to enforce uniqueness.\n\nThe `values` attribute is an association proxy that simplifies access to the `value` attributes of associated C instances. This proxy allows developers to interact with the `value` attributes of C instances as if they were directly part of the B instance, improving code readability and usability.\n\nThe `__init__` method initializes instances of B with a `key` and an optional `values` parameter. If `values` is provided, it assigns them to the `values` attribute, which in turn populates the `elements` relationship with corresponding C instances.\n\nIn the project, `B` is used by the `__missing__` method of the `GenDefaultCollection` class. When a key is not found in the collection, this method creates a new instance of B with the missing key and returns it. This ensures that the collection dynamically creates and manages instances of B as needed.\n\n**Note**: When using the `B` class, ensure that the `key` attribute is unique and meaningful, as it serves as an identifier for instances of B. Additionally, the `values` attribute should be used with caution, as it directly manipulates the `elements` relationship. Properly managing the relationship between B and C instances is crucial for maintaining data integrity."
      ],
      "code_start_line": 57,
      "code_end_line": 70,
      "params": [],
      "have_return": false,
      "code_content": "class B(Base):\n    __tablename__ = \"b\"\n    a_id = Column(Integer, ForeignKey(\"a.id\"), nullable=False)\n    elements = relationship(\"C\", collection_class=set)\n    key = Column(String)\n\n    values = association_proxy(\"elements\", \"value\")\n    \"\"\"Bridge the association from 'elements' over to the\n    'value' element of C.\"\"\"\n\n    def __init__(self, key, values=None):\n        self.key = key\n        if values:\n            self.values = values\n",
      "name_column": 6,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "dataset/dict_of_sets_with_default.py/GenDefaultCollection/__missing__"
      ],
      "reference_who": [
        "dataset/dict_of_sets_with_default.py/Base"
      ],
      "special_reference_type": [
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [
        "**__init__**: The function of __init__ is to initialize an instance of the class with a key and optional values.\n\n**parameters**: The parameters of this Function.\n· key: The key to be assigned to the instance. This is a required parameter and is used to set the `key` attribute of the instance.\n· values: An optional parameter that represents the values to be assigned to the instance. If provided, it sets the `values` attribute of the instance. If not provided, the `values` attribute is not set during initialization.\n\n**Code Description**: The __init__ function is the constructor method for the class. It takes two parameters: `key` and `values`. The `key` parameter is mandatory and is directly assigned to the instance attribute `self.key`. The `values` parameter is optional. If it is provided (i.e., not `None`), it is assigned to the instance attribute `self.values`. If `values` is not provided, the `self.values` attribute remains unset. This allows for flexible initialization of the instance, where the `values` attribute can be set later if needed.\n\n**Note**: When using this function, ensure that the `key` parameter is always provided, as it is required for the instance to be properly initialized. The `values` parameter is optional, and its absence will not prevent the instance from being created, but the `self.values` attribute will not be set until explicitly assigned."
      ],
      "code_start_line": 67,
      "code_end_line": 70,
      "params": [
        "self",
        "key",
        "values"
      ],
      "have_return": false,
      "code_content": "    def __init__(self, key, values=None):\n        self.key = key\n        if values:\n            self.values = values\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "ClassDef",
      "name": "C",
      "md_content": [
        "**C**: The function of C is to represent a database model that stores a value and maintains a foreign key relationship with another table.\n\n**attributes**: The attributes of this Class.\n· b_id: An Integer column that serves as a foreign key referencing the `id` column of table `b`. It is marked as non-nullable, ensuring that every instance of `C` must be associated with a record in table `b`.\n· value: An Integer column that stores a numerical value associated with the instance of `C`.\n\n**Code Description**: The `C` class is a database model that inherits from the `Base` class, which provides a primary key (`id`) for uniquely identifying instances. The `C` class defines two additional attributes: `b_id` and `value`. \n\n- The `b_id` attribute is a foreign key that establishes a relationship with the `b` table. This ensures that every instance of `C` is linked to a specific record in the `b` table, enforcing referential integrity in the database.\n- The `value` attribute is an Integer column that stores a numerical value. This value is specific to each instance of `C` and can be used to represent any relevant data in the context of the application.\n\nThe `__init__` method of the `C` class initializes the `value` attribute when an instance is created. This allows for easy instantiation of `C` objects with a predefined value.\n\n**Note**: When using the `C` class, ensure that the `b_id` attribute is properly set to reference an existing record in the `b` table, as it is non-nullable. Additionally, the `value` attribute should be assigned a valid integer value during instantiation or through subsequent updates. Properly defining the `__tablename__` attribute as `\"c\"` ensures that SQLAlchemy correctly maps this class to the corresponding database table."
      ],
      "code_start_line": 73,
      "code_end_line": 79,
      "params": [],
      "have_return": false,
      "code_content": "class C(Base):\n    __tablename__ = \"c\"\n    b_id = Column(Integer, ForeignKey(\"b.id\"), nullable=False)\n    value = Column(Integer)\n\n    def __init__(self, value):\n        self.value = value\n",
      "name_column": 6,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "dataset/dict_of_sets_with_default.py/Base"
      ],
      "special_reference_type": [
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [
        "**__init__**: The function of __init__ is to initialize an instance of the class with a given value.\n\n**parameters**: The parameters of this Function.\n· value: The value to be assigned to the instance attribute `self.value`.\n\n**Code Description**: The `__init__` method is a special method in Python classes, commonly known as the constructor. It is automatically called when a new instance of the class is created. In this specific implementation, the `__init__` method takes a single parameter, `value`, and assigns it to the instance attribute `self.value`. This ensures that every instance of the class will have its own `value` attribute, initialized with the value passed during instantiation.\n\n**Note**: When creating an instance of this class, you must provide a value for the `value` parameter, as it is required for the proper initialization of the instance."
      ],
      "code_start_line": 78,
      "code_end_line": 79,
      "params": [
        "self",
        "value"
      ],
      "have_return": false,
      "code_content": "    def __init__(self, value):\n        self.value = value\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    }
  ],
  "dataset/inventory.py": [
    {
      "type": "FunctionDef",
      "name": "update_prediction_data",
      "md_content": [
        "**update_prediction_data**: The function of update_prediction_data is to fetch prediction data, process it, and write the filtered results to Cassandra.\n\n**parameters**: The parameters of this Function.\n· This function does not take any explicit parameters. It relies on predefined constants and helper functions within the module.\n\n**Code Description**: The description of this Function.\nThe update_prediction_data function performs the following steps to fetch, process, and store prediction data:\n\n1. It calls the helper function _min_daily_pageviews_by_sr with the constant NDAYS_TO_QUERY to retrieve a dictionary mapping subreddit names to their minimum daily pageviews over the specified number of days. This dictionary is stored in the variable min_daily_by_sr.\n\n2. The function then checks if the dictionary contains an entry with an empty string key (''). This key represents the front page of the platform. If such an entry exists, the function combines its value with the value of the front page subreddit (DefaultSR.name.lower()) in the dictionary. The empty key is then removed from the dictionary to clean up the data.\n\n3. The function filters the dictionary to retain only those subreddits whose minimum daily pageviews exceed 100. This is done using a dictionary comprehension that iterates over the items in min_daily_by_sr and includes only those entries where the pageview count is greater than 100. The filtered results are stored in the variable filtered.\n\n4. Finally, the function writes the filtered dictionary to Cassandra using the PromoMetrics.set method. The data is stored under the key specified by the constant MIN_DAILY_CASS_KEY.\n\nThe function relies on the helper function _min_daily_pageviews_by_sr to fetch the initial data and on the PromoMetrics.set method to persist the processed data. It ensures that only relevant subreddits (those with significant traffic) are stored, optimizing storage and retrieval efficiency.\n\n**Note**: Points to note about the use of the code\n- The function assumes that the constants NDAYS_TO_QUERY and MIN_DAILY_CASS_KEY are defined elsewhere in the codebase.\n- The function is designed to handle edge cases, such as the presence of an empty key representing the front page, ensuring data consistency.\n- The filtering threshold (100 pageviews) is hardcoded and may need adjustment based on specific use cases or traffic patterns."
      ],
      "code_start_line": 62,
      "code_end_line": 74,
      "params": [],
      "have_return": false,
      "code_content": "def update_prediction_data():\n    \"\"\"Fetch prediction data and write it to cassandra.\"\"\"\n    min_daily_by_sr = _min_daily_pageviews_by_sr(NDAYS_TO_QUERY)\n\n    # combine front page values (sometimes frontpage gets '' for its name)\n    if '' in min_daily_by_sr:\n        fp = DefaultSR.name.lower()\n        min_daily_by_sr[fp] = min_daily_by_sr.get(fp, 0) + min_daily_by_sr['']\n        del min_daily_by_sr['']\n\n    filtered = {sr_name: num for sr_name, num in min_daily_by_sr.iteritems()\n                if num > 100}\n    PromoMetrics.set(MIN_DAILY_CASS_KEY, filtered)\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "dataset/inventory.py/_min_daily_pageviews_by_sr"
      ],
      "special_reference_type": [
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "_min_daily_pageviews_by_sr",
      "md_content": [
        "**_min_daily_pageviews_by_sr**: The function of _min_daily_pageviews_by_sr is to return a dictionary mapping subreddit names to their minimum daily pageviews over a specified number of days.\n\n**parameters**: The parameters of this Function.\n· ndays: The number of days to query for minimum pageviews. Defaults to NDAYS_TO_QUERY.\n· end_date: The end date for the query range. If not provided, it defaults to one day before the last modified date of traffic data.\n\n**Code Description**: \nThe function _min_daily_pageviews_by_sr calculates the minimum daily pageviews for each subreddit over a specified number of days. It starts by determining the end date for the query range. If no end date is provided, it defaults to one day before the last modified date of the traffic data. The start date is then calculated by subtracting the number of days (ndays) from the end date.\n\nThe function queries the traffic data for pageviews by subreddit and path, filtering for daily intervals and specific time points within the calculated date range. It groups the results by subreddit path and calculates the minimum pageview count for each subreddit. The subreddit paths are then matched against a regular expression to extract the subreddit names, and the results are stored in a dictionary where the keys are subreddit names and the values are the corresponding minimum pageview counts.\n\nThis function is called by update_prediction_data, which uses the returned dictionary to filter and store subreddit pageview data in Cassandra. Specifically, update_prediction_data combines front page values and filters out subreddits with pageviews below a certain threshold before storing the data.\n\n**Note**: \n- The function relies on the traffic module to fetch traffic data and the PAGEVIEWS_REGEXP to extract subreddit names from paths.\n- The default value for ndays is set to NDAYS_TO_QUERY, which should be defined elsewhere in the codebase.\n- The function assumes that the traffic data is available and up-to-date.\n\n**Output Example**: \nA possible return value of the function could be:\n```python\n{\n    'lightpainting': 16,\n    'photography': 45,\n    'art': 120,\n    ...\n}\n```\nThis dictionary maps subreddit names to their minimum daily pageviews over the specified number of days."
      ],
      "code_start_line": 77,
      "code_end_line": 98,
      "params": [
        "ndays",
        "end_date"
      ],
      "have_return": true,
      "code_content": "def _min_daily_pageviews_by_sr(ndays=NDAYS_TO_QUERY, end_date=None):\n    \"\"\"Return dict mapping sr_name to min_pageviews over the last ndays.\"\"\"\n    if not end_date:\n        last_modified = traffic.get_traffic_last_modified()\n        end_date = last_modified - timedelta(days=1)\n    stop = end_date\n    start = stop - timedelta(ndays)\n    time_points = traffic.get_time_points('day', start, stop)\n    cls = traffic.PageviewsBySubredditAndPath\n    q = (traffic.Session.query(cls.srpath, func.min(cls.pageview_count))\n                               .filter(cls.interval == 'day')\n                               .filter(cls.date.in_(time_points))\n                               .filter(cls.srpath.like('%-GET_listing'))\n                               .group_by(cls.srpath))\n\n    # row looks like: ('lightpainting-GET_listing', 16)\n    retval = {}\n    for row in q:\n        m = PAGEVIEWS_REGEXP.match(row[0])\n        if m:\n            retval[m.group(1)] = row[1]\n    return retval\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "dataset/inventory.py/update_prediction_data"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "get_date_range",
      "md_content": [
        "**get_date_range**: The function of get_date_range is to generate a list of dates between a specified start date and end date, inclusive.\n\n**parameters**: The parameters of this Function.\n· start: The starting date of the range. This can be a date object or a string that can be converted to a date.\n· end: The ending date of the range. This can also be a date object or a string that can be converted to a date.\n\n**Code Description**: The get_date_range function takes two parameters, `start` and `end`, which represent the beginning and end of a date range, respectively. The function first converts these inputs into date objects using the `to_date` function, ensuring that both `start` and `end` are in a consistent format. It then calculates the number of days between the two dates and generates a list of dates by iterating from the start date to the end date, incrementing by one day at a time using `timedelta`. The resulting list includes all dates from the start date up to and including the end date.\n\nThis function is primarily used in the project to assist in date-related calculations, such as determining the range of dates for campaigns or available pageviews. For example, in `get_campaigns_by_date`, it is used to generate the set of dates for which campaigns are active, and in `get_available_pageviews`, it helps in calculating the date range for which pageview data is needed. The function ensures that date ranges are handled consistently across different parts of the project.\n\n**Note**: Ensure that the `start` and `end` parameters are either date objects or strings that can be parsed into dates. The function assumes that `to_date` and `timedelta` are properly imported and available in the scope where this function is used.\n\n**Output Example**: If `start` is \"2023-10-01\" and `end` is \"2023-10-03\", the function will return:\n```python\n[datetime.date(2023, 10, 1), datetime.date(2023, 10, 2), datetime.date(2023, 10, 3)]\n```"
      ],
      "code_start_line": 101,
      "code_end_line": 104,
      "params": [
        "start",
        "end"
      ],
      "have_return": true,
      "code_content": "def get_date_range(start, end):\n    start, end = map(to_date, [start, end])\n    dates = [start + timedelta(i) for i in xrange((end - start).days)]\n    return dates\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "dataset/inventory.py/get_campaigns_by_date",
        "dataset/inventory.py/get_available_pageviews"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "get_campaigns_by_date",
      "md_content": [
        "**get_campaigns_by_date**: The function of get_campaigns_by_date is to retrieve and filter promotional campaigns that are active within a specified date range for a given set of subreddits.\n\n**parameters**: The parameters of this Function.\n· srs: A collection of subreddit objects or a single subreddit object. The function retrieves campaigns associated with these subreddits.\n· start: The start date of the date range for which campaigns are to be retrieved. This can be a date object or a string that can be converted to a date.\n· end: The end date of the date range for which campaigns are to be retrieved. This can also be a date object or a string that can be converted to a date.\n· ignore: An optional parameter representing a campaign object that should be excluded from the results. If provided, the function will discard this campaign from the retrieved list.\n\n**Code Description**: The get_campaigns_by_date function retrieves promotional campaigns that are active within a specified date range for a given set of subreddits. It first converts the input subreddits into a tuple and extracts their names. Using these names, it fetches the campaign IDs associated with the subreddits within the specified date range by calling the `PromotionWeights.get_campaign_ids` function. If an `ignore` parameter is provided, the function excludes the specified campaign from the results.\n\nNext, the function retrieves the full campaign objects using the fetched campaign IDs by calling `PromoCampaign._byID`. It filters out any deleted campaigns that might still have associated `PromotionWeights`. The function then identifies transaction IDs associated with the campaigns, excluding those with `NO_TRANSACTION`. If valid transaction IDs are found, it queries the `Bid` table to retrieve the corresponding transactions and indexes them by transaction and campaign ID.\n\nThe function generates a set of dates within the specified range using the `get_date_range` function. It then iterates through the campaigns, filtering out those with no valid transaction or zero impressions. For each campaign, it checks if the associated transaction is either authorized or charged. If so, it adds the campaign to the result set for each date within the campaign's active date range that also falls within the specified date range.\n\nThis function is primarily used in the project to retrieve and filter campaigns for specific subreddits within a given date range. It is called by the `find_campaigns` function, which uses it to gather campaigns across multiple subreddits and their targeted subreddits. The function ensures that only valid, active campaigns are included in the results, making it a critical component for campaign management and analysis.\n\n**Note**: Ensure that the `start` and `end` parameters are either date objects or strings that can be parsed into dates. The function assumes that the `PromotionWeights`, `PromoCampaign`, and `Bid` classes, as well as the `get_date_range` function, are properly imported and available in the scope where this function is used.\n\n**Output Example**: If the function is called with a start date of \"2023-10-01\", an end date of \"2023-10-03\", and a set of subreddits, it might return:\n```python\n{\n    datetime.date(2023, 10, 1): {campaign1, campaign2},\n    datetime.date(2023, 10, 2): {campaign1},\n    datetime.date(2023, 10, 3): {campaign2, campaign3}\n}\n```\nHere, `campaign1`, `campaign2`, and `campaign3` are instances of the `PromoCampaign` class that are active on the respective dates."
      ],
      "code_start_line": 107,
      "code_end_line": 149,
      "params": [
        "srs",
        "start",
        "end",
        "ignore"
      ],
      "have_return": true,
      "code_content": "def get_campaigns_by_date(srs, start, end, ignore=None):\n    srs = tup(srs)\n    sr_names = [sr.name for sr in srs]\n    campaign_ids = PromotionWeights.get_campaign_ids(\n        start, end=end, sr_names=sr_names)\n    if ignore:\n        campaign_ids.discard(ignore._id)\n    campaigns = PromoCampaign._byID(campaign_ids, data=True, return_dict=False)\n\n    # filter out deleted campaigns that didn't have their PromotionWeights\n    # deleted\n    campaigns = filter(lambda camp: not camp._deleted, campaigns)\n\n    transaction_ids = {camp.trans_id for camp in campaigns\n                                     if camp.trans_id != NO_TRANSACTION}\n\n    if transaction_ids:\n        transactions = Bid.query().filter(Bid.transaction.in_(transaction_ids))\n        # index transactions by transaction and campaign id because freebies\n        # reuse the same transaction id (they always use -link id)\n        transaction_by_id = {\n            (bid.transaction, bid.campaign): bid for bid in transactions}\n    else:\n        transaction_by_id = {}\n\n    dates = set(get_date_range(start, end))\n    ret = {date: set() for date in dates}\n    for camp in campaigns:\n        if camp.trans_id == NO_TRANSACTION:\n            continue\n\n        if camp.impressions <= 0:\n            # pre-CPM campaign\n            continue\n\n        transaction = transaction_by_id[(camp.trans_id, camp._id)]\n        if not (transaction.is_auth() or transaction.is_charged()):\n            continue\n\n        camp_dates = set(get_date_range(camp.start_date, camp.end_date))\n        for date in camp_dates.intersection(dates):\n            ret[date].add(camp)\n    return ret\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "dataset/inventory.py/find_campaigns"
      ],
      "reference_who": [
        "dataset/inventory.py/get_date_range"
      ],
      "special_reference_type": [
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "get_predicted_pageviews",
      "md_content": [
        "**get_predicted_pageviews**: The function of get_predicted_pageviews is to return the predicted number of pageviews for sponsored headlines, taking into account geotargeting and default subreddit factors.\n\n**parameters**: The parameters of this Function.\n· srs: A single Subreddit object or a list of Subreddit objects for which the predicted pageviews are calculated.\n· location: An optional Location object representing the geographic targeting. If not provided, the function assumes no geographic targeting.\n\n**Code Description**: \nThe function `get_predicted_pageviews` calculates the predicted number of pageviews for sponsored headlines based on the provided subreddits (`srs`) and an optional geographic location (`location`). The prediction is influenced by two main factors: the default inventory factor for subreddits and the location factor for geotargeting.\n\n1. **Input Handling**: The function first ensures that the input `srs` is treated as a list, even if a single Subreddit object is provided. This is done using the `tup` utility, which also returns a flag (`is_single`) indicating whether the input was originally a single object.\n\n2. **Location Factor Calculation**: If a `location` is provided, the function calculates the location factor. This factor represents the proportion of pageviews for the specified location relative to the total pageviews without any geographic targeting. The factor is computed using data from `LocationPromoMetrics` for the default subreddit (`DefaultSR`). If no location is provided, the location factor defaults to 1.0, meaning no geographic targeting is applied.\n\n3. **Daily Inventory Retrieval**: The function retrieves the daily inventory of pageviews for the provided subreddits using `PromoMetrics.get`. This data represents the base pageviews without any adjustments.\n\n4. **Default Subreddit Adjustment**: For each subreddit, the function checks if it is a default subreddit (using `LocalizedDefaultSubreddits.get_global_defaults`). If it is, the function applies a different inventory factor (`DEFAULT_INVENTORY_FACTOR`). Otherwise, it uses the standard `INVENTORY_FACTOR`.\n\n5. **Final Calculation**: The predicted pageviews for each subreddit are calculated by multiplying the base pageviews by the appropriate inventory factor and the location factor. The result is then rounded to the nearest integer.\n\n6. **Return Value**: If the input was a single Subreddit object, the function returns the predicted pageviews for that subreddit. Otherwise, it returns a dictionary mapping subreddit names to their respective predicted pageviews.\n\n**Relationship with Callers**: \nThe function `get_predicted_pageviews` is called by `get_available_pageviews` to estimate the predicted pageviews for subreddits and locations. This information is used in `get_available_pageviews` to calculate the available inventory for advertising campaigns, considering both geographic targeting and subreddit-specific factors.\n\n**Note**: \n- The function assumes that the input subreddits are valid and that the necessary metrics (e.g., `PromoMetrics`, `LocationPromoMetrics`) are available.\n- The location factor calculation relies on the availability of data for the default subreddit (`DefaultSR`). If this data is missing, the location factor defaults to 0, which may affect the accuracy of the predictions.\n\n**Output Example**: \nIf the function is called with a single subreddit and a specific location, the output might look like this:\n```python\n12345\n```\nIf called with multiple subreddits and no location, the output might be:\n```python\n{\n    'subreddit1': 10000,\n    'subreddit2': 15000,\n    'subreddit3': 20000\n}\n```"
      ],
      "code_start_line": 152,
      "code_end_line": 195,
      "params": [
        "srs",
        "location"
      ],
      "have_return": true,
      "code_content": "def get_predicted_pageviews(srs, location=None):\n    \"\"\"\n    Return predicted number of pageviews for sponsored headlines.\n\n    Predicted geotargeted impressions are estimated as:\n\n    geotargeted impressions = (predicted untargeted impressions) *\n                                 (fp impressions for location / fp impressions)\n\n    \"\"\"\n\n    srs, is_single = tup(srs, ret_is_single=True)\n    sr_names = [sr.name for sr in srs]\n\n    # default subreddits require a different inventory factor\n    default_srids = LocalizedDefaultSubreddits.get_global_defaults()\n\n    if location:\n        no_location = Location(None)\n        r = LocationPromoMetrics.get(DefaultSR, [no_location, location])\n        location_pageviews = r[(DefaultSR, location)]\n        all_pageviews = r[(DefaultSR, no_location)]\n        if all_pageviews:\n            location_factor = float(location_pageviews) / float(all_pageviews)\n        else:\n            location_factor = 0.\n    else:\n        location_factor = 1.0\n\n    # prediction does not vary by date\n    daily_inventory = PromoMetrics.get(MIN_DAILY_CASS_KEY, sr_names=sr_names)\n    ret = {}\n    for sr in srs:\n        if not isinstance(sr, FakeSubreddit) and sr._id in default_srids:\n            default_factor = DEFAULT_INVENTORY_FACTOR\n        else:\n            default_factor = INVENTORY_FACTOR\n        base_pageviews = daily_inventory.get(sr.name, 0)\n        ret[sr.name] = int(base_pageviews * default_factor * location_factor)\n\n    if is_single:\n        return ret[srs[0].name]\n    else:\n        return ret\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "dataset/inventory.py/get_available_pageviews"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "make_target_name",
      "md_content": [
        "**make_target_name**: The function of make_target_name is to generate a descriptive name for a target object based on its properties.\n\n**parameters**: The parameters of this Function.\n· target: The target object for which the name is to be generated. This object must have the properties `is_collection`, `collection.name`, and `subreddit_name`.\n\n**Code Description**: \nThe `make_target_name` function constructs a name for a given target object by checking whether the target is a collection or a subreddit. If the target is a collection (i.e., `target.is_collection` is `True`), the function returns a string in the format \"collection: [collection name]\", where `[collection name]` is the name of the collection obtained from `target.collection.name`. If the target is not a collection, the function returns the `subreddit_name` of the target.\n\nThis function is primarily used in the `get_available_pageviews` function within the same project. In `get_available_pageviews`, `make_target_name` is called to generate a name for each target object, which is then used as a key in the returned dictionary. This ensures that the results are organized and easily identifiable by their descriptive names.\n\n**Note**: \n- The target object passed to this function must have the properties `is_collection`, `collection.name`, and `subreddit_name`. If any of these properties are missing or incorrectly structured, the function will raise an error.\n- The function assumes that the target is either a collection or a subreddit, and it does not handle cases where the target might be neither.\n\n**Output Example**: \nIf the target is a collection with the name \"Tech News\", the function will return:\n```\n\"collection: Tech News\"\n```\nIf the target is a subreddit with the name \"funny\", the function will return:\n```\n\"funny\"\n```"
      ],
      "code_start_line": 198,
      "code_end_line": 201,
      "params": [
        "target"
      ],
      "have_return": true,
      "code_content": "def make_target_name(target):\n    name = (\"collection: %s\" % target.collection.name if target.is_collection\n                                           else target.subreddit_name)\n    return name\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "dataset/inventory.py/get_available_pageviews"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "find_campaigns",
      "md_content": [
        "**find_campaigns**: The function of find_campaigns is to retrieve all campaigns associated with a given set of subreddits and expand the search to include campaigns targeting other subreddits linked to the initial set.\n\n**parameters**: The parameters of this Function.\n· srs: A collection of subreddit objects or a single subreddit object. The function retrieves campaigns associated with these subreddits and their targeted subreddits.\n· start: The start date of the date range for which campaigns are to be retrieved. This can be a date object or a string that can be converted to a date.\n· end: The end date of the date range for which campaigns are to be retrieved. This can also be a date object or a string that can be converted to a date.\n· ignore: An optional parameter representing a campaign object that should be excluded from the results. If provided, the function will discard this campaign from the retrieved list.\n\n**Code Description**: The find_campaigns function is designed to gather all promotional campaigns associated with a given set of subreddits and extend the search to include campaigns targeting other subreddits linked to the initial set. The function begins by initializing two sets: `all_sr_names` to store the names of all subreddits encountered during the search, and `all_campaigns` to store the campaigns retrieved.\n\nThe function processes the input subreddits (`srs`) in a loop. For each iteration, it updates the `all_sr_names` set with the names of the current subreddits. It then calls the `get_campaigns_by_date` function to retrieve campaigns active within the specified date range (`start` to `end`) for the current subreddits, excluding any campaigns specified in the `ignore` parameter. The retrieved campaigns are added to the `all_campaigns` set.\n\nNext, the function identifies new subreddit names targeted by the retrieved campaigns. These new subreddit names are added to the search if they haven't been processed before. The loop continues until no new subreddits are found to expand the search. Finally, the function returns the complete set of campaigns (`all_campaigns`) associated with the initial subreddits and their targeted subreddits.\n\nThis function is primarily used in the project to gather campaigns across multiple subreddits and their targeted subreddits. It is called by the `get_available_pageviews` function, which uses it to determine the available pageviews for a given set of targets and locations. The `find_campaigns` function ensures that all relevant campaigns are included in the results, making it a critical component for campaign management and inventory analysis.\n\n**Note**: Ensure that the `start` and `end` parameters are either date objects or strings that can be parsed into dates. The function assumes that the `Subreddit` class and the `get_campaigns_by_date` function are properly imported and available in the scope where this function is used.\n\n**Output Example**: If the function is called with a set of subreddits, a start date of \"2023-10-01\", an end date of \"2023-10-03\", and an optional `ignore` parameter, it might return:\n```python\n{campaign1, campaign2, campaign3}\n```\nHere, `campaign1`, `campaign2`, and `campaign3` are instances of the `PromoCampaign` class that are active within the specified date range and associated with the given subreddits or their targeted subreddits."
      ],
      "code_start_line": 204,
      "code_end_line": 221,
      "params": [
        "srs",
        "start",
        "end",
        "ignore"
      ],
      "have_return": true,
      "code_content": "def find_campaigns(srs, start, end, ignore):\n    \"\"\"Get all campaigns in srs and pull in campaigns in other targeted srs.\"\"\"\n    all_sr_names = set()\n    all_campaigns = set()\n    srs = set(srs)\n\n    while srs:\n        all_sr_names |= {sr.name for sr in srs}\n        new_campaigns_by_date = get_campaigns_by_date(srs, start, end, ignore)\n        new_campaigns = set(chain.from_iterable(\n            new_campaigns_by_date.itervalues()))\n        all_campaigns.update(new_campaigns)\n        new_sr_names = set(chain.from_iterable(\n            campaign.target.subreddit_names for campaign in new_campaigns\n        ))\n        new_sr_names -= all_sr_names\n        srs = set(Subreddit._by_name(new_sr_names).values())\n    return all_campaigns\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "dataset/inventory.py/get_available_pageviews"
      ],
      "reference_who": [
        "dataset/inventory.py/get_campaigns_by_date"
      ],
      "special_reference_type": [
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "get_available_pageviews",
      "md_content": [
        "**get_available_pageviews**: The function of get_available_pageviews is to return the available pageviews by date for the specified targets and location, considering all equal and higher-level location targeting constraints.\n\n**parameters**: The parameters of this Function.\n· targets: A single target object or a list of target objects representing the subreddits or collections for which available pageviews are calculated.\n· start: The start date of the date range for which pageviews are to be calculated. This can be a date object or a string that can be converted to a date.\n· end: The end date of the date range for which pageviews are to be calculated. This can also be a date object or a string that can be converted to a date.\n· location: An optional Location object representing the geographic targeting. If not provided, the function assumes no geographic targeting.\n· datestr: A boolean flag indicating whether the returned dates should be formatted as strings (default is False).\n· ignore: An optional parameter representing a campaign object that should be excluded from the calculations. If provided, the function will ignore this campaign.\n· platform: A string specifying the platform for which pageviews are calculated. Options include 'all', 'mobile', or 'desktop' (default is 'all').\n\n**Code Description**: \nThe `get_available_pageviews` function calculates the available pageviews for a given set of targets and an optional geographic location over a specified date range. The function considers all levels of location targeting, ensuring that the available inventory is constrained by the most specific location targeting applied.\n\n1. **Location Targeting Levels**: The function first assembles the levels of location targeting. If a location is provided, it includes the location and, if applicable, its higher-level components (e.g., metro and country). This ensures that the function accounts for all relevant targeting constraints.\n\n2. **Campaign Retrieval**: The function retrieves all campaigns that are directly or indirectly involved with the specified targets using the `find_campaigns` function. This includes campaigns targeting the specified subreddits and any additional subreddits linked to them.\n\n3. **Predicted Pageviews**: The function calculates the predicted pageviews for each subreddit and location combination using the `get_predicted_pageviews` function. This provides the base pageview data for further calculations.\n\n4. **Booked Impressions**: The function determines the booked impressions for each target and location on each date within the specified range. This is done by iterating through the retrieved campaigns and summing up the daily impressions for each target and location.\n\n5. **Available Pageviews Calculation**: The function calculates the available pageviews for each target and location by subtracting the booked impressions from the predicted pageviews. The available pageviews for a target are constrained by the most restrictive location targeting level.\n\n6. **Platform Adjustment**: If the `platform` parameter is specified as 'mobile' or 'desktop', the function adjusts the available pageviews based on the percentage of mobile traffic (`PERCENT_MOBILE`). This ensures that the results reflect the platform-specific inventory.\n\n7. **Return Value**: The function returns a dictionary where each target is mapped to a dictionary of dates and their corresponding available pageviews. If a single target is provided, the function returns the dictionary for that target directly.\n\n**Relationship with Callers and Callees**:\n- The function is called by `get_oversold` to determine the oversold status of a campaign by comparing the available pageviews with the daily request.\n- The function relies on `get_date_range` to generate the date range for calculations, `get_predicted_pageviews` to estimate pageviews, `find_campaigns` to retrieve relevant campaigns, and `make_target_name` to generate descriptive names for targets.\n\n**Note**: \n- Ensure that the `start` and `end` parameters are either date objects or strings that can be parsed into dates.\n- The function assumes that the `Location`, `Subreddit`, and `PromoCampaign` classes, as well as the helper functions (`get_date_range`, `get_predicted_pageviews`, `find_campaigns`, and `make_target_name`), are properly imported and available in the scope where this function is used.\n\n**Output Example**: \nIf the function is called with a single target, a start date of \"2023-10-01\", an end date of \"2023-10-03\", and no location, the output might look like this:\n```python\n{\n    \"2023-10-01\": 10000,\n    \"2023-10-02\": 12000,\n    \"2023-10-03\": 11000\n}\n```\nIf called with multiple targets and a location, the output might be:\n```python\n{\n    \"target1\": {\n        \"2023-10-01\": 8000,\n        \"2023-10-02\": 9000,\n        \"2023-10-03\": 8500\n    },\n    \"target2\": {\n        \"2023-10-01\": 7000,\n        \"2023-10-02\": 7500,\n        \"2023-10-03\": 7200\n    }\n}\n```"
      ],
      "code_start_line": 224,
      "code_end_line": 317,
      "params": [
        "targets",
        "start",
        "end",
        "location",
        "datestr",
        "ignore",
        "platform"
      ],
      "have_return": true,
      "code_content": "def get_available_pageviews(targets, start, end, location=None, datestr=False,\n                            ignore=None, platform='all'):\n    \"\"\"\n    Return the available pageviews by date for the targets and location.\n\n    Available pageviews depends on all equal and higher level locations:\n    A location is: subreddit > country > metro\n\n    e.g. if a campaign is targeting /r/funny in USA/Boston we need to check that\n    there's enough inventory in:\n    * /r/funny (all campaigns targeting /r/funny regardless of location)\n    * /r/funny + USA (all campaigns targeting /r/funny and USA with or without\n      metro level targeting)\n    * /r/funny + USA + Boston (all campaigns targeting /r/funny and USA and\n      Boston)\n    The available inventory is the smallest of these values.\n\n    \"\"\"\n\n    # assemble levels of location targeting, None means untargeted\n    locations = [None]\n    if location:\n        locations.append(location)\n\n        if location.metro:\n            locations.append(Location(country=location.country))\n\n    # get all the campaigns directly and indirectly involved in our target\n    targets, is_single = tup(targets, ret_is_single=True)\n    target_srs = list(chain.from_iterable(\n        target.subreddits_slow for target in targets))\n    all_campaigns = find_campaigns(target_srs, start, end, ignore)\n\n    # get predicted pageviews for each subreddit and location\n    all_sr_names = set(sr.name for sr in target_srs)\n    all_sr_names |= set(chain.from_iterable(\n        campaign.target.subreddit_names for campaign in all_campaigns\n    ))\n    all_srs = Subreddit._by_name(all_sr_names).values()\n    pageviews_dict = {location: get_predicted_pageviews(all_srs, location)\n                          for location in locations}\n\n    # determine booked impressions by target and location for each day\n    dates = set(get_date_range(start, end))\n    booked_dict = {}\n    for date in dates:\n        booked_dict[date] = {}\n        for location in locations:\n            booked_dict[date][location] = defaultdict(int)\n\n    for campaign in all_campaigns:\n        camp_dates = set(get_date_range(campaign.start_date, campaign.end_date))\n        sr_names = tuple(sorted(campaign.target.subreddit_names))\n        daily_impressions = campaign.impressions / campaign.ndays\n\n        for location in locations:\n            if location and not location.contains(campaign.location):\n                # campaign's location is less specific than location\n                continue\n\n            for date in camp_dates.intersection(dates):\n                booked_dict[date][location][sr_names] += daily_impressions\n\n    # calculate inventory for each target and location on each date\n    datekey = lambda dt: dt.strftime('%m/%d/%Y') if datestr else dt\n\n    ret = {}\n    for target in targets:\n        name = make_target_name(target)\n        subreddit_names = target.subreddit_names\n        ret[name] = {}\n        for date in dates:\n            pageviews_by_location = {}\n            for location in locations:\n                # calculate available impressions for each location\n                booked_by_target = booked_dict[date][location]\n                pageviews_by_sr_name = pageviews_dict[location]\n                pageviews_by_location[location] = get_maximized_pageviews(\n                    subreddit_names, booked_by_target, pageviews_by_sr_name)\n            # available pageviews is the minimum from all locations\n            min_pageviews = min(pageviews_by_location.values())\n            if PERCENT_MOBILE != 0:\n                mobile_pageviews = min_pageviews * (float(PERCENT_MOBILE) / 100)\n                if platform == 'mobile':\n                    min_pageviews = mobile_pageviews\n                if platform == 'desktop':\n                    min_pageviews = min_pageviews - mobile_pageviews\n            ret[name][datekey(date)] = max(0, min_pageviews)\n\n    if is_single:\n        name = make_target_name(targets[0])\n        return ret[name]\n    else:\n        return ret\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "dataset/inventory.py/get_oversold"
      ],
      "reference_who": [
        "dataset/inventory.py/get_date_range",
        "dataset/inventory.py/get_predicted_pageviews",
        "dataset/inventory.py/make_target_name",
        "dataset/inventory.py/find_campaigns"
      ],
      "special_reference_type": [
        false,
        false,
        false,
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "get_oversold",
      "md_content": [
        "**get_oversold**: The function of get_oversold is to identify dates within a specified range where the available pageviews for a target are insufficient to meet the daily request, indicating an oversold condition.\n\n**parameters**: The parameters of this Function.\n· target: A target object representing the subreddit or collection for which the oversold condition is being checked.\n· start: The start date of the date range for which the oversold condition is being evaluated. This can be a date object or a string that can be converted to a date.\n· end: The end date of the date range for which the oversold condition is being evaluated. This can also be a date object or a string that can be converted to a date.\n· daily_request: An integer representing the number of pageviews requested per day for the target.\n· ignore: An optional parameter representing a campaign object that should be excluded from the calculations. If provided, the function will ignore this campaign.\n· location: An optional Location object representing the geographic targeting. If not provided, the function assumes no geographic targeting.\n\n**Code Description**: The description of this Function.\nThe `get_oversold` function evaluates whether the available pageviews for a given target are sufficient to meet the daily request over a specified date range. It does this by first calling the `get_available_pageviews` function to retrieve the available pageviews for the target, start date, end date, and optional location. The function then iterates through the available pageviews by date and compares them to the daily request. If the available pageviews for a particular date are less than the daily request, that date is marked as oversold, and the available pageviews for that date are recorded in the result dictionary. The function returns a dictionary where the keys are the dates that are oversold, and the values are the corresponding available pageviews for those dates.\n\nThe function relies on `get_available_pageviews` to calculate the available pageviews for the target, considering all equal and higher-level location targeting constraints. This ensures that the oversold condition is accurately determined based on the most restrictive location targeting applied.\n\n**Note**: \n- Ensure that the `start` and `end` parameters are either date objects or strings that can be parsed into dates.\n- The function assumes that the `Location` and `Subreddit` classes, as well as the `get_available_pageviews` function, are properly imported and available in the scope where this function is used.\n\n**Output Example**: \nIf the function is called with a target, a start date of \"2023-10-01\", an end date of \"2023-10-03\", a daily request of 15000, and no location, the output might look like this:\n```python\n{\n    \"2023-10-01\": 14000,\n    \"2023-10-03\": 14500\n}\n```\nThis indicates that on October 1st and October 3rd, the available pageviews were insufficient to meet the daily request of 15000."
      ],
      "code_start_line": 320,
      "code_end_line": 327,
      "params": [
        "target",
        "start",
        "end",
        "daily_request",
        "ignore",
        "location"
      ],
      "have_return": true,
      "code_content": "def get_oversold(target, start, end, daily_request, ignore=None, location=None):\n    available_by_date = get_available_pageviews(target, start, end, location,\n                                                datestr=True, ignore=ignore)\n    oversold = {}\n    for datestr, available in available_by_date.iteritems():\n        if available < daily_request:\n            oversold[datestr] = available\n    return oversold\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "dataset/inventory.py/get_available_pageviews"
      ],
      "special_reference_type": [
        false
      ]
    }
  ],
  "dataset/conftest.py": [],
  "dataset/clip_ops.py": [
    {
      "type": "FunctionDef",
      "name": "clip_by_value",
      "md_content": [
        "**clip_by_value**: The function of clip_by_value is to clip tensor values to a specified minimum and maximum range.\n\n**parameters**: The parameters of this Function.\n· t: A `Tensor` or `IndexedSlices` whose values will be clipped.\n· clip_value_min: The minimum value to clip to. This can be a scalar `Tensor` or one that is broadcastable to the shape of `t`.\n· clip_value_max: The maximum value to clip to. This can be a scalar `Tensor` or one that is broadcastable to the shape of `t`.\n· name: An optional name for the operation.\n\n**Code Description**: \nThe `clip_by_value` function takes a tensor `t` and clips its values to the range defined by `clip_value_min` and `clip_value_max`. Any values in `t` that are less than `clip_value_min` are set to `clip_value_min`, and any values greater than `clip_value_max` are set to `clip_value_max`. The function ensures that the shape of the clipped tensor is compatible with the original tensor to prevent unintentional broadcasting. If `t` is an `IndexedSlices`, the function handles it appropriately by preserving the indices and dense shape.\n\nThe function first converts `t` to a tensor if it is an `IndexedSlices`. It then computes the minimum of the tensor values and `clip_value_max` to ensure no value exceeds `clip_value_max`. Next, it computes the maximum of the resulting tensor and `clip_value_min` to ensure no value falls below `clip_value_min`. The function asserts that the shape of the resulting tensor is compatible with the original tensor's shape. If the input `t` is an `IndexedSlices`, the function returns an `IndexedSlices` object with the clipped values, original indices, and dense shape.\n\n**Note**: \n- `clip_value_min` must be less than or equal to `clip_value_max` for correct results.\n- If the input tensor `t` is of type `int32` and the clipping values are of type `float32`, the function will raise a `TypeError`. In such cases, the input tensor should be cast to `float32` before clipping.\n- Broadcasting is allowed, but it will fail if the clip tensors would expand the dimensions of `t`.\n\n**Output Example**: \nFor example, given a tensor `t` with values `[[-10., -1., 0.], [0., 2., 10.]]`, and clipping values `clip_value_min=-1` and `clip_value_max=1`, the output will be:\n```\narray([[-1., -1.,  0.],\n       [ 0.,  1.,  1.]], dtype=float32)\n```"
      ],
      "code_start_line": 35,
      "code_end_line": 120,
      "params": [
        "t",
        "clip_value_min",
        "clip_value_max",
        "name"
      ],
      "have_return": true,
      "code_content": "def clip_by_value(t, clip_value_min, clip_value_max,\n                  name=None):\n  \"\"\"Clips tensor values to a specified min and max.\n\n  Given a tensor `t`, this operation returns a tensor of the same type and\n  shape as `t` with its values clipped to `clip_value_min` and `clip_value_max`.\n  Any values less than `clip_value_min` are set to `clip_value_min`. Any values\n  greater than `clip_value_max` are set to `clip_value_max`.\n\n  Note: `clip_value_min` needs to be smaller or equal to `clip_value_max` for\n  correct results.\n\n  For example:\n\n  Basic usage passes a scalar as the min and max value.\n\n  >>> t = tf.constant([[-10., -1., 0.], [0., 2., 10.]])\n  >>> t2 = tf.clip_by_value(t, clip_value_min=-1, clip_value_max=1)\n  >>> t2.numpy()\n  array([[-1., -1.,  0.],\n         [ 0.,  1.,  1.]], dtype=float32)\n\n  The min and max can be the same size as `t`, or broadcastable to that size.\n\n  >>> t = tf.constant([[-1, 0., 10.], [-1, 0, 10]])\n  >>> clip_min = [[2],[1]]\n  >>> t3 = tf.clip_by_value(t, clip_value_min=clip_min, clip_value_max=100)\n  >>> t3.numpy()\n  array([[ 2.,  2., 10.],\n         [ 1.,  1., 10.]], dtype=float32)\n\n  Broadcasting fails, intentionally, if you would expand the dimensions of `t`\n\n  >>> t = tf.constant([[-1, 0., 10.], [-1, 0, 10]])\n  >>> clip_min = [[[2, 1]]] # Has a third axis\n  >>> t4 = tf.clip_by_value(t, clip_value_min=clip_min, clip_value_max=100)\n  Traceback (most recent call last):\n  ...\n  InvalidArgumentError: Incompatible shapes: [2,3] vs. [1,1,2]\n\n  It throws a `TypeError` if you try to clip an `int` to a `float` value\n  (`tf.cast` the input to `float` first).\n\n  >>> t = tf.constant([[1, 2], [3, 4]], dtype=tf.int32)\n  >>> t5 = tf.clip_by_value(t, clip_value_min=-3.1, clip_value_max=3.1)\n  Traceback (most recent call last):\n  ...\n  TypeError: Cannot convert ...\n\n\n  Args:\n    t: A `Tensor` or `IndexedSlices`.\n    clip_value_min: The minimum value to clip to. A scalar `Tensor` or one that\n      is broadcastable to the shape of `t`.\n    clip_value_max: The maximum value to clip to. A scalar `Tensor` or one that\n      is broadcastable to the shape of `t`.\n    name: A name for the operation (optional).\n\n  Returns:\n    A clipped `Tensor` or `IndexedSlices`.\n\n  Raises:\n    `tf.errors.InvalidArgumentError`: If the clip tensors would trigger array\n      broadcasting that would make the returned tensor larger than the input.\n    TypeError: If dtype of the input is `int32` and dtype of\n      the `clip_value_min` or `clip_value_max` is `float32`\n  \"\"\"\n  with ops.name_scope(name, \"clip_by_value\",\n                      [t, clip_value_min, clip_value_max]) as name:\n    values = ops.convert_to_tensor(\n        t.values if isinstance(t, indexed_slices.IndexedSlices) else t,\n        name=\"t\")\n\n    # Go through list of tensors, for each value in each tensor clip\n    t_min = math_ops.minimum(values, clip_value_max)\n    # Assert that the shape is compatible with the initial shape,\n    # to prevent unintentional broadcasting.\n    values.shape.assert_is_compatible_with(t_min.shape)\n\n    t_max = math_ops.maximum(t_min, clip_value_min, name=name)\n    values.shape.assert_is_compatible_with(t_max.shape)\n\n    if isinstance(t, indexed_slices.IndexedSlices):\n      t_max = indexed_slices.IndexedSlices(t_max, t.indices, t.dense_shape)\n\n  return t_max\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "_clip_by_value_grad",
      "md_content": [
        "**_clip_by_value_grad**: The function of _clip_by_value_grad is to compute the gradients for the `clip_by_value` operation, which clips tensor values to a specified range.\n\n**parameters**: The parameters of this Function.\n· op: The operation object that contains the inputs and other attributes required for gradient computation.\n· grad: The gradient tensor backpropagated from the subsequent operations.\n\n**Code Description**: \nThe `_clip_by_value_grad` function computes the gradients for the `clip_by_value` operation, which ensures that tensor values are clipped within a specified range defined by `y` (lower bound) and `z` (upper bound). The function performs the following steps:\n\n1. **Input Extraction**: The function retrieves the inputs `x`, `y`, and `z` from the operation object `op`. These represent the original tensor, the lower bound, and the upper bound, respectively.\n\n2. **Shape and Data Type Handling**: The shapes of `x`, `y`, and `z` are computed using `array_ops.shape`. The data type of the gradient `grad` is also stored for later use.\n\n3. **Mask Creation**: Two masks are created:\n   - `xymask`: A boolean mask indicating where the values of `x` are less than the lower bound `y`.\n   - `xzmask`: A boolean mask indicating where the values of `x` are greater than the upper bound `z`.\n\n4. **Broadcast Gradient Arguments**: The function computes the reduction indices for `y` and `z` using `gen_array_ops.broadcast_gradient_args` to handle broadcasting between the shapes of `x`, `y`, and `z`.\n\n5. **Gradient Computation**:\n   - `xgrad`: The gradient for `x` is computed by setting the gradient to zero where `x` is outside the bounds (i.e., where `xymask` or `xzmask` is true). Otherwise, the original gradient `grad` is used.\n   - `ygrad`: The gradient for `y` is computed by setting the gradient to `grad` where `x` is less than `y` (i.e., where `xymask` is true). Otherwise, it is set to zero.\n   - `zgrad`: The gradient for `z` is computed by setting the gradient to `grad` where `x` is greater than `z` (i.e., where `xzmask` is true). Otherwise, it is set to zero.\n\n6. **Reduction and Reshaping**: The gradients for `y` and `z` are summed over their respective reduction indices and reshaped to match their original shapes.\n\n7. **Return Values**: The function returns the computed gradients for `x`, `y`, and `z` as `xgrad`, `gy`, and `gz`, respectively.\n\n**Note**: \n- This function is typically used internally by TensorFlow during backpropagation to compute gradients for the `clip_by_value` operation.\n- Ensure that the input tensors `x`, `y`, and `z` are compatible in terms of broadcasting to avoid errors during gradient computation.\n\n**Output Example**: \nThe function returns a tuple of three tensors:\n- `xgrad`: A tensor of the same shape as `x`, representing the gradient of `x`.\n- `gy`: A tensor of the same shape as `y`, representing the gradient of `y`.\n- `gz`: A tensor of the same shape as `z`, representing the gradient of `z`.\n\nFor example, if `x` is a tensor of shape `(3, 3)`, `y` is a scalar, and `z` is a scalar, the output might look like:\n```python\n(xgrad, gy, gz) = (\n    [[0.1, 0.2, 0.3],\n     [0.4, 0.5, 0.6],\n     [0.7, 0.8, 0.9]],\n    0.5,\n    0.8\n)\n```"
      ],
      "code_start_line": 127,
      "code_end_line": 147,
      "params": [
        "op",
        "grad"
      ],
      "have_return": true,
      "code_content": "def _clip_by_value_grad(op, grad):\n  \"\"\"Returns grad of clip_by_value.\"\"\"\n  x = op.inputs[0]\n  y = op.inputs[1]\n  z = op.inputs[2]\n  gdtype = grad.dtype\n  sx = array_ops.shape(x)\n  sy = array_ops.shape(y)\n  sz = array_ops.shape(z)\n  gradshape = array_ops.shape(grad)\n  zeros = array_ops.zeros(gradshape, gdtype)\n  xymask = math_ops.less(x, y)\n  xzmask = math_ops.greater(x, z)\n  _, ry = gen_array_ops.broadcast_gradient_args(sx, sy)\n  _, rz = gen_array_ops.broadcast_gradient_args(sx, sz)\n  xgrad = array_ops.where(math_ops.logical_or(xymask, xzmask), zeros, grad)\n  ygrad = array_ops.where(xymask, grad, zeros)\n  zgrad = array_ops.where(xzmask, grad, zeros)\n  gy = array_ops.reshape(math_ops.reduce_sum(ygrad, ry), sy)\n  gz = array_ops.reshape(math_ops.reduce_sum(zgrad, rz), sz)\n  return xgrad, gy, gz\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "clip_by_norm",
      "md_content": [
        "**clip_by_norm**: The function of clip_by_norm is to clip tensor values to a maximum L2-norm, ensuring that the L2-norm of the tensor does not exceed a specified value.\n\n**parameters**: The parameters of this Function.\n· t: A `Tensor` or `IndexedSlices` of floating point type. This is the input tensor whose values will be clipped.\n· clip_norm: A 0-D (scalar) `Tensor` greater than 0. This specifies the maximum allowable L2-norm for the tensor.\n· axes: A 1-D (vector) `Tensor` of type int32. This parameter specifies the dimensions along which the L2-norm is computed. If `None`, the L2-norm is computed over all dimensions.\n· name: An optional name for the operation.\n\n**Code Description**: \nThe `clip_by_norm` function normalizes the input tensor `t` such that its L2-norm is less than or equal to the specified `clip_norm`. If the L2-norm of `t` is already within the limit, the tensor is returned unchanged. Otherwise, the tensor values are scaled down proportionally to ensure the L2-norm of the output tensor equals `clip_norm`.\n\nThe function first converts the input tensor `t` to a standard tensor format if it is an `IndexedSlices`. It then calculates the L2-norm of the tensor along the specified axes. If the L2-norm is greater than zero, the function computes the safe L2-norm to avoid NaN gradients. The tensor values are then scaled by the ratio of `clip_norm` to the computed L2-norm. The function ensures that the shape of the intermediate tensor is compatible with the original tensor shape to prevent unintended broadcasting. Finally, the function returns the clipped tensor, preserving the structure if the input was an `IndexedSlices`.\n\n**Note**: \n- The function is commonly used to clip gradients before applying them in optimization algorithms, preventing gradient explosion.\n- The input tensor `t` must be of a floating point or complex type.\n- The `clip_norm` parameter must be a scalar tensor greater than 0.\n\n**Output Example**: \nFor an input tensor `some_nums = tf.constant([[1, 2, 3, 4, 5]], dtype=tf.float32)` and `clip_norm = 2.0`, the output might look like:\n```\narray([[0.26967996, 0.5393599 , 0.80903983, 1.0787199 , 1.3483998 ]],\n      dtype=float32)\n```"
      ],
      "code_start_line": 152,
      "code_end_line": 232,
      "params": [
        "t",
        "clip_norm",
        "axes",
        "name"
      ],
      "have_return": true,
      "code_content": "def clip_by_norm(t, clip_norm, axes=None, name=None):\n  \"\"\"Clips tensor values to a maximum L2-norm.\n\n  Given a tensor `t`, and a maximum clip value `clip_norm`, this operation\n  normalizes `t` so that its L2-norm is less than or equal to `clip_norm`,\n  along the dimensions given in `axes`. Specifically, in the default case\n  where all dimensions are used for calculation, if the L2-norm of `t` is\n  already less than or equal to `clip_norm`, then `t` is not modified. If\n  the L2-norm is greater than `clip_norm`, then this operation returns a\n  tensor of the same type and shape as `t` with its values set to:\n\n  `t * clip_norm / l2norm(t)`\n\n  In this case, the L2-norm of the output tensor is `clip_norm`.\n\n  As another example, if `t` is a matrix and `axes == [1]`, then each row\n  of the output will have L2-norm less than or equal to `clip_norm`. If\n  `axes == [0]` instead, each column of the output will be clipped.\n\n  Code example:\n\n  >>> some_nums = tf.constant([[1, 2, 3, 4, 5]], dtype=tf.float32)\n  >>> tf.clip_by_norm(some_nums, 2.0).numpy()\n  array([[0.26967996, 0.5393599 , 0.80903983, 1.0787199 , 1.3483998 ]],\n        dtype=float32)\n\n  This operation is typically used to clip gradients before applying them with\n  an optimizer.  Most gradient data is a collection of different shaped tensors\n  for different parts of the model.  Thus, this is a common usage:\n\n  ```\n  # Get your gradients after training\n  loss_value, grads = grad(model, features, labels)\n\n  # Apply some clipping\n  grads = [tf.clip_by_norm(g, norm)\n               for g in grads]\n\n  # Continue on with training\n  optimizer.apply_gradients(grads)\n  ```\n\n  Args:\n    t: A `Tensor` or `IndexedSlices`.  This must be a floating point type.\n    clip_norm: A 0-D (scalar) `Tensor` > 0. A maximum clipping value, also\n      floating point\n    axes: A 1-D (vector) `Tensor` of type int32 containing the dimensions\n      to use for computing the L2-norm. If `None` (the default), uses all\n      dimensions.\n    name: A name for the operation (optional).\n\n  Returns:\n    A clipped `Tensor` or `IndexedSlices`.\n\n  Raises:\n    ValueError: If the clip_norm tensor is not a 0-D scalar tensor.\n    TypeError: If dtype of the input is not a floating point or\n      complex type.\n  \"\"\"\n  with ops.name_scope(name, \"clip_by_norm\", [t, clip_norm]) as name:\n    values = ops.convert_to_tensor(\n        t.values if isinstance(t, indexed_slices.IndexedSlices) else t,\n        name=\"t\")\n\n    # Calculate L2-norm, clip elements by ratio of clip_norm to L2-norm\n    l2sum = math_ops.reduce_sum(values * values, axes, keepdims=True)\n    pred = l2sum > 0\n    # Two-tap tf.where trick to bypass NaN gradients\n    l2sum_safe = array_ops.where(pred, l2sum, array_ops.ones_like(l2sum))\n    l2norm = array_ops.where(pred, math_ops.sqrt(l2sum_safe), l2sum)\n    intermediate = values * clip_norm\n    # Assert that the shape is compatible with the initial shape,\n    # to prevent unintentional broadcasting.\n    values.shape.assert_is_compatible_with(intermediate.shape)\n    values_clip = array_ops.identity(\n        intermediate / math_ops.maximum(l2norm, clip_norm), name=name)\n\n    if isinstance(t, indexed_slices.IndexedSlices):\n      return indexed_slices.IndexedSlices(values_clip, t.indices, t.dense_shape)\n\n    return values_clip\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "global_norm",
      "md_content": [
        "**global_norm**: The function of global_norm is to compute the global norm of multiple tensors.\n\n**parameters**: The parameters of this Function.\n· t_list: A tuple or list of mixed `Tensors`, `IndexedSlices`, or None. This parameter represents the collection of tensors for which the global norm is to be computed.\n· name: A name for the operation (optional). This parameter allows the user to specify a name for the operation, which can be useful for debugging and visualization purposes.\n\n**Code Description**: The `global_norm` function calculates the global norm of a list of tensors. The global norm is defined as the square root of the sum of the squared L2 norms of all the tensors in the list. The function first checks if `t_list` is a valid sequence (not a string) and raises a `TypeError` if it is not. It then processes each tensor in the list, converting `IndexedSlices` to dense tensors if necessary, and computes the L2 loss (half the squared L2 norm) for each tensor. These half-squared norms are summed up, and the global norm is obtained by taking the square root of twice this sum. The function returns a scalar tensor representing the global norm.\n\nThe `global_norm` function is primarily used by the `clip_by_global_norm` function, which clips the values of multiple tensors by the ratio of the sum of their norms. In this context, `global_norm` is called to compute the global norm of the tensors, which is then used to determine the scaling factor for clipping. This relationship is crucial for gradient clipping in machine learning models, where it helps prevent exploding gradients by scaling down the gradients when their norm exceeds a specified threshold.\n\n**Note**: \n- The function ignores any `None` entries in `t_list`.\n- The function assumes that the input tensors are compatible with the operations used to compute the L2 loss and the global norm.\n- The function is designed to work with both dense tensors and `IndexedSlices`, making it versatile for different types of tensor representations.\n\n**Output Example**: The function returns a scalar tensor of type `float`. For example, if the input tensors have a combined L2 norm of 4.0, the function will return a tensor with the value `2.8284271247461903` (which is the square root of 8.0, i.e., twice the sum of the squared L2 norms)."
      ],
      "code_start_line": 238,
      "code_end_line": 285,
      "params": [
        "t_list",
        "name"
      ],
      "have_return": true,
      "code_content": "def global_norm(t_list, name=None):\n  \"\"\"Computes the global norm of multiple tensors.\n\n  Given a tuple or list of tensors `t_list`, this operation returns the\n  global norm of the elements in all tensors in `t_list`. The global norm is\n  computed as:\n\n  `global_norm = sqrt(sum([l2norm(t)**2 for t in t_list]))`\n\n  Any entries in `t_list` that are of type None are ignored.\n\n  Args:\n    t_list: A tuple or list of mixed `Tensors`, `IndexedSlices`, or None.\n    name: A name for the operation (optional).\n\n  Returns:\n    A 0-D (scalar) `Tensor` of type `float`.\n\n  Raises:\n    TypeError: If `t_list` is not a sequence.\n  \"\"\"\n  if (not isinstance(t_list, collections_abc.Sequence) or\n      isinstance(t_list, str)):\n    raise TypeError(\"`t_list` should be a sequence of tensors. Received \"\n                    f\"{type(t_list)}.\")\n  t_list = list(t_list)\n  with ops.name_scope(name, \"global_norm\", t_list) as name:\n    values = [\n        ops.convert_to_tensor(\n            t.values if isinstance(t, indexed_slices.IndexedSlices) else t,\n            name=\"t_%d\" % i) if t is not None else t\n        for i, t in enumerate(t_list)\n    ]\n    half_squared_norms = []\n    for v in values:\n      if v is not None:\n        with ops.colocate_with(v):\n          half_squared_norms.append(gen_nn_ops.l2_loss(v))\n\n    half_squared_norm = math_ops.reduce_sum(\n        array_ops_stack.stack(half_squared_norms))\n\n    norm = math_ops.sqrt(\n        half_squared_norm *\n        constant_op.constant(2.0, dtype=half_squared_norm.dtype),\n        name=\"global_norm\")\n\n  return norm\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "dataset/clip_ops.py/clip_by_global_norm"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "clip_by_global_norm",
      "md_content": [
        "**clip_by_global_norm**: The function of clip_by_global_norm is to clip the values of multiple tensors by the ratio of the sum of their norms.\n\n**parameters**: The parameters of this Function.\n· t_list: A tuple or list of mixed `Tensors`, `IndexedSlices`, or None. This parameter represents the collection of tensors whose values will be clipped based on the global norm.\n· clip_norm: A 0-D (scalar) `Tensor` greater than 0. This parameter specifies the maximum allowed norm for the tensors. If the global norm exceeds this value, the tensors will be scaled down proportionally.\n· use_norm: A 0-D (scalar) `Tensor` of type `float` (optional). This parameter allows the user to provide a precomputed global norm. If not provided, the function will compute the global norm internally using the `global_norm` function.\n· name: A name for the operation (optional). This parameter allows the user to specify a name for the operation, which can be useful for debugging and visualization purposes.\n\n**Code Description**: The `clip_by_global_norm` function is designed to clip the values of multiple tensors based on their global norm. The global norm is computed as the square root of the sum of the squared L2 norms of all tensors in `t_list`. If the global norm exceeds the specified `clip_norm`, the tensors are scaled down by the ratio of `clip_norm` to the global norm. This ensures that the combined norm of the tensors does not exceed the specified threshold.\n\nThe function first checks if `t_list` is a valid sequence (not a string) and raises a `TypeError` if it is not. If `use_norm` is not provided, the function computes the global norm using the `global_norm` function. The scaling factor for clipping is then calculated as the minimum of `1.0 / use_norm` and `1.0 / clip_norm`, ensuring that the tensors are scaled appropriately. If `use_norm` is infinite or NaN, the scaling factor is set to NaN, which will result in the tensors being set to NaN to signal an error.\n\nThe function processes each tensor in `t_list`, converting `IndexedSlices` to dense tensors if necessary, and applies the scaling factor to each tensor. The clipped tensors are then returned along with the global norm.\n\nThis function is particularly useful in machine learning for gradient clipping, where it helps prevent exploding gradients by scaling down the gradients when their norm exceeds a specified threshold. It is slower than `clip_by_norm` because it requires all parameters to be ready before the clipping operation can be performed.\n\n**Note**: \n- The function ignores any `None` entries in `t_list`.\n- If `clip_norm` is greater than the global norm, the tensors remain unchanged.\n- If the global norm is infinite, the tensors are set to NaN to indicate an error.\n- The function works with both dense tensors and `IndexedSlices`.\n\n**Output Example**: The function returns a tuple containing two elements:\n1. A list of clipped tensors, where each tensor has been scaled by the ratio of `clip_norm` to the global norm.\n2. A scalar tensor representing the global norm.\n\nFor example, if the input tensors have a global norm of 5.0 and `clip_norm` is set to 2.0, the function will return a list of tensors scaled by a factor of 0.4 (2.0 / 5.0) and a scalar tensor with the value 5.0."
      ],
      "code_start_line": 290,
      "code_end_line": 379,
      "params": [
        "t_list",
        "clip_norm",
        "use_norm",
        "name"
      ],
      "have_return": true,
      "code_content": "def clip_by_global_norm(t_list, clip_norm, use_norm=None, name=None):\n  \"\"\"Clips values of multiple tensors by the ratio of the sum of their norms.\n\n  Given a tuple or list of tensors `t_list`, and a clipping ratio `clip_norm`,\n  this operation returns a list of clipped tensors `list_clipped`\n  and the global norm (`global_norm`) of all tensors in `t_list`. Optionally,\n  if you've already computed the global norm for `t_list`, you can specify\n  the global norm with `use_norm`.\n\n  To perform the clipping, the values `t_list[i]` are set to:\n\n      t_list[i] * clip_norm / max(global_norm, clip_norm)\n\n  where:\n\n      global_norm = sqrt(sum([l2norm(t)**2 for t in t_list]))\n\n  If `clip_norm > global_norm` then the entries in `t_list` remain as they are,\n  otherwise they're all shrunk by the global ratio.\n\n  If `global_norm == infinity` then the entries in `t_list` are all set to `NaN`\n  to signal that an error occurred.\n\n  Any of the entries of `t_list` that are of type `None` are ignored.\n\n  This is the correct way to perform gradient clipping (Pascanu et al., 2012).\n\n  However, it is slower than `clip_by_norm()` because all the parameters must be\n  ready before the clipping operation can be performed.\n\n  Args:\n    t_list: A tuple or list of mixed `Tensors`, `IndexedSlices`, or None.\n    clip_norm: A 0-D (scalar) `Tensor` > 0. The clipping ratio.\n    use_norm: A 0-D (scalar) `Tensor` of type `float` (optional). The global\n      norm to use. If not provided, `global_norm()` is used to compute the norm.\n    name: A name for the operation (optional).\n\n  Returns:\n    list_clipped: A list of `Tensors` of the same type as `list_t`.\n    global_norm: A 0-D (scalar) `Tensor` representing the global norm.\n\n  Raises:\n    TypeError: If `t_list` is not a sequence.\n\n  References:\n    On the difficulty of training Recurrent Neural Networks:\n      [Pascanu et al., 2012](http://proceedings.mlr.press/v28/pascanu13.html)\n      ([pdf](http://proceedings.mlr.press/v28/pascanu13.pdf))\n  \"\"\"\n  if (not isinstance(t_list, collections_abc.Sequence) or\n      isinstance(t_list, str)):\n    raise TypeError(\"`t_list` should be a sequence of tensors. Received \"\n                    f\"{type(t_list)}.\")\n  t_list = list(t_list)\n  if use_norm is None:\n    use_norm = global_norm(t_list, name)\n\n  with ops.name_scope(name, \"clip_by_global_norm\",\n                      t_list + [clip_norm]) as name:\n    # Calculate L2-norm, clip elements by ratio of clip_norm to L2-norm\n    scale_for_finite = clip_norm * math_ops.minimum(\n        1.0 / use_norm,\n        constant_op.constant(1.0, dtype=use_norm.dtype) / clip_norm)\n    # If use_norm is any finite number, this is a no-op. For inf/-inf/NaN,\n    # this will make scale NaN.\n    scale = scale_for_finite + (use_norm - use_norm)\n\n    values = [\n        ops.convert_to_tensor(\n            t.values if isinstance(t, indexed_slices.IndexedSlices) else t,\n            name=\"t_%d\" % i) if t is not None else t\n        for i, t in enumerate(t_list)\n    ]\n\n    values_clipped = []\n    for i, v in enumerate(values):\n      if v is None:\n        values_clipped.append(None)\n      else:\n        with ops.colocate_with(v):\n          values_clipped.append(\n              array_ops.identity(v * scale, name=\"%s_%d\" % (name, i)))\n\n    list_clipped = [\n        indexed_slices.IndexedSlices(c_v, t.indices, t.dense_shape)\n        if isinstance(t, indexed_slices.IndexedSlices) else c_v\n        for (c_v, t) in zip(values_clipped, t_list)\n    ]\n\n  return list_clipped, use_norm\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "dataset/clip_ops.py/global_norm"
      ],
      "special_reference_type": [
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "clip_by_average_norm",
      "md_content": [
        "**clip_by_average_norm**: The function of clip_by_average_norm is to clip tensor values to ensure that their average L2-norm does not exceed a specified maximum value.\n\n**parameters**: The parameters of this Function.\n· t: A `Tensor` whose values need to be clipped. This is the input tensor that will be normalized based on the average L2-norm.\n· clip_norm: A 0-D (scalar) `Tensor` greater than 0. This specifies the maximum allowed average L2-norm for the tensor values.\n· name: An optional name for the operation. If provided, it will be used as the name scope for the operation.\n\n**Code Description**: The function `clip_by_average_norm` ensures that the average L2-norm of the input tensor `t` does not exceed the specified `clip_norm`. It achieves this by first calculating the L2-norm of the tensor. If the average L2-norm of the tensor is already less than or equal to `clip_norm`, the tensor remains unchanged. However, if the average L2-norm exceeds `clip_norm`, the tensor values are scaled down proportionally so that the average L2-norm of the resulting tensor equals `clip_norm`.\n\nThe function internally performs the following steps:\n1. Converts the input tensor `t` to a TensorFlow tensor if it is not already one.\n2. Calculates the number of elements in the tensor and converts it to a float32 value.\n3. Computes the inverse of the L2-norm for each element in the tensor.\n4. Scales the tensor values by the minimum of two values: the product of the inverse L2-norm and the number of elements, or the inverse of `clip_norm`.\n5. Returns the clipped tensor with the same shape and type as the input tensor.\n\nThis function is particularly useful in machine learning applications, such as gradient clipping, where it is necessary to prevent gradients from becoming too large during optimization.\n\n**Note**: The function assumes that the input tensor `t` and `clip_norm` are valid and compatible for the operations performed. The `clip_norm` must be a positive scalar value. If the tensor is already within the desired norm, no changes are made to it.\n\n**Output Example**: If the input tensor `t` has values that result in an average L2-norm greater than `clip_norm`, the output will be a tensor of the same shape and type as `t`, but with values scaled down such that the average L2-norm equals `clip_norm`. For example, if `t` is `[3.0, 4.0]` and `clip_norm` is `2.0`, the output might be `[1.2, 1.6]` (values are illustrative and depend on the exact calculation)."
      ],
      "code_start_line": 389,
      "code_end_line": 427,
      "params": [
        "t",
        "clip_norm",
        "name"
      ],
      "have_return": true,
      "code_content": "def clip_by_average_norm(t, clip_norm, name=None):\n  \"\"\"Clips tensor values to a maximum average L2-norm.\n\n  Given a tensor `t`, and a maximum clip value `clip_norm`, this operation\n  normalizes `t` so that its average L2-norm is less than or equal to\n  `clip_norm`. Specifically, if the average L2-norm is already less than or\n  equal to `clip_norm`, then `t` is not modified. If the average L2-norm is\n  greater than `clip_norm`, then this operation returns a tensor of the same\n  type and shape as `t` with its values set to:\n\n  `t * clip_norm / l2norm_avg(t)`\n\n  In this case, the average L2-norm of the output tensor is `clip_norm`.\n\n  This operation is typically used to clip gradients before applying them with\n  an optimizer.\n\n  Args:\n    t: A `Tensor`.\n    clip_norm: A 0-D (scalar) `Tensor` > 0. A maximum clipping value.\n    name: A name for the operation (optional).\n\n  Returns:\n    A clipped `Tensor`.\n  \"\"\"\n  with ops.name_scope(name, \"clip_by_average_norm\", [t, clip_norm]) as name:\n    t = ops.convert_to_tensor(t, name=\"t\")\n\n    # Calculate L2-norm per element, clip elements by ratio of clip_norm to\n    # L2-norm per element\n    n_element = math_ops.cast(array_ops.size(t), dtypes.float32)\n    l2norm_inv = math_ops.rsqrt(\n        math_ops.reduce_sum(t * t, math_ops.range(array_ops.rank(t))))\n    tclip = array_ops.identity(\n        t * clip_norm * math_ops.minimum(\n            l2norm_inv * n_element, constant_op.constant(1.0) / clip_norm),\n        name=name)\n\n  return tclip\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    }
  ],
  "dataset/message-capture-parser.py": [
    {
      "type": "ClassDef",
      "name": "ProgressBar",
      "md_content": [
        "**ProgressBar**: The function of ProgressBar is to visually represent the progress of a task in the terminal using a dynamic progress bar.\n\n**attributes**: The attributes of this Class.\n· total: A float value representing the total amount of work to be completed.\n· running: A float value representing the amount of work that has been completed so far.\n\n**Code Description**: The ProgressBar class is designed to provide a visual representation of progress for tasks such as file processing. It initializes with a total amount of work (`total`) and tracks the completed work (`running`). The `set_progress` method calculates the progress percentage and displays a progress bar in the terminal. The progress bar consists of `#` characters representing completed work and spaces representing remaining work. The `update` method increments the `running` attribute by a specified amount and updates the progress bar accordingly.\n\nIn the project, the ProgressBar class is used in conjunction with the `process_file` function to track the progress of reading and processing binary message capture files. The `process_file` function reads the file in chunks and updates the progress bar after each chunk is processed. The `main` function initializes the ProgressBar with the total size of all files to be processed and passes it to `process_file` for each file. This ensures that the progress bar accurately reflects the overall progress of the task.\n\n**Note**: \n1. The progress bar is only displayed if the terminal width is greater than 12 columns. If the terminal is too narrow, the progress bar will not be shown.\n2. The progress bar is updated dynamically in the terminal, overwriting the previous progress bar display. This requires the terminal to support carriage return (`\\r`) for proper rendering.\n3. The progress bar is designed for tasks where the total amount of work is known in advance, such as processing files of known sizes.\n\n**Output Example**: \nWhen the progress bar is displayed, it might look like this in the terminal:\n```\n[ #########################                          ]  65%\n```\nThis indicates that 65% of the task has been completed. The `#` characters represent completed work, and the spaces represent remaining work. The percentage is displayed to the right of the progress bar."
      ],
      "code_start_line": 51,
      "code_end_line": 70,
      "params": [],
      "have_return": true,
      "code_content": "class ProgressBar:\n    def __init__(self, total: float):\n        self.total = total\n        self.running = 0\n\n    def set_progress(self, progress: float):\n        cols = shutil.get_terminal_size()[0]\n        if cols <= 12:\n            return\n        max_blocks = cols - 9\n        num_blocks = int(max_blocks * progress)\n        print('\\r[ {}{} ] {:3.0f}%'\n              .format('#' * num_blocks,\n                      ' ' * (max_blocks - num_blocks),\n                      progress * 100),\n              end ='')\n\n    def update(self, more: float):\n        self.running += more\n        self.set_progress(self.running / self.total)\n",
      "name_column": 6,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "dataset/message-capture-parser.py/process_file",
        "dataset/message-capture-parser.py/main"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [
        "**__init__**: The function of __init__ is to initialize an instance of the ProgressBar class with a total value and set the initial running progress to zero.\n\n**parameters**: The parameters of this Function.\n· total: A float value representing the total progress or the maximum value that the progress bar will track.\n\n**Code Description**: The __init__ method is the constructor for the ProgressBar class. It takes a single parameter, `total`, which is a float representing the total amount of progress to be tracked. Upon initialization, the method assigns this `total` value to the instance variable `self.total`. Additionally, it initializes another instance variable, `self.running`, to zero. This `self.running` variable is intended to keep track of the current progress as the task progresses.\n\n**Note**: Ensure that the `total` parameter is set to a meaningful value that accurately represents the total progress to be tracked. The `self.running` variable should be updated appropriately during the execution of the task to reflect the current progress."
      ],
      "code_start_line": 52,
      "code_end_line": 54,
      "params": [
        "self",
        "total"
      ],
      "have_return": false,
      "code_content": "    def __init__(self, total: float):\n        self.total = total\n        self.running = 0\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "set_progress",
      "md_content": [
        "**set_progress**: The function of set_progress is to visually display the progress of a task in the terminal using a progress bar.\n\n**parameters**: The parameters of this Function.\n· progress: A float value representing the current progress of the task, typically ranging from 0 to 1, where 0 indicates no progress and 1 indicates completion.\n\n**Code Description**: The `set_progress` function is designed to update and display a progress bar in the terminal. It first retrieves the terminal's width using `shutil.get_terminal_size()[0]` to determine how many columns are available for the progress bar. If the terminal width is too small (less than or equal to 12 columns), the function exits early without displaying anything. Otherwise, it calculates the maximum number of blocks that can be displayed in the progress bar by subtracting 9 from the terminal width (to leave space for the percentage display). The number of blocks to be filled is determined by multiplying the maximum blocks by the `progress` parameter. The progress bar is then printed to the terminal using a formatted string, where filled blocks are represented by `#` and empty blocks by spaces. The percentage of completion is displayed alongside the progress bar. The `end=''` argument in the `print` function ensures that the cursor remains on the same line, allowing for smooth updates to the progress bar.\n\nThis function is primarily called by the `update` method of the `ProgressBar` class, which updates the progress based on the amount of work completed (`more`) relative to the total work (`total`). It is also called in the `main` function to set the progress bar to 100% when the task is complete.\n\n**Note**: The progress bar will not be displayed if the terminal width is too small (less than or equal to 12 columns). Additionally, the function assumes that the terminal supports carriage return (`\\r`) for updating the same line.\n\n**Output Example**: \n```\n[ #########################                          ]  75%\n```\nThis output represents a progress bar that is 75% complete, with filled blocks (`#`) and empty blocks (` `) visually indicating the progress. The percentage is displayed at the end of the bar."
      ],
      "code_start_line": 56,
      "code_end_line": 66,
      "params": [
        "self",
        "progress"
      ],
      "have_return": true,
      "code_content": "    def set_progress(self, progress: float):\n        cols = shutil.get_terminal_size()[0]\n        if cols <= 12:\n            return\n        max_blocks = cols - 9\n        num_blocks = int(max_blocks * progress)\n        print('\\r[ {}{} ] {:3.0f}%'\n              .format('#' * num_blocks,\n                      ' ' * (max_blocks - num_blocks),\n                      progress * 100),\n              end ='')\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "dataset/message-capture-parser.py/ProgressBar/update",
        "dataset/message-capture-parser.py/main"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "update",
      "md_content": [
        "**update**: The function of update is to increment the progress of a task by a specified amount and update the progress bar accordingly.\n\n**parameters**: The parameters of this Function.\n· more: A float value representing the amount of progress to be added to the current progress. This value is typically a fraction of the total work to be done.\n\n**Code Description**: The `update` function is a method of the `ProgressBar` class and is used to increment the progress of a task. It takes a single parameter, `more`, which specifies the amount of progress to be added. The function updates the `running` attribute of the `ProgressBar` instance by adding the value of `more` to it. After updating the `running` attribute, the function calls the `set_progress` method, passing the ratio of `running` to `total` as the progress value. This ratio represents the current progress of the task, where `running` is the amount of work completed so far, and `total` is the total amount of work to be done.\n\nThe `update` function is primarily used in conjunction with the `process_file` function, where it is called to update the progress bar as the file is being processed. In `process_file`, the `update` function is called with the difference in the file pointer position (`diff`) to reflect the progress of reading and processing the file. This ensures that the progress bar is updated in real-time as the file is being read, providing visual feedback to the user.\n\nThe `set_progress` method, which is called by `update`, is responsible for visually displaying the progress bar in the terminal. It calculates the number of blocks to be filled based on the current progress and prints the progress bar to the terminal. The `update` function, therefore, serves as a bridge between the task's progress and the visual representation of that progress.\n\n**Note**: The `update` function assumes that the `running` and `total` attributes of the `ProgressBar` instance are properly initialized before it is called. Additionally, the `more` parameter should be a positive value representing a meaningful increment in progress. If `more` is zero or negative, the progress bar will not advance, and the visual representation will remain unchanged."
      ],
      "code_start_line": 68,
      "code_end_line": 70,
      "params": [
        "self",
        "more"
      ],
      "have_return": false,
      "code_content": "    def update(self, more: float):\n        self.running += more\n        self.set_progress(self.running / self.total)\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "dataset/message-capture-parser.py/process_file"
      ],
      "reference_who": [
        "dataset/message-capture-parser.py/ProgressBar/set_progress"
      ],
      "special_reference_type": [
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "to_jsonable",
      "md_content": [
        "**to_jsonable**: The function of to_jsonable is to convert a given object into a JSON-serializable format. This function handles various types of objects, including those with `__dict__` or `__slots__` attributes, lists, bytes, and other basic types.\n\n**parameters**: The parameters of this Function.\n· obj: The object to be converted into a JSON-serializable format. This can be any type of object, including custom classes, lists, bytes, or basic data types.\n\n**Code Description**: \nThe `to_jsonable` function is designed to recursively convert an object into a format that can be easily serialized into JSON. It first checks if the object has a `__dict__` attribute, which is common in custom classes. If so, it returns the object's `__dict__`, which contains all the object's attributes as key-value pairs. If the object does not have a `__dict__` but has `__slots__`, the function iterates over the slots, retrieves the corresponding values, and converts them into a dictionary. Special handling is applied for certain slots that contain integers or lists of integers, which are converted into hexadecimal strings using the `ser_uint256` function.\n\nIf the object is a list, the function recursively applies `to_jsonable` to each element in the list. If the object is of type `bytes`, it converts the bytes into a hexadecimal string. For all other types, the function returns the object as-is, assuming it is already JSON-serializable.\n\nThis function is called within the `process_file` function, where it is used to convert the body of a deserialized message into a JSON-serializable format before appending it to the `messages` list. This ensures that the message data can be easily serialized into JSON for further processing or output.\n\n**Note**: \n- The function assumes that objects with `__slots__` have attributes that can be safely accessed using `getattr`. If an object has private or protected slots, this function may not handle them correctly.\n- The function does not handle circular references, which could lead to infinite recursion if present in the object structure.\n- The `ser_uint256` function is used to convert integers into hexadecimal strings, but its implementation is not provided in the code snippet.\n\n**Output Example**: \nGiven an object with the following structure:\n```python\nclass Example:\n    __slots__ = ['a', 'b', 'c']\n    def __init__(self):\n        self.a = 123\n        self.b = [456, 789]\n        self.c = b'hello'\n```\nThe output of `to_jsonable(Example())` would be:\n```python\n{\n    'a': '7b',  # 123 in hexadecimal\n    'b': ['1c8', '315'],  # [456, 789] in hexadecimal\n    'c': '68656c6c6f'  # 'hello' in hexadecimal\n}\n```"
      ],
      "code_start_line": 73,
      "code_end_line": 92,
      "params": [
        "obj"
      ],
      "have_return": true,
      "code_content": "def to_jsonable(obj: Any) -> Any:\n    if hasattr(obj, \"__dict__\"):\n        return obj.__dict__\n    elif hasattr(obj, \"__slots__\"):\n        ret = {}    # type: Any\n        for slot in obj.__slots__:\n            val = getattr(obj, slot, None)\n            if slot in HASH_INTS and isinstance(val, int):\n                ret[slot] = ser_uint256(val).hex()\n            elif slot in HASH_INT_VECTORS and all(isinstance(a, int) for a in val):\n                ret[slot] = [ser_uint256(a).hex() for a in val]\n            else:\n                ret[slot] = to_jsonable(val)\n        return ret\n    elif isinstance(obj, list):\n        return [to_jsonable(a) for a in obj]\n    elif isinstance(obj, bytes):\n        return obj.hex()\n    else:\n        return obj\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "dataset/message-capture-parser.py/process_file"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "process_file",
      "md_content": [
        "**process_file**: The function of process_file is to read and parse a binary message capture file, extract message headers and bodies, and store the processed data in a list of dictionaries.\n\n**parameters**: The parameters of this Function.\n· path: A string representing the file path of the binary message capture file to be processed.\n· messages: A list where the processed message data will be stored. Each message is represented as a dictionary.\n· recv: A boolean flag indicating whether the messages in the file are received (True) or sent (False).\n· progress_bar: An optional ProgressBar object used to visually track the progress of file processing. If provided, the progress bar will be updated as the file is read.\n\n**Code Description**: \nThe `process_file` function is responsible for reading and parsing binary message capture files. It opens the file in binary mode and processes it in chunks. The function reads the file header, which contains metadata such as the message timestamp, type, and length. It then reads the message body based on the length specified in the header. The function converts the message into a dictionary format, which includes the message direction (received or sent), timestamp, size, type, and body. If the message type is unrecognized or the message body cannot be deserialized, the function logs a warning and appends an error message to the dictionary.\n\nThe function uses the `to_jsonable` helper function to convert the message body into a JSON-serializable format. This ensures that the message data can be easily serialized into JSON for further processing or output. The processed message dictionaries are appended to the `messages` list, which is passed as a parameter.\n\nIf a `progress_bar` is provided, the function updates it after reading each chunk of the file. This provides real-time feedback on the progress of file processing. The progress bar is updated based on the difference in the file pointer position before and after reading each chunk.\n\nThe `process_file` function is called by the `main` function, which handles command-line arguments, initializes the progress bar, and processes multiple message capture files. The `main` function sorts the processed messages by timestamp and outputs them in JSON format, either to a file or to the standard output.\n\n**Note**: \n- The function assumes that the binary message capture file follows a specific format, with headers containing timestamp, message type, and length fields.\n- The function handles unrecognized message types and deserialization errors gracefully by logging warnings and including error information in the output.\n- The progress bar is only updated if it is provided and if the file is being read in chunks. If the progress bar is not provided, the function processes the file without visual feedback."
      ],
      "code_start_line": 95,
      "code_end_line": 167,
      "params": [
        "path",
        "messages",
        "recv",
        "progress_bar"
      ],
      "have_return": false,
      "code_content": "def process_file(path: str, messages: List[Any], recv: bool, progress_bar: Optional[ProgressBar]) -> None:\n    with open(path, 'rb') as f_in:\n        if progress_bar:\n            bytes_read = 0\n\n        while True:\n            if progress_bar:\n                # Update progress bar\n                diff = f_in.tell() - bytes_read - 1\n                progress_bar.update(diff)\n                bytes_read = f_in.tell() - 1\n\n            # Read the Header\n            tmp_header_raw = f_in.read(TIME_SIZE + LENGTH_SIZE + MSGTYPE_SIZE)\n            if not tmp_header_raw:\n                break\n            tmp_header = BytesIO(tmp_header_raw)\n            time = int.from_bytes(tmp_header.read(TIME_SIZE), \"little\")      # type: int\n            msgtype = tmp_header.read(MSGTYPE_SIZE).split(b'\\x00', 1)[0]     # type: bytes\n            length = int.from_bytes(tmp_header.read(LENGTH_SIZE), \"little\")  # type: int\n\n            # Start converting the message to a dictionary\n            msg_dict = {}\n            msg_dict[\"direction\"] = \"recv\" if recv else \"sent\"\n            msg_dict[\"time\"] = time\n            msg_dict[\"size\"] = length   # \"size\" is less readable here, but more readable in the output\n\n            msg_ser = BytesIO(f_in.read(length))\n\n            # Determine message type\n            if msgtype not in MESSAGEMAP:\n                # Unrecognized message type\n                try:\n                    msgtype_tmp = msgtype.decode()\n                    if not msgtype_tmp.isprintable():\n                        raise UnicodeDecodeError\n                    msg_dict[\"msgtype\"] = msgtype_tmp\n                except UnicodeDecodeError:\n                    msg_dict[\"msgtype\"] = \"UNREADABLE\"\n                msg_dict[\"body\"] = msg_ser.read().hex()\n                msg_dict[\"error\"] = \"Unrecognized message type.\"\n                messages.append(msg_dict)\n                print(f\"WARNING - Unrecognized message type {msgtype} in {path}\", file=sys.stderr)\n                continue\n\n            # Deserialize the message\n            msg = MESSAGEMAP[msgtype]()\n            msg_dict[\"msgtype\"] = msgtype.decode()\n\n            try:\n                msg.deserialize(msg_ser)\n            except KeyboardInterrupt:\n                raise\n            except Exception:\n                # Unable to deserialize message body\n                msg_ser.seek(0, os.SEEK_SET)\n                msg_dict[\"body\"] = msg_ser.read().hex()\n                msg_dict[\"error\"] = \"Unable to deserialize message.\"\n                messages.append(msg_dict)\n                print(f\"WARNING - Unable to deserialize message in {path}\", file=sys.stderr)\n                continue\n\n            # Convert body of message into a jsonable object\n            if length:\n                msg_dict[\"body\"] = to_jsonable(msg)\n            messages.append(msg_dict)\n\n        if progress_bar:\n            # Update the progress bar to the end of the current file\n            # in case we exited the loop early\n            f_in.seek(0, os.SEEK_END)   # Go to end of file\n            diff = f_in.tell() - bytes_read - 1\n            progress_bar.update(diff)\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "dataset/message-capture-parser.py/main"
      ],
      "reference_who": [
        "dataset/message-capture-parser.py/ProgressBar",
        "dataset/message-capture-parser.py/ProgressBar/update",
        "dataset/message-capture-parser.py/to_jsonable"
      ],
      "special_reference_type": [
        true,
        false,
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "main",
      "md_content": [
        "**main**: The function of main is to parse binary message capture files, process their contents, and output the results in JSON format either to a file or to the standard output.\n\n**parameters**: The parameters of this Function.\n· capturepaths: A list of file paths pointing to the binary message capture files to be parsed.\n· output: An optional argument specifying the output file path where the JSON results will be saved. If not provided, the results are printed to stdout.\n· no-progress-bar: An optional flag to disable the progress bar during file processing. The progress bar is automatically disabled if the output is not a terminal.\n\n**Code Description**: \nThe `main` function serves as the entry point for parsing binary message capture files. It begins by setting up an argument parser using `argparse.ArgumentParser`, which handles command-line arguments. The parser is configured with a description, an example usage, and three arguments: `capturepaths`, `output`, and `no-progress-bar`. The `capturepaths` argument accepts one or more file paths, while the `output` argument specifies the destination file for the JSON output. The `no-progress-bar` flag allows users to disable the progress bar, which is useful for non-interactive environments.\n\nOnce the arguments are parsed, the function converts the provided file paths into absolute paths using `Path.cwd()`. It also determines whether to display a progress bar based on the `no-progress-bar` flag and whether the output is a terminal (`sys.stdout.isatty()`). If a progress bar is enabled, the total size of all input files is calculated to initialize the `ProgressBar` object.\n\nThe function then iterates over each file path in `capturepaths` and processes the file using the `process_file` function. This function reads the binary message capture file, extracts message headers and bodies, and stores the processed data in a list of dictionaries (`messages`). The `process_file` function also updates the progress bar if it is enabled.\n\nAfter all files are processed, the messages are sorted by their timestamp using the `sort` method. If a progress bar is active, it is set to 100% completion. Finally, the messages are serialized into a JSON string using `json.dumps`. If an output file is specified, the JSON string is written to the file; otherwise, it is printed to the standard output.\n\n**Note**: \n- The progress bar is only displayed if the output is a terminal and the `no-progress-bar` flag is not set. This ensures compatibility with non-interactive environments.\n- The function assumes that the binary message capture files follow a specific format, as handled by the `process_file` function.\n- The output JSON is sorted by message timestamps, providing a chronological view of the captured messages."
      ],
      "code_start_line": 170,
      "code_end_line": 211,
      "params": [],
      "have_return": false,
      "code_content": "def main():\n    parser = argparse.ArgumentParser(\n        description=__doc__,\n        epilog=\"EXAMPLE \\n\\t{0} -o out.json <data-dir>/message_capture/**/*.dat\".format(sys.argv[0]),\n        formatter_class=argparse.RawTextHelpFormatter)\n    parser.add_argument(\n        \"capturepaths\",\n        nargs='+',\n        help=\"binary message capture files to parse.\")\n    parser.add_argument(\n        \"-o\", \"--output\",\n        help=\"output file.  If unset print to stdout\")\n    parser.add_argument(\n        \"-n\", \"--no-progress-bar\",\n        action='store_true',\n        help=\"disable the progress bar.  Automatically set if the output is not a terminal\")\n    args = parser.parse_args()\n    capturepaths = [Path.cwd() / Path(capturepath) for capturepath in args.capturepaths]\n    output = Path.cwd() / Path(args.output) if args.output else False\n    use_progress_bar = (not args.no_progress_bar) and sys.stdout.isatty()\n\n    messages = []   # type: List[Any]\n    if use_progress_bar:\n        total_size = sum(capture.stat().st_size for capture in capturepaths)\n        progress_bar = ProgressBar(total_size)\n    else:\n        progress_bar = None\n\n    for capture in capturepaths:\n        process_file(str(capture), messages, \"recv\" in capture.stem, progress_bar)\n\n    messages.sort(key=lambda msg: msg['time'])\n\n    if use_progress_bar:\n        progress_bar.set_progress(1)\n\n    jsonrep = json.dumps(messages)\n    if output:\n        with open(str(output), 'w+', encoding=\"utf8\") as f_out:\n            f_out.write(jsonrep)\n    else:\n        print(jsonrep)\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "dataset/message-capture-parser.py/ProgressBar",
        "dataset/message-capture-parser.py/ProgressBar/set_progress",
        "dataset/message-capture-parser.py/process_file"
      ],
      "special_reference_type": [
        false,
        false,
        false
      ]
    }
  ],
  "dataset/basic_association.py": [
    {
      "type": "ClassDef",
      "name": "Order",
      "md_content": [
        "**Order**: The function of Order is to represent an order entity in a database, storing details such as the customer's name, order date, and associated order items.\n\n**attributes**: The attributes of this Class.\n· order_id: A unique identifier for the order, serving as the primary key in the database table.\n· customer_name: The name of the customer who placed the order. This field is mandatory and cannot be null.\n· order_date: The date and time when the order was placed. It defaults to the current date and time if not specified.\n· order_items: A relationship attribute that links the order to its associated order items. It supports cascading operations, ensuring that all related order items are deleted if the order is deleted.\n\n**Code Description**: The Order class is a SQLAlchemy model that maps to a database table named \"order\". It inherits from a Base class, which is typically the declarative base used in SQLAlchemy for defining database models. The class includes four main attributes:\n1. `order_id`: An integer column that uniquely identifies each order. It is marked as the primary key.\n2. `customer_name`: A string column that stores the name of the customer. It is defined as non-nullable, meaning it must have a value.\n3. `order_date`: A DateTime column that records the date and time of the order. It defaults to the current date and time using `datetime.now()` if no value is provided.\n4. `order_items`: A relationship attribute that establishes a one-to-many relationship with the `OrderItem` class. The `cascade=\"all, delete-orphan\"` option ensures that all related `OrderItem` records are deleted if the parent `Order` is deleted. The `backref=\"order\"` option creates a reverse reference from `OrderItem` back to `Order`.\n\nThe class also includes an `__init__` method that initializes the `customer_name` attribute when a new `Order` instance is created.\n\n**Note**: When using this class, ensure that the `customer_name` is always provided, as it is a non-nullable field. Additionally, be cautious with the `order_items` relationship, as it will automatically delete related `OrderItem` records if the parent `Order` is deleted. This behavior is controlled by the `cascade` option."
      ],
      "code_start_line": 31,
      "code_end_line": 42,
      "params": [],
      "have_return": false,
      "code_content": "class Order(Base):\n    __tablename__ = \"order\"\n\n    order_id = Column(Integer, primary_key=True)\n    customer_name = Column(String(30), nullable=False)\n    order_date = Column(DateTime, nullable=False, default=datetime.now())\n    order_items = relationship(\n        \"OrderItem\", cascade=\"all, delete-orphan\", backref=\"order\"\n    )\n\n    def __init__(self, customer_name):\n        self.customer_name = customer_name\n",
      "name_column": 6,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [
        "**__init__**: The function of __init__ is to initialize an instance of the Order class with a customer name.\n\n**parameters**: The parameters of this Function.\n· customer_name: A string representing the name of the customer associated with the order.\n\n**Code Description**: The __init__ method is the constructor for the Order class. It takes one parameter, `customer_name`, which is used to initialize the instance attribute `self.customer_name`. This attribute stores the name of the customer who placed the order. When an instance of the Order class is created, this method is automatically called, and the provided `customer_name` is assigned to the instance's `customer_name` attribute.\n\n**Note**: Ensure that the `customer_name` parameter is passed as a string when creating an instance of the Order class. This method does not perform any validation on the input, so it is the responsibility of the caller to ensure that the `customer_name` is in the correct format."
      ],
      "code_start_line": 41,
      "code_end_line": 42,
      "params": [
        "self",
        "customer_name"
      ],
      "have_return": false,
      "code_content": "    def __init__(self, customer_name):\n        self.customer_name = customer_name\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "ClassDef",
      "name": "Item",
      "md_content": [
        "**Item**: The function of Item is to represent a product or service in the system, storing its description and price.\n\n**attributes**: The attributes of this Class.\n· item_id: A unique identifier for the item, automatically generated as the primary key.\n· description: A string that describes the item, with a maximum length of 30 characters. This field cannot be null.\n· price: A floating-point number representing the price of the item. This field cannot be null.\n\n**Code Description**: The Item class is a SQLAlchemy model that maps to the \"item\" table in the database. It has three columns: item_id, description, and price. The item_id is an auto-incrementing integer that serves as the primary key. The description is a required field that stores a brief description of the item, and the price is a required field that stores the cost of the item.\n\nThe class includes an __init__ method that allows for the creation of an Item instance by passing in a description and a price. The __repr__ method provides a string representation of the Item object, which is useful for debugging and logging purposes.\n\nIn the context of the project, the Item class is used in conjunction with the OrderItem class. The OrderItem class establishes a many-to-many relationship between orders and items, where each OrderItem instance links an order to an item and stores the price at which the item was sold in that order. The relationship is defined using SQLAlchemy's relationship function, which allows for easy querying and manipulation of related objects.\n\n**Note**: When creating an Item instance, ensure that the description and price are provided, as these fields are non-nullable. Additionally, the price should be a valid floating-point number representing the cost of the item.\n\n**Output Example**: An example of the string representation of an Item object might look like this: `Item('Widget A', 19.99)`. This indicates an item with the description \"Widget A\" and a price of 19.99."
      ],
      "code_start_line": 45,
      "code_end_line": 56,
      "params": [],
      "have_return": true,
      "code_content": "class Item(Base):\n    __tablename__ = \"item\"\n    item_id = Column(Integer, primary_key=True)\n    description = Column(String(30), nullable=False)\n    price = Column(Float, nullable=False)\n\n    def __init__(self, description, price):\n        self.description = description\n        self.price = price\n\n    def __repr__(self):\n        return \"Item(%r, %r)\" % (self.description, self.price)\n",
      "name_column": 6,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "dataset/basic_association.py/OrderItem"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [
        "**__init__**: The function of __init__ is to initialize an instance of the `Item` class with a description and a price.\n\n**parameters**: The parameters of this Function.\n· description: A string that represents the description of the item.\n· price: A numeric value that represents the price of the item.\n\n**Code Description**: The `__init__` method is the constructor for the `Item` class. When an instance of the `Item` class is created, this method is automatically called to initialize the object with the provided `description` and `price`. The method assigns the `description` parameter to the instance attribute `self.description` and the `price` parameter to the instance attribute `self.price`. This ensures that each `Item` object has its own unique description and price attributes.\n\n**Note**: Ensure that the `price` parameter is a numeric value (e.g., integer or float) to avoid potential errors in operations that involve calculations with the price. The `description` should be a string to accurately represent the item's details."
      ],
      "code_start_line": 51,
      "code_end_line": 53,
      "params": [
        "self",
        "description",
        "price"
      ],
      "have_return": false,
      "code_content": "    def __init__(self, description, price):\n        self.description = description\n        self.price = price\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "__repr__",
      "md_content": [
        "**__repr__**: The function of __repr__ is to provide a string representation of the Item object that can be used to recreate the object.\n\n**parameters**: The parameters of this Function.\n· self: Refers to the instance of the Item class.\n\n**Code Description**: The __repr__ method is a special method in Python that returns a string representation of an object. In this case, it returns a string in the format \"Item(description, price)\", where `description` and `price` are the attributes of the Item object. The `%r` format specifier is used to ensure that the values are represented in a way that can be safely used to recreate the object. This method is particularly useful for debugging and logging, as it provides a clear and unambiguous representation of the object.\n\n**Note**: The __repr__ method should ideally return a string that, when passed to the `eval()` function, would recreate the object. This is not always possible or practical, but the returned string should at least be informative and unambiguous.\n\n**Output Example**: If an Item object has a description of \"Book\" and a price of 19.99, the __repr__ method would return the string \"Item('Book', 19.99)\"."
      ],
      "code_start_line": 55,
      "code_end_line": 56,
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def __repr__(self):\n        return \"Item(%r, %r)\" % (self.description, self.price)\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "ClassDef",
      "name": "OrderItem",
      "md_content": [
        "**OrderItem**: The function of OrderItem is to establish a many-to-many relationship between orders and items, linking an order to a specific item and storing the price at which the item was sold in that order.\n\n**attributes**: The attributes of this Class.\n· order_id: An integer that serves as a foreign key referencing the primary key of the \"order\" table. It is part of the composite primary key for the OrderItem table.\n· item_id: An integer that serves as a foreign key referencing the primary key of the \"item\" table. It is part of the composite primary key for the OrderItem table.\n· price: A floating-point number representing the price at which the item was sold in the order. This field cannot be null.\n\n**Code Description**: The OrderItem class is a SQLAlchemy model that maps to the \"orderitem\" table in the database. It is designed to manage the relationship between orders and items, where each OrderItem instance represents a specific item sold in a specific order. The class uses a composite primary key consisting of `order_id` and `item_id`, which are foreign keys referencing the \"order\" and \"item\" tables, respectively.\n\nThe `price` attribute stores the price at which the item was sold in the order. This price can be explicitly provided during the creation of an OrderItem instance or default to the price of the associated item if not specified. The `item` attribute is a relationship to the `Item` class, allowing for easy access to the associated item's details. The relationship is configured with `lazy=\"joined\"`, meaning that the associated item will be loaded eagerly when querying the OrderItem.\n\nThe `__init__` method allows for the creation of an OrderItem instance by passing in an `Item` object and an optional `price`. If the price is not provided, it defaults to the price of the associated item. This ensures that the price is always set, either explicitly or implicitly.\n\nIn the context of the project, the OrderItem class plays a crucial role in managing the many-to-many relationship between orders and items. It allows for the tracking of which items are included in which orders and at what price, facilitating order management and reporting.\n\n**Note**: When creating an OrderItem instance, ensure that the `item` parameter is provided, as it is required to establish the relationship with the Item class. The `price` parameter is optional but will default to the price of the associated item if not provided. Additionally, the `order_id` and `item_id` must correspond to valid entries in the \"order\" and \"item\" tables, respectively, to maintain referential integrity."
      ],
      "code_start_line": 59,
      "code_end_line": 69,
      "params": [],
      "have_return": false,
      "code_content": "class OrderItem(Base):\n    __tablename__ = \"orderitem\"\n    order_id = Column(Integer, ForeignKey(\"order.order_id\"), primary_key=True)\n    item_id = Column(Integer, ForeignKey(\"item.item_id\"), primary_key=True)\n    price = Column(Float, nullable=False)\n\n    def __init__(self, item, price=None):\n        self.item = item\n        self.price = price or item.price\n\n    item = relationship(Item, lazy=\"joined\")\n",
      "name_column": 6,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "dataset/basic_association.py/Item"
      ],
      "special_reference_type": [
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [
        "**__init__**: The function of __init__ is to initialize an instance of the OrderItem class with an item and an optional price.\n\n**parameters**: The parameters of this Function.\n· item: The item to be associated with the OrderItem instance. This is a required parameter and represents the item being ordered.\n· price: The price of the item. This is an optional parameter. If not provided, the price will default to the price of the item passed as the first parameter.\n\n**Code Description**: The description of this Function.\nThe __init__ method is the constructor for the OrderItem class. It takes two parameters: `item` and `price`. The `item` parameter is mandatory and is assigned directly to the instance attribute `self.item`. The `price` parameter is optional. If the `price` is not provided (i.e., it is `None`), the method assigns the price of the `item` (accessed via `item.price`) to the instance attribute `self.price`. If the `price` is provided, it is assigned directly to `self.price`. This ensures that the OrderItem instance always has a valid price, either explicitly provided or derived from the item itself.\n\n**Note**: Points to note about the use of the code\n- Ensure that the `item` passed to the constructor has a `price` attribute if the `price` parameter is not provided. Otherwise, an AttributeError may occur.\n- The `price` parameter is optional, but if provided, it will override the price of the `item`. This allows for flexibility in setting the price independently of the item's default price."
      ],
      "code_start_line": 65,
      "code_end_line": 67,
      "params": [
        "self",
        "item",
        "price"
      ],
      "have_return": false,
      "code_content": "    def __init__(self, item, price=None):\n        self.item = item\n        self.price = price or item.price\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    }
  ]
}