{
  "dataset/_kmeans.py": [
    {
      "type": "FunctionDef",
      "name": "kmeans_plusplus",
      "md_content": [
        "**kmeans_plusplus**: The function of kmeans_plusplus is to initialize cluster centers for the k-means clustering algorithm using the k-means++ method.\n\n**parameters**: The parameters of this Function.\n· X: {array-like, sparse matrix} of shape (n_samples, n_features) - The data to pick seeds from.\n· n_clusters: int - The number of centroids to initialize.\n· sample_weight: array-like of shape (n_samples,), default=None - The weights for each observation in `X`. If `None`, all observations are assigned equal weight. `sample_weight` is ignored if `init` is a callable or a user provided array.\n· x_squared_norms: array-like of shape (n_samples,), default=None - Squared Euclidean norm of each data point.\n· random_state: int or RandomState instance, default=None - Determines random number generation for centroid initialization. Pass an int for reproducible output across multiple function calls.\n· n_local_trials: int, default=None - The number of seeding trials for each center (except the first), of which the one reducing inertia the most is greedily chosen. Set to None to make the number of trials depend logarithmically on the number of seeds (2+log(k)) which is the recommended setting.\n\n**Code Description**: The kmeans_plusplus function is designed to select initial cluster centers for the k-means clustering algorithm in an efficient manner. It employs the k-means++ initialization method, which significantly improves the convergence speed of the k-means algorithm by carefully selecting the initial centroids. \n\nThe function begins by validating the input data `X` and checking that the number of samples is greater than or equal to the number of clusters specified. It also verifies the provided `sample_weight` and calculates the squared Euclidean norms of the data points if not provided. The random state is set for reproducibility of results.\n\nThe core of the function involves calling the private helper function _kmeans_plusplus, which performs the actual initialization of the centroids. This helper function selects the first centroid randomly from the dataset, and subsequent centroids are chosen based on their distance from existing centroids, with a probability proportional to the squared distance. This method ensures that points further away from existing centers have a higher chance of being selected, which is crucial for effective clustering.\n\nThe kmeans_plusplus function ultimately returns two outputs: the initialized centers and their corresponding indices in the original dataset. This function serves as a public interface for initializing cluster centers and is utilized within the k-means algorithm implementation, ensuring that the clustering process starts with a well-informed selection of initial centroids.\n\n**Note**: It is essential to ensure that the number of samples in `X` is greater than or equal to the number of clusters specified. The function assumes that prior validation of the data has been conducted.\n\n**Output Example**: A possible return value of the function could be:\ncenters: array([[10, 2],\n                [1, 0]])\nindices: array([3, 2])"
      ],
      "code_start_line": 72,
      "code_end_line": 175,
      "params": [
        "X",
        "n_clusters"
      ],
      "have_return": true,
      "code_content": "def kmeans_plusplus(\n    X,\n    n_clusters,\n    *,\n    sample_weight=None,\n    x_squared_norms=None,\n    random_state=None,\n    n_local_trials=None,\n):\n    \"\"\"Init n_clusters seeds according to k-means++.\n\n    .. versionadded:: 0.24\n\n    Parameters\n    ----------\n    X : {array-like, sparse matrix} of shape (n_samples, n_features)\n        The data to pick seeds from.\n\n    n_clusters : int\n        The number of centroids to initialize.\n\n    sample_weight : array-like of shape (n_samples,), default=None\n        The weights for each observation in `X`. If `None`, all observations\n        are assigned equal weight. `sample_weight` is ignored if `init`\n        is a callable or a user provided array.\n\n        .. versionadded:: 1.3\n\n    x_squared_norms : array-like of shape (n_samples,), default=None\n        Squared Euclidean norm of each data point.\n\n    random_state : int or RandomState instance, default=None\n        Determines random number generation for centroid initialization. Pass\n        an int for reproducible output across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    n_local_trials : int, default=None\n        The number of seeding trials for each center (except the first),\n        of which the one reducing inertia the most is greedily chosen.\n        Set to None to make the number of trials depend logarithmically\n        on the number of seeds (2+log(k)) which is the recommended setting.\n        Setting to 1 disables the greedy cluster selection and recovers the\n        vanilla k-means++ algorithm which was empirically shown to work less\n        well than its greedy variant.\n\n    Returns\n    -------\n    centers : ndarray of shape (n_clusters, n_features)\n        The initial centers for k-means.\n\n    indices : ndarray of shape (n_clusters,)\n        The index location of the chosen centers in the data array X. For a\n        given index and center, X[index] = center.\n\n    Notes\n    -----\n    Selects initial cluster centers for k-mean clustering in a smart way\n    to speed up convergence. see: Arthur, D. and Vassilvitskii, S.\n    \"k-means++: the advantages of careful seeding\". ACM-SIAM symposium\n    on Discrete algorithms. 2007\n\n    Examples\n    --------\n\n    >>> from sklearn.cluster import kmeans_plusplus\n    >>> import numpy as np\n    >>> X = np.array([[1, 2], [1, 4], [1, 0],\n    ...               [10, 2], [10, 4], [10, 0]])\n    >>> centers, indices = kmeans_plusplus(X, n_clusters=2, random_state=0)\n    >>> centers\n    array([[10,  2],\n           [ 1,  0]])\n    >>> indices\n    array([3, 2])\n    \"\"\"\n    # Check data\n    check_array(X, accept_sparse=\"csr\", dtype=[np.float64, np.float32])\n    sample_weight = _check_sample_weight(sample_weight, X, dtype=X.dtype)\n\n    if X.shape[0] < n_clusters:\n        raise ValueError(\n            f\"n_samples={X.shape[0]} should be >= n_clusters={n_clusters}.\"\n        )\n\n    # Check parameters\n    if x_squared_norms is None:\n        x_squared_norms = row_norms(X, squared=True)\n    else:\n        x_squared_norms = check_array(x_squared_norms, dtype=X.dtype, ensure_2d=False)\n\n    if x_squared_norms.shape[0] != X.shape[0]:\n        raise ValueError(\n            f\"The length of x_squared_norms {x_squared_norms.shape[0]} should \"\n            f\"be equal to the length of n_samples {X.shape[0]}.\"\n        )\n\n    random_state = check_random_state(random_state)\n\n    # Call private k-means++\n    centers, indices = _kmeans_plusplus(\n        X, n_clusters, x_squared_norms, sample_weight, random_state, n_local_trials\n    )\n\n    return centers, indices\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "dataset/_kmeans.py/_kmeans_plusplus"
      ],
      "special_reference_type": [
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "_kmeans_plusplus",
      "md_content": [
        "**_kmeans_plusplus**: The function of _kmeans_plusplus is to initialize cluster centers for the k-means clustering algorithm using the k-means++ method.\n\n**parameters**: The parameters of this Function.\n· X: {ndarray, sparse matrix} of shape (n_samples, n_features) - The data to pick seeds for.\n· n_clusters: int - The number of seeds to choose.\n· sample_weight: ndarray of shape (n_samples,) - The weights for each observation in `X`.\n· x_squared_norms: ndarray of shape (n_samples,) - Squared Euclidean norm of each data point.\n· random_state: RandomState instance - The generator used to initialize the centers.\n· n_local_trials: int, default=None - The number of seeding trials for each center (except the first).\n\n**Code Description**: The _kmeans_plusplus function is a computational component designed to facilitate the initialization of cluster centers in the k-means clustering algorithm by employing the k-means++ strategy. This method is particularly advantageous as it selects initial cluster centers in a way that is expected to improve the convergence speed of the algorithm.\n\nThe function begins by determining the number of samples and features from the input data `X`. It then initializes an empty array to hold the cluster centers. If the number of local trials is not specified, it defaults to a logarithmic function of the number of clusters, which helps in selecting better initial centers.\n\nThe first cluster center is chosen randomly from the data points, weighted by the provided sample weights. The function then calculates the squared distances from this center to all other points in the dataset, which is crucial for the selection of subsequent centers. The remaining centers are chosen based on their distance from the existing centers, with a probability proportional to the squared distance, ensuring that points farther away from existing centers have a higher chance of being selected.\n\nThe function also handles sparse matrices appropriately, converting them to dense arrays when necessary. Finally, it returns the initialized centers and their corresponding indices in the original dataset.\n\nThis function is called by the kmeans_plusplus function, which serves as a public interface for initializing cluster centers. The kmeans_plusplus function performs preliminary checks on the input data and parameters before delegating the actual initialization task to _kmeans_plusplus. Additionally, it is utilized within the _init_centroids method of the _BaseKMeans class, which is responsible for computing the initial centroids based on the specified initialization method.\n\n**Note**: It is important to ensure that the number of samples in `X` is greater than or equal to the number of clusters specified. The function assumes that prior validation of the data has been conducted.\n\n**Output Example**: A possible return value of the function could be:\ncenters: array([[10, 2],\n                [1, 0]])\nindices: array([3, 2])"
      ],
      "code_start_line": 178,
      "code_end_line": 276,
      "params": [
        "X",
        "n_clusters",
        "x_squared_norms",
        "sample_weight",
        "random_state",
        "n_local_trials"
      ],
      "have_return": true,
      "code_content": "def _kmeans_plusplus(\n    X, n_clusters, x_squared_norms, sample_weight, random_state, n_local_trials=None\n):\n    \"\"\"Computational component for initialization of n_clusters by\n    k-means++. Prior validation of data is assumed.\n\n    Parameters\n    ----------\n    X : {ndarray, sparse matrix} of shape (n_samples, n_features)\n        The data to pick seeds for.\n\n    n_clusters : int\n        The number of seeds to choose.\n\n    sample_weight : ndarray of shape (n_samples,)\n        The weights for each observation in `X`.\n\n    x_squared_norms : ndarray of shape (n_samples,)\n        Squared Euclidean norm of each data point.\n\n    random_state : RandomState instance\n        The generator used to initialize the centers.\n        See :term:`Glossary <random_state>`.\n\n    n_local_trials : int, default=None\n        The number of seeding trials for each center (except the first),\n        of which the one reducing inertia the most is greedily chosen.\n        Set to None to make the number of trials depend logarithmically\n        on the number of seeds (2+log(k)); this is the default.\n\n    Returns\n    -------\n    centers : ndarray of shape (n_clusters, n_features)\n        The initial centers for k-means.\n\n    indices : ndarray of shape (n_clusters,)\n        The index location of the chosen centers in the data array X. For a\n        given index and center, X[index] = center.\n    \"\"\"\n    n_samples, n_features = X.shape\n\n    centers = np.empty((n_clusters, n_features), dtype=X.dtype)\n\n    # Set the number of local seeding trials if none is given\n    if n_local_trials is None:\n        # This is what Arthur/Vassilvitskii tried, but did not report\n        # specific results for other than mentioning in the conclusion\n        # that it helped.\n        n_local_trials = 2 + int(np.log(n_clusters))\n\n    # Pick first center randomly and track index of point\n    center_id = random_state.choice(n_samples, p=sample_weight / sample_weight.sum())\n    indices = np.full(n_clusters, -1, dtype=int)\n    if sp.issparse(X):\n        centers[0] = X[[center_id]].toarray()\n    else:\n        centers[0] = X[center_id]\n    indices[0] = center_id\n\n    # Initialize list of closest distances and calculate current potential\n    closest_dist_sq = _euclidean_distances(\n        centers[0, np.newaxis], X, Y_norm_squared=x_squared_norms, squared=True\n    )\n    current_pot = closest_dist_sq @ sample_weight\n\n    # Pick the remaining n_clusters-1 points\n    for c in range(1, n_clusters):\n        # Choose center candidates by sampling with probability proportional\n        # to the squared distance to the closest existing center\n        rand_vals = random_state.uniform(size=n_local_trials) * current_pot\n        candidate_ids = np.searchsorted(\n            stable_cumsum(sample_weight * closest_dist_sq), rand_vals\n        )\n        # XXX: numerical imprecision can result in a candidate_id out of range\n        np.clip(candidate_ids, None, closest_dist_sq.size - 1, out=candidate_ids)\n\n        # Compute distances to center candidates\n        distance_to_candidates = _euclidean_distances(\n            X[candidate_ids], X, Y_norm_squared=x_squared_norms, squared=True\n        )\n\n        # update closest distances squared and potential for each candidate\n        np.minimum(closest_dist_sq, distance_to_candidates, out=distance_to_candidates)\n        candidates_pot = distance_to_candidates @ sample_weight.reshape(-1, 1)\n\n        # Decide which candidate is the best\n        best_candidate = np.argmin(candidates_pot)\n        current_pot = candidates_pot[best_candidate]\n        closest_dist_sq = distance_to_candidates[best_candidate]\n        best_candidate = candidate_ids[best_candidate]\n\n        # Permanently add best center candidate found in local tries\n        if sp.issparse(X):\n            centers[c] = X[[best_candidate]].toarray()\n        else:\n            centers[c] = X[best_candidate]\n        indices[c] = best_candidate\n\n    return centers, indices\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "dataset/_kmeans.py/kmeans_plusplus",
        "dataset/_kmeans.py/_BaseKMeans/_init_centroids"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "_tolerance",
      "md_content": [
        "**_tolerance**: The function of _tolerance is to calculate a tolerance value based on the dataset provided.\n\n**parameters**: The parameters of this Function.\n· parameter1: X - The input dataset, which can be either a dense or sparse matrix.\n· parameter2: tol - A scalar value representing the tolerance factor.\n\n**Code Description**: The _tolerance function computes a tolerance value that is dependent on the characteristics of the dataset X. If the tol parameter is set to zero, the function immediately returns zero, indicating no tolerance. For datasets represented as sparse matrices, the function calculates the variances of the features using the mean_variance_axis function, which is assumed to return both the mean and variance along the specified axis. For dense matrices, it directly computes the variances using NumPy's var function. Finally, the function returns the mean of these variances multiplied by the tol parameter, effectively scaling the tolerance based on the variability of the dataset.\n\nThis function is called within the _check_params_vs_input method of the _BaseKMeans class. In this context, it is used to set the instance variable self._tol, which represents the computed tolerance for the KMeans clustering algorithm. The _check_params_vs_input method ensures that the number of samples in the dataset is sufficient relative to the number of clusters specified. By calculating the tolerance using the _tolerance function, it helps in determining the convergence criteria for the clustering process, thereby influencing the behavior of the KMeans algorithm during its execution.\n\n**Note**: It is important to ensure that the tol parameter is set appropriately, as a value of zero will lead to a tolerance of zero, which may affect the clustering results. Additionally, the function is designed to handle both sparse and dense datasets, making it versatile for different types of input data.\n\n**Output Example**: For a dataset X with variances [0.5, 1.0, 1.5] and a tol value of 0.1, the function would return (0.5 + 1.0 + 1.5) / 3 * 0.1 = 0.1."
      ],
      "code_start_line": 283,
      "code_end_line": 291,
      "params": [
        "X",
        "tol"
      ],
      "have_return": true,
      "code_content": "def _tolerance(X, tol):\n    \"\"\"Return a tolerance which is dependent on the dataset.\"\"\"\n    if tol == 0:\n        return 0\n    if sp.issparse(X):\n        variances = mean_variance_axis(X, axis=0)[1]\n    else:\n        variances = np.var(X, axis=0)\n    return np.mean(variances) * tol\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "dataset/_kmeans.py/_BaseKMeans/_check_params_vs_input"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "k_means",
      "md_content": [
        "**k_means**: The function of k_means is to perform K-means clustering algorithm on a given dataset.\n\n**parameters**: The parameters of this Function.\n· X: {array-like, sparse matrix} of shape (n_samples, n_features) - The observations to cluster. The data will be converted to C ordering, which may cause a memory copy if the data is not C-contiguous.\n· n_clusters: int - The number of clusters to form as well as the number of centroids to generate.\n· sample_weight: array-like of shape (n_samples,), default=None - The weights for each observation in X. If None, all observations are assigned equal weight. sample_weight is not used during initialization if init is a callable or a user-provided array.\n· init: {'k-means++', 'random'}, callable or array-like of shape (n_clusters, n_features), default='k-means++' - Method for initialization of cluster centers.\n· n_init: 'auto' or int, default=\"auto\" - Number of times the k-means algorithm will be run with different centroid seeds.\n· max_iter: int, default=300 - Maximum number of iterations of the k-means algorithm to run.\n· verbose: bool, default=False - Verbosity mode.\n· tol: float, default=1e-4 - Relative tolerance with regards to Frobenius norm of the difference in the cluster centers of two consecutive iterations to declare convergence.\n· random_state: int, RandomState instance or None, default=None - Determines random number generation for centroid initialization.\n· copy_x: bool, default=True - When pre-computing distances, indicates whether to modify the original data.\n· algorithm: {\"lloyd\", \"elkan\"}, default=\"lloyd\" - K-means algorithm to use, either \"lloyd\" or \"elkan\".\n· return_n_iter: bool, default=False - Whether or not to return the number of iterations.\n\n**Code Description**: The k_means function serves as a high-level interface for performing K-means clustering. It accepts a dataset X and various parameters that control the clustering process. The function initializes an instance of the KMeans class with the provided parameters and calls its fit method to execute the clustering algorithm. The KMeans class is responsible for managing the clustering process, including the initialization of cluster centers, the iterative assignment of data points to clusters, and the updating of cluster centers until convergence is achieved or the maximum number of iterations is reached.\n\nThe k_means function returns the final cluster centroids, the labels indicating the closest centroid for each observation, and the inertia, which measures how tightly the clusters are packed. If the return_n_iter parameter is set to True, it also returns the number of iterations taken to reach the final clustering solution.\n\nThe relationship between k_means and its callees is straightforward: k_means acts as a user-friendly wrapper around the KMeans class, allowing users to perform clustering without needing to directly interact with the class's internal methods.\n\n**Note**: It is essential to ensure that the input data X is properly formatted and that the parameters are set according to the desired clustering behavior. The choice of initialization method and the number of initializations can significantly impact the performance and outcome of the K-means clustering process.\n\n**Output Example**: A possible return value from the k_means function could be:\n```python\n(array([[10.,  2.],\n         [ 1.,  2.]]), \n array([1, 1, 1, 0, 0, 0], dtype=int32), \n 16.0)\n```\nThis output indicates the coordinates of the cluster centers, the cluster assignment for each sample in the input data, and the final inertia value."
      ],
      "code_start_line": 302,
      "code_end_line": 457,
      "params": [
        "X",
        "n_clusters"
      ],
      "have_return": true,
      "code_content": "def k_means(\n    X,\n    n_clusters,\n    *,\n    sample_weight=None,\n    init=\"k-means++\",\n    n_init=\"auto\",\n    max_iter=300,\n    verbose=False,\n    tol=1e-4,\n    random_state=None,\n    copy_x=True,\n    algorithm=\"lloyd\",\n    return_n_iter=False,\n):\n    \"\"\"Perform K-means clustering algorithm.\n\n    Read more in the :ref:`User Guide <k_means>`.\n\n    Parameters\n    ----------\n    X : {array-like, sparse matrix} of shape (n_samples, n_features)\n        The observations to cluster. It must be noted that the data\n        will be converted to C ordering, which will cause a memory copy\n        if the given data is not C-contiguous.\n\n    n_clusters : int\n        The number of clusters to form as well as the number of\n        centroids to generate.\n\n    sample_weight : array-like of shape (n_samples,), default=None\n        The weights for each observation in `X`. If `None`, all observations\n        are assigned equal weight. `sample_weight` is not used during\n        initialization if `init` is a callable or a user provided array.\n\n    init : {'k-means++', 'random'}, callable or array-like of shape \\\n            (n_clusters, n_features), default='k-means++'\n        Method for initialization:\n\n        - `'k-means++'` : selects initial cluster centers for k-mean\n          clustering in a smart way to speed up convergence. See section\n          Notes in k_init for more details.\n        - `'random'`: choose `n_clusters` observations (rows) at random from data\n          for the initial centroids.\n        - If an array is passed, it should be of shape `(n_clusters, n_features)`\n          and gives the initial centers.\n        - If a callable is passed, it should take arguments `X`, `n_clusters` and a\n          random state and return an initialization.\n\n    n_init : 'auto' or int, default=\"auto\"\n        Number of time the k-means algorithm will be run with different\n        centroid seeds. The final results will be the best output of\n        n_init consecutive runs in terms of inertia.\n\n        When `n_init='auto'`, the number of runs depends on the value of init:\n        10 if using `init='random'` or `init` is a callable;\n        1 if using `init='k-means++'` or `init` is an array-like.\n\n        .. versionadded:: 1.2\n           Added 'auto' option for `n_init`.\n\n        .. versionchanged:: 1.4\n           Default value for `n_init` changed to `'auto'`.\n\n    max_iter : int, default=300\n        Maximum number of iterations of the k-means algorithm to run.\n\n    verbose : bool, default=False\n        Verbosity mode.\n\n    tol : float, default=1e-4\n        Relative tolerance with regards to Frobenius norm of the difference\n        in the cluster centers of two consecutive iterations to declare\n        convergence.\n\n    random_state : int, RandomState instance or None, default=None\n        Determines random number generation for centroid initialization. Use\n        an int to make the randomness deterministic.\n        See :term:`Glossary <random_state>`.\n\n    copy_x : bool, default=True\n        When pre-computing distances it is more numerically accurate to center\n        the data first. If `copy_x` is True (default), then the original data is\n        not modified. If False, the original data is modified, and put back\n        before the function returns, but small numerical differences may be\n        introduced by subtracting and then adding the data mean. Note that if\n        the original data is not C-contiguous, a copy will be made even if\n        `copy_x` is False. If the original data is sparse, but not in CSR format,\n        a copy will be made even if `copy_x` is False.\n\n    algorithm : {\"lloyd\", \"elkan\"}, default=\"lloyd\"\n        K-means algorithm to use. The classical EM-style algorithm is `\"lloyd\"`.\n        The `\"elkan\"` variation can be more efficient on some datasets with\n        well-defined clusters, by using the triangle inequality. However it's\n        more memory intensive due to the allocation of an extra array of shape\n        `(n_samples, n_clusters)`.\n\n        .. versionchanged:: 0.18\n            Added Elkan algorithm\n\n        .. versionchanged:: 1.1\n            Renamed \"full\" to \"lloyd\", and deprecated \"auto\" and \"full\".\n            Changed \"auto\" to use \"lloyd\" instead of \"elkan\".\n\n    return_n_iter : bool, default=False\n        Whether or not to return the number of iterations.\n\n    Returns\n    -------\n    centroid : ndarray of shape (n_clusters, n_features)\n        Centroids found at the last iteration of k-means.\n\n    label : ndarray of shape (n_samples,)\n        The `label[i]` is the code or index of the centroid the\n        i'th observation is closest to.\n\n    inertia : float\n        The final value of the inertia criterion (sum of squared distances to\n        the closest centroid for all observations in the training set).\n\n    best_n_iter : int\n        Number of iterations corresponding to the best results.\n        Returned only if `return_n_iter` is set to True.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.cluster import k_means\n    >>> X = np.array([[1, 2], [1, 4], [1, 0],\n    ...               [10, 2], [10, 4], [10, 0]])\n    >>> centroid, label, inertia = k_means(\n    ...     X, n_clusters=2, n_init=\"auto\", random_state=0\n    ... )\n    >>> centroid\n    array([[10.,  2.],\n           [ 1.,  2.]])\n    >>> label\n    array([1, 1, 1, 0, 0, 0], dtype=int32)\n    >>> inertia\n    16.0\n    \"\"\"\n    est = KMeans(\n        n_clusters=n_clusters,\n        init=init,\n        n_init=n_init,\n        max_iter=max_iter,\n        verbose=verbose,\n        tol=tol,\n        random_state=random_state,\n        copy_x=copy_x,\n        algorithm=algorithm,\n    ).fit(X, sample_weight=sample_weight)\n    if return_n_iter:\n        return est.cluster_centers_, est.labels_, est.inertia_, est.n_iter_\n    else:\n        return est.cluster_centers_, est.labels_, est.inertia_\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "dataset/_kmeans.py/KMeans",
        "dataset/_kmeans.py/KMeans/fit"
      ],
      "special_reference_type": [
        false,
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "_kmeans_single_elkan",
      "md_content": [
        "**_kmeans_single_elkan**: The function of _kmeans_single_elkan is to perform a single run of the k-means clustering algorithm using the Elkan variant, which is optimized for speed and efficiency.\n\n**parameters**: The parameters of this Function.\n· X: {ndarray, sparse matrix} of shape (n_samples, n_features) - The observations to cluster. If a sparse matrix is provided, it must be in CSR format.\n· sample_weight: array-like of shape (n_samples,) - The weights for each observation in X.\n· centers_init: ndarray of shape (n_clusters, n_features) - The initial centers for the clusters.\n· max_iter: int, default=300 - Maximum number of iterations of the k-means algorithm to run.\n· verbose: bool, default=False - Verbosity mode to control the output of the function.\n· tol: float, default=1e-4 - Relative tolerance with respect to the Frobenius norm of the difference in the cluster centers of two consecutive iterations to declare convergence.\n· n_threads: int, default=1 - The number of OpenMP threads to use for the computation, allowing for parallelism on the main Cython loop.\n\n**Code Description**: The _kmeans_single_elkan function executes a single iteration of the k-means clustering algorithm using the Elkan method, which is designed to reduce the number of distance calculations needed during the clustering process. The function begins by initializing several buffers to store intermediate results, such as the new cluster centers, weights in clusters, and labels for each sample. It checks whether the input data X is sparse or dense and sets up the appropriate functions for initialization and iteration accordingly.\n\nThe function then enters a loop that runs for a maximum of max_iter iterations. In each iteration, it updates the cluster centers and assigns labels to the samples based on their proximity to the centers. The function also checks for convergence by comparing the current labels with the previous labels and by evaluating the shift in cluster centers against the specified tolerance. If convergence is achieved, the loop breaks early.\n\nAfter the iterations, the function performs a final assignment of labels to ensure that they match the updated cluster centers. It calculates the final inertia, which is the sum of squared distances from each sample to its closest cluster center, and returns the labels, inertia, final cluster centers, and the number of iterations run.\n\nThis function is called by the fit method of the KMeans class, which is responsible for computing k-means clustering. The fit method validates the input data, initializes the cluster centers, and then calls _kmeans_single_elkan to perform the clustering. The results from _kmeans_single_elkan are used to determine the best clustering configuration based on inertia and distinct clusters.\n\n**Note**: It is important to ensure that the input data X is in the correct format (either dense or sparse) and that the sample weights are appropriately defined. The function is optimized for performance, especially when using multiple threads, which can significantly speed up the clustering process.\n\n**Output Example**: \nAn example of the return value from the function could look like this:\n```python\nlabels = np.array([0, 1, 0, 1, 0])\ninertia = 12.34\ncentroids = np.array([[1.0, 2.0], [3.0, 4.0]])\nn_iter = 5\n``` \nThis output indicates the labels assigned to each sample, the final inertia value, the coordinates of the cluster centroids, and the number of iterations completed during the clustering process."
      ],
      "code_start_line": 460,
      "code_end_line": 622,
      "params": [
        "X",
        "sample_weight",
        "centers_init",
        "max_iter",
        "verbose",
        "tol",
        "n_threads"
      ],
      "have_return": true,
      "code_content": "def _kmeans_single_elkan(\n    X,\n    sample_weight,\n    centers_init,\n    max_iter=300,\n    verbose=False,\n    tol=1e-4,\n    n_threads=1,\n):\n    \"\"\"A single run of k-means elkan, assumes preparation completed prior.\n\n    Parameters\n    ----------\n    X : {ndarray, sparse matrix} of shape (n_samples, n_features)\n        The observations to cluster. If sparse matrix, must be in CSR format.\n\n    sample_weight : array-like of shape (n_samples,)\n        The weights for each observation in X.\n\n    centers_init : ndarray of shape (n_clusters, n_features)\n        The initial centers.\n\n    max_iter : int, default=300\n        Maximum number of iterations of the k-means algorithm to run.\n\n    verbose : bool, default=False\n        Verbosity mode.\n\n    tol : float, default=1e-4\n        Relative tolerance with regards to Frobenius norm of the difference\n        in the cluster centers of two consecutive iterations to declare\n        convergence.\n        It's not advised to set `tol=0` since convergence might never be\n        declared due to rounding errors. Use a very small number instead.\n\n    n_threads : int, default=1\n        The number of OpenMP threads to use for the computation. Parallelism is\n        sample-wise on the main cython loop which assigns each sample to its\n        closest center.\n\n    Returns\n    -------\n    centroid : ndarray of shape (n_clusters, n_features)\n        Centroids found at the last iteration of k-means.\n\n    label : ndarray of shape (n_samples,)\n        label[i] is the code or index of the centroid the\n        i'th observation is closest to.\n\n    inertia : float\n        The final value of the inertia criterion (sum of squared distances to\n        the closest centroid for all observations in the training set).\n\n    n_iter : int\n        Number of iterations run.\n    \"\"\"\n    n_samples = X.shape[0]\n    n_clusters = centers_init.shape[0]\n\n    # Buffers to avoid new allocations at each iteration.\n    centers = centers_init\n    centers_new = np.zeros_like(centers)\n    weight_in_clusters = np.zeros(n_clusters, dtype=X.dtype)\n    labels = np.full(n_samples, -1, dtype=np.int32)\n    labels_old = labels.copy()\n    center_half_distances = euclidean_distances(centers) / 2\n    distance_next_center = np.partition(\n        np.asarray(center_half_distances), kth=1, axis=0\n    )[1]\n    upper_bounds = np.zeros(n_samples, dtype=X.dtype)\n    lower_bounds = np.zeros((n_samples, n_clusters), dtype=X.dtype)\n    center_shift = np.zeros(n_clusters, dtype=X.dtype)\n\n    if sp.issparse(X):\n        init_bounds = init_bounds_sparse\n        elkan_iter = elkan_iter_chunked_sparse\n        _inertia = _inertia_sparse\n    else:\n        init_bounds = init_bounds_dense\n        elkan_iter = elkan_iter_chunked_dense\n        _inertia = _inertia_dense\n\n    init_bounds(\n        X,\n        centers,\n        center_half_distances,\n        labels,\n        upper_bounds,\n        lower_bounds,\n        n_threads=n_threads,\n    )\n\n    strict_convergence = False\n\n    for i in range(max_iter):\n        elkan_iter(\n            X,\n            sample_weight,\n            centers,\n            centers_new,\n            weight_in_clusters,\n            center_half_distances,\n            distance_next_center,\n            upper_bounds,\n            lower_bounds,\n            labels,\n            center_shift,\n            n_threads,\n        )\n\n        # compute new pairwise distances between centers and closest other\n        # center of each center for next iterations\n        center_half_distances = euclidean_distances(centers_new) / 2\n        distance_next_center = np.partition(\n            np.asarray(center_half_distances), kth=1, axis=0\n        )[1]\n\n        if verbose:\n            inertia = _inertia(X, sample_weight, centers, labels, n_threads)\n            print(f\"Iteration {i}, inertia {inertia}\")\n\n        centers, centers_new = centers_new, centers\n\n        if np.array_equal(labels, labels_old):\n            # First check the labels for strict convergence.\n            if verbose:\n                print(f\"Converged at iteration {i}: strict convergence.\")\n            strict_convergence = True\n            break\n        else:\n            # No strict convergence, check for tol based convergence.\n            center_shift_tot = (center_shift**2).sum()\n            if center_shift_tot <= tol:\n                if verbose:\n                    print(\n                        f\"Converged at iteration {i}: center shift \"\n                        f\"{center_shift_tot} within tolerance {tol}.\"\n                    )\n                break\n\n        labels_old[:] = labels\n\n    if not strict_convergence:\n        # rerun E-step so that predicted labels match cluster centers\n        elkan_iter(\n            X,\n            sample_weight,\n            centers,\n            centers,\n            weight_in_clusters,\n            center_half_distances,\n            distance_next_center,\n            upper_bounds,\n            lower_bounds,\n            labels,\n            center_shift,\n            n_threads,\n            update_centers=False,\n        )\n\n    inertia = _inertia(X, sample_weight, centers, labels, n_threads)\n\n    return labels, inertia, centers, i + 1\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "dataset/_kmeans.py/KMeans/fit"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "_kmeans_single_lloyd",
      "md_content": [
        "**_kmeans_single_lloyd**: The function of _kmeans_single_lloyd is to perform a single run of the k-means clustering algorithm using the Lloyd's method.\n\n**parameters**: The parameters of this Function.\n· X: {ndarray, sparse matrix} of shape (n_samples, n_features) - The observations to cluster. If a sparse matrix is provided, it must be in CSR format.  \n· sample_weight: ndarray of shape (n_samples,) - The weights for each observation in X.  \n· centers_init: ndarray of shape (n_clusters, n_features) - The initial centers for the clusters.  \n· max_iter: int, default=300 - The maximum number of iterations of the k-means algorithm to run.  \n· verbose: bool, default=False - Controls the verbosity of the output during the execution of the algorithm.  \n· tol: float, default=1e-4 - The relative tolerance with respect to the Frobenius norm of the difference in the cluster centers of two consecutive iterations, used to declare convergence.  \n· n_threads: int, default=1 - The number of OpenMP threads to use for computation, allowing for parallel processing on the main Cython loop.\n\n**Code Description**: The _kmeans_single_lloyd function implements the Lloyd's algorithm for k-means clustering. It begins by initializing several buffers to store the current and new cluster centers, labels for each observation, and other necessary variables. The function checks if the input data X is sparse or dense and assigns the appropriate iteration function and inertia calculation method accordingly.\n\nThe algorithm iterates up to max_iter times, updating the cluster centers and labels based on the distances of the observations to the centers. If the labels do not change between iterations, strict convergence is declared. If the change in cluster centers is below the specified tolerance (tol), convergence is also declared. The function finally calculates the inertia, which is the sum of squared distances from each observation to its closest cluster center, and returns the final labels, inertia, cluster centers, and the number of iterations run.\n\nThis function is called by the fit method of the KMeans class in the same module. During the fitting process, the KMeans class prepares the data and initializes the cluster centers before invoking _kmeans_single_lloyd to perform the clustering. The results from this function are then used to determine the best clustering configuration based on inertia and distinct clusters found.\n\n**Note**: It is important to ensure that the input data X is properly formatted and that the sample weights are correctly specified. The function assumes that the necessary preparations have been completed prior to its invocation.\n\n**Output Example**: A possible return value of the function could be:\n- labels: array([0, 1, 0, 1, 0])  # Indicating the closest centroid for each observation\n- inertia: 12.34  # The final inertia value\n- centers: array([[1.5, 2.5], [3.5, 4.5]])  # Final cluster centers\n- n_iter: 10  # Number of iterations run"
      ],
      "code_start_line": 625,
      "code_end_line": 756,
      "params": [
        "X",
        "sample_weight",
        "centers_init",
        "max_iter",
        "verbose",
        "tol",
        "n_threads"
      ],
      "have_return": true,
      "code_content": "def _kmeans_single_lloyd(\n    X,\n    sample_weight,\n    centers_init,\n    max_iter=300,\n    verbose=False,\n    tol=1e-4,\n    n_threads=1,\n):\n    \"\"\"A single run of k-means lloyd, assumes preparation completed prior.\n\n    Parameters\n    ----------\n    X : {ndarray, sparse matrix} of shape (n_samples, n_features)\n        The observations to cluster. If sparse matrix, must be in CSR format.\n\n    sample_weight : ndarray of shape (n_samples,)\n        The weights for each observation in X.\n\n    centers_init : ndarray of shape (n_clusters, n_features)\n        The initial centers.\n\n    max_iter : int, default=300\n        Maximum number of iterations of the k-means algorithm to run.\n\n    verbose : bool, default=False\n        Verbosity mode\n\n    tol : float, default=1e-4\n        Relative tolerance with regards to Frobenius norm of the difference\n        in the cluster centers of two consecutive iterations to declare\n        convergence.\n        It's not advised to set `tol=0` since convergence might never be\n        declared due to rounding errors. Use a very small number instead.\n\n    n_threads : int, default=1\n        The number of OpenMP threads to use for the computation. Parallelism is\n        sample-wise on the main cython loop which assigns each sample to its\n        closest center.\n\n    Returns\n    -------\n    centroid : ndarray of shape (n_clusters, n_features)\n        Centroids found at the last iteration of k-means.\n\n    label : ndarray of shape (n_samples,)\n        label[i] is the code or index of the centroid the\n        i'th observation is closest to.\n\n    inertia : float\n        The final value of the inertia criterion (sum of squared distances to\n        the closest centroid for all observations in the training set).\n\n    n_iter : int\n        Number of iterations run.\n    \"\"\"\n    n_clusters = centers_init.shape[0]\n\n    # Buffers to avoid new allocations at each iteration.\n    centers = centers_init\n    centers_new = np.zeros_like(centers)\n    labels = np.full(X.shape[0], -1, dtype=np.int32)\n    labels_old = labels.copy()\n    weight_in_clusters = np.zeros(n_clusters, dtype=X.dtype)\n    center_shift = np.zeros(n_clusters, dtype=X.dtype)\n\n    if sp.issparse(X):\n        lloyd_iter = lloyd_iter_chunked_sparse\n        _inertia = _inertia_sparse\n    else:\n        lloyd_iter = lloyd_iter_chunked_dense\n        _inertia = _inertia_dense\n\n    strict_convergence = False\n\n    # Threadpoolctl context to limit the number of threads in second level of\n    # nested parallelism (i.e. BLAS) to avoid oversubscription.\n    with threadpool_limits(limits=1, user_api=\"blas\"):\n        for i in range(max_iter):\n            lloyd_iter(\n                X,\n                sample_weight,\n                centers,\n                centers_new,\n                weight_in_clusters,\n                labels,\n                center_shift,\n                n_threads,\n            )\n\n            if verbose:\n                inertia = _inertia(X, sample_weight, centers, labels, n_threads)\n                print(f\"Iteration {i}, inertia {inertia}.\")\n\n            centers, centers_new = centers_new, centers\n\n            if np.array_equal(labels, labels_old):\n                # First check the labels for strict convergence.\n                if verbose:\n                    print(f\"Converged at iteration {i}: strict convergence.\")\n                strict_convergence = True\n                break\n            else:\n                # No strict convergence, check for tol based convergence.\n                center_shift_tot = (center_shift**2).sum()\n                if center_shift_tot <= tol:\n                    if verbose:\n                        print(\n                            f\"Converged at iteration {i}: center shift \"\n                            f\"{center_shift_tot} within tolerance {tol}.\"\n                        )\n                    break\n\n            labels_old[:] = labels\n\n        if not strict_convergence:\n            # rerun E-step so that predicted labels match cluster centers\n            lloyd_iter(\n                X,\n                sample_weight,\n                centers,\n                centers,\n                weight_in_clusters,\n                labels,\n                center_shift,\n                n_threads,\n                update_centers=False,\n            )\n\n    inertia = _inertia(X, sample_weight, centers, labels, n_threads)\n\n    return labels, inertia, centers, i + 1\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "dataset/_kmeans.py/KMeans/fit"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "_labels_inertia",
      "md_content": [
        "**_labels_inertia**: The function of _labels_inertia is to compute the labels and the inertia of the given samples and centers in the K-means EM algorithm.\n\n**parameters**: The parameters of this Function.\n· X: {ndarray, sparse matrix} of shape (n_samples, n_features) - The input samples to assign to the labels. If sparse matrix, must be in CSR format.  \n· sample_weight: ndarray of shape (n_samples,) - The weights for each observation in X.  \n· centers: ndarray of shape (n_clusters, n_features) - The cluster centers.  \n· n_threads: int, default=1 - The number of OpenMP threads to use for the computation. Parallelism is sample-wise on the main cython loop which assigns each sample to its closest center.  \n· return_inertia: bool, default=True - Whether to compute and return the inertia.  \n\n**Code Description**: The _labels_inertia function is a critical component of the K-means clustering algorithm, specifically designed to perform the expectation step (E step) of the K-means EM algorithm. It takes a dataset X and a set of cluster centers and computes the closest cluster for each sample in X, assigning labels accordingly. The function also calculates the inertia, which is the sum of squared distances from each sample to its nearest cluster center, if requested.\n\nThe function begins by determining the number of samples and clusters based on the input data shapes. It initializes an array to hold the labels for each sample, setting them to -1 initially. Depending on whether the input data X is a sparse matrix or a dense array, it selects the appropriate implementation for label assignment and inertia calculation.\n\nThe label assignment is performed by calling either the _labels function for sparse or dense data. After the labels are assigned, if the return_inertia parameter is set to True, the function computes the inertia using the selected inertia calculation method and returns both the labels and the inertia value. If return_inertia is False, only the labels are returned.\n\nThis function is called by other functions within the K-means implementation, such as _mini_batch_step, which utilizes _labels_inertia to first assign labels to the samples before updating the cluster centers. This relationship highlights the function's role in the overall K-means algorithm, where accurate label assignment is essential for effective clustering.\n\n**Note**: It is important to ensure that the input data X is in the correct format (ndarray or CSR sparse matrix) and that the sample weights are appropriately defined. The function is optimized for performance with the option to utilize multiple threads for computation.\n\n**Output Example**: A possible return value of the function when called with a dataset of 5 samples and 3 clusters might look like this:  \nLabels: [0, 1, 0, 2, 1]  \nInertia: 12.34"
      ],
      "code_start_line": 759,
      "code_end_line": 826,
      "params": [
        "X",
        "sample_weight",
        "centers",
        "n_threads",
        "return_inertia"
      ],
      "have_return": true,
      "code_content": "def _labels_inertia(X, sample_weight, centers, n_threads=1, return_inertia=True):\n    \"\"\"E step of the K-means EM algorithm.\n\n    Compute the labels and the inertia of the given samples and centers.\n\n    Parameters\n    ----------\n    X : {ndarray, sparse matrix} of shape (n_samples, n_features)\n        The input samples to assign to the labels. If sparse matrix, must\n        be in CSR format.\n\n    sample_weight : ndarray of shape (n_samples,)\n        The weights for each observation in X.\n\n    x_squared_norms : ndarray of shape (n_samples,)\n        Precomputed squared euclidean norm of each data point, to speed up\n        computations.\n\n    centers : ndarray of shape (n_clusters, n_features)\n        The cluster centers.\n\n    n_threads : int, default=1\n        The number of OpenMP threads to use for the computation. Parallelism is\n        sample-wise on the main cython loop which assigns each sample to its\n        closest center.\n\n    return_inertia : bool, default=True\n        Whether to compute and return the inertia.\n\n    Returns\n    -------\n    labels : ndarray of shape (n_samples,)\n        The resulting assignment.\n\n    inertia : float\n        Sum of squared distances of samples to their closest cluster center.\n        Inertia is only returned if return_inertia is True.\n    \"\"\"\n    n_samples = X.shape[0]\n    n_clusters = centers.shape[0]\n\n    labels = np.full(n_samples, -1, dtype=np.int32)\n    center_shift = np.zeros(n_clusters, dtype=centers.dtype)\n\n    if sp.issparse(X):\n        _labels = lloyd_iter_chunked_sparse\n        _inertia = _inertia_sparse\n    else:\n        _labels = lloyd_iter_chunked_dense\n        _inertia = _inertia_dense\n\n    _labels(\n        X,\n        sample_weight,\n        centers,\n        centers_new=None,\n        weight_in_clusters=None,\n        labels=labels,\n        center_shift=center_shift,\n        n_threads=n_threads,\n        update_centers=False,\n    )\n\n    if return_inertia:\n        inertia = _inertia(X, sample_weight, centers, labels, n_threads)\n        return labels, inertia\n\n    return labels\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "dataset/_kmeans.py/_labels_inertia_threadpool_limit",
        "dataset/_kmeans.py/_mini_batch_step"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "_labels_inertia_threadpool_limit",
      "md_content": [
        "**_labels_inertia_threadpool_limit**: The function of _labels_inertia_threadpool_limit is to compute the labels and inertia of the given samples and centers in a controlled thread pool context.\n\n**parameters**: The parameters of this Function.\n· X: {ndarray, sparse matrix} of shape (n_samples, n_features) - The input samples to assign to the labels. If sparse matrix, must be in CSR format.  \n· sample_weight: ndarray of shape (n_samples,) - The weights for each observation in X.  \n· centers: ndarray of shape (n_clusters, n_features) - The cluster centers.  \n· n_threads: int, default=1 - The number of OpenMP threads to use for the computation. Parallelism is sample-wise on the main cython loop which assigns each sample to its closest center.  \n· return_inertia: bool, default=True - Whether to compute and return the inertia.  \n\n**Code Description**: The _labels_inertia_threadpool_limit function serves as a wrapper around the _labels_inertia function, specifically designed to operate within a thread pool limit context. This function ensures that the computation of labels and inertia adheres to a specified threading limit, which is particularly useful in environments where resource management is critical.\n\nUpon invocation, the function sets the thread pool limit to 1 for the BLAS user API, ensuring that the underlying computations do not exceed this limit. It then calls the _labels_inertia function, passing along the parameters it received. The _labels_inertia function is responsible for computing the labels for each sample in the dataset X by determining the closest cluster center and, if requested, calculating the inertia, which quantifies the compactness of the clusters.\n\nThe _labels_inertia_threadpool_limit function is called by several methods within the K-means implementation, including the predict method of the _BaseKMeans class, the score method, and the fit and partial_fit methods of the MiniBatchKMeans class. In these contexts, it is used to obtain the labels for new data points or to evaluate the clustering performance on validation sets. The function's design allows it to maintain efficient resource usage while performing essential computations for the K-means algorithm.\n\n**Note**: It is important to ensure that the input data X is in the correct format (ndarray or CSR sparse matrix) and that the sample weights are appropriately defined. The function is optimized for performance with the option to utilize multiple threads for computation, but it enforces a limit to prevent excessive resource consumption.\n\n**Output Example**: A possible return value of the function when called with a dataset of 5 samples and 3 clusters might look like this:  \nLabels: [0, 1, 0, 2, 1]  \nInertia: 12.34"
      ],
      "code_start_line": 829,
      "code_end_line": 836,
      "params": [
        "X",
        "sample_weight",
        "centers",
        "n_threads",
        "return_inertia"
      ],
      "have_return": true,
      "code_content": "def _labels_inertia_threadpool_limit(\n    X, sample_weight, centers, n_threads=1, return_inertia=True\n):\n    \"\"\"Same as _labels_inertia but in a threadpool_limits context.\"\"\"\n    with threadpool_limits(limits=1, user_api=\"blas\"):\n        result = _labels_inertia(X, sample_weight, centers, n_threads, return_inertia)\n\n    return result\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "dataset/_kmeans.py/_BaseKMeans/predict",
        "dataset/_kmeans.py/_BaseKMeans/score",
        "dataset/_kmeans.py/MiniBatchKMeans/fit",
        "dataset/_kmeans.py/MiniBatchKMeans/partial_fit"
      ],
      "reference_who": [
        "dataset/_kmeans.py/_labels_inertia"
      ],
      "special_reference_type": [
        false
      ]
    },
    {
      "type": "ClassDef",
      "name": "_BaseKMeans",
      "md_content": [
        "**_BaseKMeans**: The function of _BaseKMeans is to serve as a base class for KMeans and MiniBatchKMeans clustering algorithms, providing shared functionality and parameter validation.\n\n**attributes**: The attributes of this Class.\n· n_clusters: The number of clusters to form.\n· init: Method for initialization of cluster centers.\n· n_init: Number of times the k-means algorithm will be run with different centroid seeds.\n· max_iter: Maximum number of iterations of the k-means algorithm for a single run.\n· tol: Relative tolerance with regards to Frobenius norm of the difference in the cluster centers of two consecutive iterations to declare convergence.\n· verbose: Verbosity mode.\n· random_state: Determines random number generation for centroid initialization.\n· _parameter_constraints: A dictionary that defines the constraints for the parameters.\n\n**Code Description**: The _BaseKMeans class is an abstract base class that provides the foundational structure for implementing KMeans clustering algorithms. It inherits from several mixins and base classes, including ClassNamePrefixFeaturesOutMixin, TransformerMixin, ClusterMixin, BaseEstimator, and ABC, which equip it with various functionalities such as transformation, clustering, and parameter validation.\n\nThe class defines a set of parameter constraints that ensure the validity of the input parameters when initializing the clustering algorithm. The constructor initializes the parameters and performs validation checks to ensure that the number of samples in the input data is greater than or equal to the number of clusters specified.\n\nThe class includes several methods that are crucial for the clustering process:\n- _check_params_vs_input: Validates the input parameters against the data provided, ensuring that the number of clusters does not exceed the number of samples and that the initialization method is appropriate.\n- _validate_center_shape: Checks if the shape of the initial cluster centers is compatible with the input data and the specified number of clusters.\n- _init_centroids: Computes the initial centroids based on the specified initialization method, which can be 'k-means++', 'random', or a user-defined callable or array.\n- fit_predict, predict, fit_transform, and transform: These methods provide the functionality to fit the model to the data, predict cluster assignments, and transform the data into a cluster-distance space.\n\nThe _BaseKMeans class is designed to be subclassed by specific implementations of KMeans, such as the KMeans and MiniBatchKMeans classes. These subclasses inherit the core functionality of _BaseKMeans while adding specific features and optimizations relevant to their respective algorithms. For instance, KMeans implements the standard batch KMeans algorithm, while MiniBatchKMeans is optimized for large datasets by processing data in smaller batches.\n\n**Note**: When using the _BaseKMeans class, it is essential to ensure that the input data meets the specified constraints, particularly regarding the number of samples and clusters. Additionally, users should be aware of the initialization methods and their implications on the clustering results.\n\n**Output Example**: A possible return value from the fit_predict method could be an array of cluster labels, such as:\n```\narray([0, 0, 1, 1, 0, 1], dtype=int32)\n``` \nThis output indicates the cluster assignment for each sample in the input data, where each unique integer represents a different cluster."
      ],
      "code_start_line": 839,
      "code_end_line": 1210,
      "params": [],
      "have_return": true,
      "code_content": "class _BaseKMeans(\n    ClassNamePrefixFeaturesOutMixin, TransformerMixin, ClusterMixin, BaseEstimator, ABC\n):\n    \"\"\"Base class for KMeans and MiniBatchKMeans\"\"\"\n\n    _parameter_constraints: dict = {\n        \"n_clusters\": [Interval(Integral, 1, None, closed=\"left\")],\n        \"init\": [StrOptions({\"k-means++\", \"random\"}), callable, \"array-like\"],\n        \"n_init\": [\n            StrOptions({\"auto\"}),\n            Interval(Integral, 1, None, closed=\"left\"),\n        ],\n        \"max_iter\": [Interval(Integral, 1, None, closed=\"left\")],\n        \"tol\": [Interval(Real, 0, None, closed=\"left\")],\n        \"verbose\": [\"verbose\"],\n        \"random_state\": [\"random_state\"],\n    }\n\n    def __init__(\n        self,\n        n_clusters,\n        *,\n        init,\n        n_init,\n        max_iter,\n        tol,\n        verbose,\n        random_state,\n    ):\n        self.n_clusters = n_clusters\n        self.init = init\n        self.max_iter = max_iter\n        self.tol = tol\n        self.n_init = n_init\n        self.verbose = verbose\n        self.random_state = random_state\n\n    def _check_params_vs_input(self, X, default_n_init=None):\n        # n_clusters\n        if X.shape[0] < self.n_clusters:\n            raise ValueError(\n                f\"n_samples={X.shape[0]} should be >= n_clusters={self.n_clusters}.\"\n            )\n\n        # tol\n        self._tol = _tolerance(X, self.tol)\n\n        # n-init\n        if self.n_init == \"auto\":\n            if isinstance(self.init, str) and self.init == \"k-means++\":\n                self._n_init = 1\n            elif isinstance(self.init, str) and self.init == \"random\":\n                self._n_init = default_n_init\n            elif callable(self.init):\n                self._n_init = default_n_init\n            else:  # array-like\n                self._n_init = 1\n        else:\n            self._n_init = self.n_init\n\n        if _is_arraylike_not_scalar(self.init) and self._n_init != 1:\n            warnings.warn(\n                (\n                    \"Explicit initial center position passed: performing only\"\n                    f\" one init in {self.__class__.__name__} instead of \"\n                    f\"n_init={self._n_init}.\"\n                ),\n                RuntimeWarning,\n                stacklevel=2,\n            )\n            self._n_init = 1\n\n    @abstractmethod\n    def _warn_mkl_vcomp(self, n_active_threads):\n        \"\"\"Issue an estimator specific warning when vcomp and mkl are both present\n\n        This method is called by `_check_mkl_vcomp`.\n        \"\"\"\n\n    def _check_mkl_vcomp(self, X, n_samples):\n        \"\"\"Check when vcomp and mkl are both present\"\"\"\n        # The BLAS call inside a prange in lloyd_iter_chunked_dense is known to\n        # cause a small memory leak when there are less chunks than the number\n        # of available threads. It only happens when the OpenMP library is\n        # vcomp (microsoft OpenMP) and the BLAS library is MKL. see #18653\n        if sp.issparse(X):\n            return\n\n        n_active_threads = int(np.ceil(n_samples / CHUNK_SIZE))\n        if n_active_threads < self._n_threads:\n            modules = threadpool_info()\n            has_vcomp = \"vcomp\" in [module[\"prefix\"] for module in modules]\n            has_mkl = (\"mkl\", \"intel\") in [\n                (module[\"internal_api\"], module.get(\"threading_layer\", None))\n                for module in modules\n            ]\n            if has_vcomp and has_mkl:\n                self._warn_mkl_vcomp(n_active_threads)\n\n    def _validate_center_shape(self, X, centers):\n        \"\"\"Check if centers is compatible with X and n_clusters.\"\"\"\n        if centers.shape[0] != self.n_clusters:\n            raise ValueError(\n                f\"The shape of the initial centers {centers.shape} does not \"\n                f\"match the number of clusters {self.n_clusters}.\"\n            )\n        if centers.shape[1] != X.shape[1]:\n            raise ValueError(\n                f\"The shape of the initial centers {centers.shape} does not \"\n                f\"match the number of features of the data {X.shape[1]}.\"\n            )\n\n    def _check_test_data(self, X):\n        X = self._validate_data(\n            X,\n            accept_sparse=\"csr\",\n            reset=False,\n            dtype=[np.float64, np.float32],\n            order=\"C\",\n            accept_large_sparse=False,\n        )\n        return X\n\n    def _init_centroids(\n        self,\n        X,\n        x_squared_norms,\n        init,\n        random_state,\n        sample_weight,\n        init_size=None,\n        n_centroids=None,\n    ):\n        \"\"\"Compute the initial centroids.\n\n        Parameters\n        ----------\n        X : {ndarray, sparse matrix} of shape (n_samples, n_features)\n            The input samples.\n\n        x_squared_norms : ndarray of shape (n_samples,)\n            Squared euclidean norm of each data point. Pass it if you have it\n            at hands already to avoid it being recomputed here.\n\n        init : {'k-means++', 'random'}, callable or ndarray of shape \\\n                (n_clusters, n_features)\n            Method for initialization.\n\n        random_state : RandomState instance\n            Determines random number generation for centroid initialization.\n            See :term:`Glossary <random_state>`.\n\n        sample_weight : ndarray of shape (n_samples,)\n            The weights for each observation in X. `sample_weight` is not used\n            during initialization if `init` is a callable or a user provided\n            array.\n\n        init_size : int, default=None\n            Number of samples to randomly sample for speeding up the\n            initialization (sometimes at the expense of accuracy).\n\n        n_centroids : int, default=None\n            Number of centroids to initialize.\n            If left to 'None' the number of centroids will be equal to\n            number of clusters to form (self.n_clusters).\n\n        Returns\n        -------\n        centers : ndarray of shape (n_clusters, n_features)\n            Initial centroids of clusters.\n        \"\"\"\n        n_samples = X.shape[0]\n        n_clusters = self.n_clusters if n_centroids is None else n_centroids\n\n        if init_size is not None and init_size < n_samples:\n            init_indices = random_state.randint(0, n_samples, init_size)\n            X = X[init_indices]\n            x_squared_norms = x_squared_norms[init_indices]\n            n_samples = X.shape[0]\n            sample_weight = sample_weight[init_indices]\n\n        if isinstance(init, str) and init == \"k-means++\":\n            centers, _ = _kmeans_plusplus(\n                X,\n                n_clusters,\n                random_state=random_state,\n                x_squared_norms=x_squared_norms,\n                sample_weight=sample_weight,\n            )\n        elif isinstance(init, str) and init == \"random\":\n            seeds = random_state.choice(\n                n_samples,\n                size=n_clusters,\n                replace=False,\n                p=sample_weight / sample_weight.sum(),\n            )\n            centers = X[seeds]\n        elif _is_arraylike_not_scalar(self.init):\n            centers = init\n        elif callable(init):\n            centers = init(X, n_clusters, random_state=random_state)\n            centers = check_array(centers, dtype=X.dtype, copy=False, order=\"C\")\n            self._validate_center_shape(X, centers)\n\n        if sp.issparse(centers):\n            centers = centers.toarray()\n\n        return centers\n\n    def fit_predict(self, X, y=None, sample_weight=None):\n        \"\"\"Compute cluster centers and predict cluster index for each sample.\n\n        Convenience method; equivalent to calling fit(X) followed by\n        predict(X).\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            New data to transform.\n\n        y : Ignored\n            Not used, present here for API consistency by convention.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            The weights for each observation in X. If None, all observations\n            are assigned equal weight.\n\n        Returns\n        -------\n        labels : ndarray of shape (n_samples,)\n            Index of the cluster each sample belongs to.\n        \"\"\"\n        return self.fit(X, sample_weight=sample_weight).labels_\n\n    def predict(self, X, sample_weight=\"deprecated\"):\n        \"\"\"Predict the closest cluster each sample in X belongs to.\n\n        In the vector quantization literature, `cluster_centers_` is called\n        the code book and each value returned by `predict` is the index of\n        the closest code in the code book.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            New data to predict.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            The weights for each observation in X. If None, all observations\n            are assigned equal weight.\n\n            .. deprecated:: 1.3\n               The parameter `sample_weight` is deprecated in version 1.3\n               and will be removed in 1.5.\n\n        Returns\n        -------\n        labels : ndarray of shape (n_samples,)\n            Index of the cluster each sample belongs to.\n        \"\"\"\n        check_is_fitted(self)\n\n        X = self._check_test_data(X)\n        if not (isinstance(sample_weight, str) and sample_weight == \"deprecated\"):\n            warnings.warn(\n                (\n                    \"'sample_weight' was deprecated in version 1.3 and \"\n                    \"will be removed in 1.5.\"\n                ),\n                FutureWarning,\n            )\n            sample_weight = _check_sample_weight(sample_weight, X, dtype=X.dtype)\n        else:\n            sample_weight = _check_sample_weight(None, X, dtype=X.dtype)\n\n        labels = _labels_inertia_threadpool_limit(\n            X,\n            sample_weight,\n            self.cluster_centers_,\n            n_threads=self._n_threads,\n            return_inertia=False,\n        )\n\n        return labels\n\n    def fit_transform(self, X, y=None, sample_weight=None):\n        \"\"\"Compute clustering and transform X to cluster-distance space.\n\n        Equivalent to fit(X).transform(X), but more efficiently implemented.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            New data to transform.\n\n        y : Ignored\n            Not used, present here for API consistency by convention.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            The weights for each observation in X. If None, all observations\n            are assigned equal weight.\n\n        Returns\n        -------\n        X_new : ndarray of shape (n_samples, n_clusters)\n            X transformed in the new space.\n        \"\"\"\n        return self.fit(X, sample_weight=sample_weight)._transform(X)\n\n    def transform(self, X):\n        \"\"\"Transform X to a cluster-distance space.\n\n        In the new space, each dimension is the distance to the cluster\n        centers. Note that even if X is sparse, the array returned by\n        `transform` will typically be dense.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            New data to transform.\n\n        Returns\n        -------\n        X_new : ndarray of shape (n_samples, n_clusters)\n            X transformed in the new space.\n        \"\"\"\n        check_is_fitted(self)\n\n        X = self._check_test_data(X)\n        return self._transform(X)\n\n    def _transform(self, X):\n        \"\"\"Guts of transform method; no input validation.\"\"\"\n        return euclidean_distances(X, self.cluster_centers_)\n\n    def score(self, X, y=None, sample_weight=None):\n        \"\"\"Opposite of the value of X on the K-means objective.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            New data.\n\n        y : Ignored\n            Not used, present here for API consistency by convention.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            The weights for each observation in X. If None, all observations\n            are assigned equal weight.\n\n        Returns\n        -------\n        score : float\n            Opposite of the value of X on the K-means objective.\n        \"\"\"\n        check_is_fitted(self)\n\n        X = self._check_test_data(X)\n        sample_weight = _check_sample_weight(sample_weight, X, dtype=X.dtype)\n\n        _, scores = _labels_inertia_threadpool_limit(\n            X, sample_weight, self.cluster_centers_, self._n_threads\n        )\n        return -scores\n\n    def _more_tags(self):\n        return {\n            \"_xfail_checks\": {\n                \"check_sample_weights_invariance\": (\n                    \"zero sample_weight is not equivalent to removing samples\"\n                ),\n            },\n        }\n",
      "name_column": 6,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "dataset/_kmeans.py/KMeans",
        "dataset/_kmeans.py/MiniBatchKMeans"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [
        "**__init__**: The function of __init__ is to initialize an instance of the BaseKMeans class with specified parameters for clustering.\n\n**parameters**: The parameters of this Function.\n· n_clusters: The number of clusters to form as well as the number of centroids to generate.\n· init: Method for initialization of the centroids. This can be a string or an array.\n· n_init: Number of times the k-means algorithm will be run with different centroid seeds.\n· max_iter: Maximum number of iterations of the k-means algorithm for a single run.\n· tol: Relative tolerance with regards to inertia to declare convergence.\n· verbose: Controls the verbosity of the output during the fitting process.\n· random_state: Determines random number generation for centroid initialization. Use an int for reproducibility.\n\n**Code Description**: The __init__ function serves as the constructor for the BaseKMeans class. It takes several parameters that dictate how the k-means clustering algorithm will operate. The parameter n_clusters specifies how many clusters the algorithm should identify in the dataset. The init parameter determines how the initial centroids are chosen, which can significantly affect the outcome of the clustering. The n_init parameter indicates how many times the algorithm should be executed with different initial centroid seeds to ensure a more reliable result. The max_iter parameter sets a limit on the number of iterations for each run of the algorithm, while tol provides a threshold for determining when the algorithm has converged. The verbose parameter allows users to control the level of detail in the output messages during the fitting process, which can be useful for debugging or understanding the algorithm's progress. Lastly, the random_state parameter ensures that the results can be reproduced by controlling the randomness in the initialization process.\n\n**Note**: It is important to provide appropriate values for these parameters to achieve optimal clustering results. Users should be aware that the choice of n_clusters can significantly influence the performance and outcome of the k-means algorithm. Additionally, setting a random_state can help in obtaining consistent results across different runs."
      ],
      "code_start_line": 857,
      "code_end_line": 874,
      "params": [
        "self",
        "n_clusters"
      ],
      "have_return": false,
      "code_content": "    def __init__(\n        self,\n        n_clusters,\n        *,\n        init,\n        n_init,\n        max_iter,\n        tol,\n        verbose,\n        random_state,\n    ):\n        self.n_clusters = n_clusters\n        self.init = init\n        self.max_iter = max_iter\n        self.tol = tol\n        self.n_init = n_init\n        self.verbose = verbose\n        self.random_state = random_state\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "_check_params_vs_input",
      "md_content": [
        "**_check_params_vs_input**: The function of _check_params_vs_input is to validate the input parameters against the dataset provided to ensure that the KMeans clustering algorithm can operate correctly.\n\n**parameters**: The parameters of this Function.\n· parameter1: X - The input dataset, which is expected to be a two-dimensional array-like structure representing the samples and features.\n· parameter2: default_n_init - An optional integer that specifies the default number of initializations to use if the initialization method is set to \"random\" or a callable.\n\n**Code Description**: The _check_params_vs_input function performs several critical checks and assignments related to the parameters used in the KMeans clustering algorithm. \n\nFirst, it verifies that the number of samples in the dataset X (represented by X.shape[0]) is greater than or equal to the number of clusters specified by self.n_clusters. If this condition is not met, a ValueError is raised, indicating that the number of samples must be sufficient to form the requested clusters.\n\nNext, the function calculates the tolerance value by calling the _tolerance function, passing the dataset X and the tolerance parameter self.tol. This computed tolerance is stored in the instance variable self._tol, which is essential for determining the convergence criteria during the clustering process.\n\nThe function then assesses the initialization method specified by self.init and determines the appropriate number of initializations (self._n_init) to perform. If self.n_init is set to \"auto\", the function checks the type of self.init. If it is a string and equals \"k-means++\", it sets self._n_init to 1. If it is \"random\", it assigns the value of default_n_init to self._n_init. If self.init is a callable, it also uses default_n_init. For any other case, including when self.init is an array-like structure, it defaults to 1.\n\nAdditionally, if self.init is an array-like structure and self._n_init is not equal to 1, a warning is issued to inform the user that only one initialization will be performed, despite the specified n_init value. This warning is raised as a RuntimeWarning, indicating that the behavior of the algorithm may differ from the user's expectations.\n\nThis function is integral to the operation of the KMeans algorithm, as it ensures that the parameters are correctly set up before the clustering process begins, thereby influencing the algorithm's performance and results.\n\n**Note**: It is crucial to ensure that the dataset X has enough samples relative to the number of clusters specified. Additionally, the initialization method and the number of initializations should be chosen carefully, as they can significantly impact the convergence and final clustering results."
      ],
      "code_start_line": 876,
      "code_end_line": 909,
      "params": [
        "self",
        "X",
        "default_n_init"
      ],
      "have_return": false,
      "code_content": "    def _check_params_vs_input(self, X, default_n_init=None):\n        # n_clusters\n        if X.shape[0] < self.n_clusters:\n            raise ValueError(\n                f\"n_samples={X.shape[0]} should be >= n_clusters={self.n_clusters}.\"\n            )\n\n        # tol\n        self._tol = _tolerance(X, self.tol)\n\n        # n-init\n        if self.n_init == \"auto\":\n            if isinstance(self.init, str) and self.init == \"k-means++\":\n                self._n_init = 1\n            elif isinstance(self.init, str) and self.init == \"random\":\n                self._n_init = default_n_init\n            elif callable(self.init):\n                self._n_init = default_n_init\n            else:  # array-like\n                self._n_init = 1\n        else:\n            self._n_init = self.n_init\n\n        if _is_arraylike_not_scalar(self.init) and self._n_init != 1:\n            warnings.warn(\n                (\n                    \"Explicit initial center position passed: performing only\"\n                    f\" one init in {self.__class__.__name__} instead of \"\n                    f\"n_init={self._n_init}.\"\n                ),\n                RuntimeWarning,\n                stacklevel=2,\n            )\n            self._n_init = 1\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "dataset/_kmeans.py/_tolerance"
      ],
      "special_reference_type": [
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "_warn_mkl_vcomp",
      "md_content": [
        "**_warn_mkl_vcomp**: The function of _warn_mkl_vcomp is to issue a specific warning related to the use of vcomp and MKL libraries when both are present in the environment.\n\n**parameters**: The parameters of this Function.\n· n_active_threads: An integer representing the number of active threads that are currently being utilized.\n\n**Code Description**: The _warn_mkl_vcomp function is designed to issue a warning when both the vcomp (Microsoft OpenMP) and MKL (Intel Math Kernel Library) libraries are detected in the execution environment. This function is invoked by the _check_mkl_vcomp method, which is responsible for checking the compatibility of these libraries during the execution of KMeans clustering algorithms.\n\nThe _check_mkl_vcomp method first assesses whether the input data X is sparse. If it is not sparse, the method calculates the number of active threads based on the number of samples and a predefined chunk size. It then checks if the number of active threads is less than the number of threads configured for the KMeans instance. If this condition is met, the method retrieves information about the threading modules in use and checks for the presence of both vcomp and MKL.\n\nIf both libraries are present, the _warn_mkl_vcomp function is called with the number of active threads as an argument. This warning mechanism is crucial as it addresses a known issue where the combination of these libraries can lead to performance degradation or memory leaks during the execution of the KMeans algorithm.\n\n**Note**: It is important for developers to be aware of the implications of using both vcomp and MKL together, as this can affect the stability and performance of the KMeans clustering process. Proper configuration and awareness of the threading libraries in use are essential to avoid potential issues."
      ],
      "code_start_line": 912,
      "code_end_line": 916,
      "params": [
        "self",
        "n_active_threads"
      ],
      "have_return": false,
      "code_content": "    def _warn_mkl_vcomp(self, n_active_threads):\n        \"\"\"Issue an estimator specific warning when vcomp and mkl are both present\n\n        This method is called by `_check_mkl_vcomp`.\n        \"\"\"\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "dataset/_kmeans.py/_BaseKMeans/_check_mkl_vcomp"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "_check_mkl_vcomp",
      "md_content": [
        "**_check_mkl_vcomp**: The function of _check_mkl_vcomp is to verify the compatibility of the vcomp and MKL libraries when both are present in the execution environment.\n\n**parameters**: The parameters of this Function.\n· X: The input data, which can be either a dense array or a sparse matrix, representing the samples to be clustered.\n· n_samples: An integer representing the total number of samples in the input data X.\n\n**Code Description**: The _check_mkl_vcomp function is designed to assess the compatibility of the vcomp (Microsoft OpenMP) and MKL (Intel Math Kernel Library) libraries during the execution of KMeans clustering algorithms. This function is particularly important as it addresses a known issue where the combination of these libraries can lead to performance degradation or memory leaks.\n\nThe function begins by checking if the input data X is sparse using the `sp.issparse` method. If X is sparse, the function exits early, as the compatibility check is not necessary for sparse data. \n\nNext, the function calculates the number of active threads based on the number of samples and a predefined chunk size (CHUNK_SIZE). It then compares the number of active threads with the number of threads configured for the KMeans instance (self._n_threads). If the number of active threads is less than the configured threads, the function proceeds to gather information about the threading modules currently in use.\n\nThe function retrieves the threading module information using the `threadpool_info()` function. It checks for the presence of both vcomp and MKL by examining the module information. Specifically, it looks for \"vcomp\" in the list of module prefixes and checks if the tuple (\"mkl\", \"intel\") is present in the internal API and threading layer information.\n\nIf both vcomp and MKL are detected, the function invokes the _warn_mkl_vcomp method, passing the number of active threads as an argument. This warning mechanism is crucial for informing users about the potential issues that may arise from using both libraries simultaneously.\n\nThe _check_mkl_vcomp function is called within the fit method of both the KMeans and MiniBatchKMeans classes. In the KMeans fit method, it is called after the input data has been validated and before the clustering algorithm is executed. In the MiniBatchKMeans fit method, it is called similarly, ensuring that the compatibility check is performed before processing the mini-batches of data. Additionally, it is also invoked in the partial_fit method of MiniBatchKMeans to ensure compatibility during incremental updates.\n\n**Note**: Developers should be aware of the implications of using both vcomp and MKL together, as this can affect the stability and performance of the KMeans clustering process. Proper configuration and awareness of the threading libraries in use are essential to avoid potential issues.\n\n**Output Example**: The function does not return a value but may trigger a warning if both vcomp and MKL are detected in the environment. An example warning message could be: \"Warning: Both vcomp and MKL are present. This may lead to performance issues.\""
      ],
      "code_start_line": 918,
      "code_end_line": 936,
      "params": [
        "self",
        "X",
        "n_samples"
      ],
      "have_return": true,
      "code_content": "    def _check_mkl_vcomp(self, X, n_samples):\n        \"\"\"Check when vcomp and mkl are both present\"\"\"\n        # The BLAS call inside a prange in lloyd_iter_chunked_dense is known to\n        # cause a small memory leak when there are less chunks than the number\n        # of available threads. It only happens when the OpenMP library is\n        # vcomp (microsoft OpenMP) and the BLAS library is MKL. see #18653\n        if sp.issparse(X):\n            return\n\n        n_active_threads = int(np.ceil(n_samples / CHUNK_SIZE))\n        if n_active_threads < self._n_threads:\n            modules = threadpool_info()\n            has_vcomp = \"vcomp\" in [module[\"prefix\"] for module in modules]\n            has_mkl = (\"mkl\", \"intel\") in [\n                (module[\"internal_api\"], module.get(\"threading_layer\", None))\n                for module in modules\n            ]\n            if has_vcomp and has_mkl:\n                self._warn_mkl_vcomp(n_active_threads)\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "dataset/_kmeans.py/KMeans/fit",
        "dataset/_kmeans.py/MiniBatchKMeans/fit",
        "dataset/_kmeans.py/MiniBatchKMeans/partial_fit"
      ],
      "reference_who": [
        "dataset/_kmeans.py/_BaseKMeans/_warn_mkl_vcomp"
      ],
      "special_reference_type": [
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "_validate_center_shape",
      "md_content": [
        "**_validate_center_shape**: The function of _validate_center_shape is to check if the shape of the initial cluster centers is compatible with the input data and the specified number of clusters.\n\n**parameters**: The parameters of this Function.\n· X: {ndarray, sparse matrix} of shape (n_samples, n_features) - The input data samples for which the clustering is performed.\n· centers: ndarray of shape (n_clusters, n_features) - The initial centers of the clusters that need to be validated.\n\n**Code Description**: The _validate_center_shape function is designed to ensure that the shape of the provided initial cluster centers aligns with the expected dimensions based on the input data and the number of clusters specified in the model. It performs two primary checks:\n\n1. It verifies that the number of rows in the centers array matches the number of clusters (self.n_clusters). If there is a mismatch, it raises a ValueError indicating that the shape of the initial centers does not correspond to the expected number of clusters.\n\n2. It checks that the number of columns in the centers array matches the number of features in the input data (X). If this condition is not met, it raises a ValueError, stating that the shape of the initial centers does not match the number of features in the input data.\n\nThis function is called within the _init_centroids method, which is responsible for computing the initial centroids for the clustering algorithm. When the initialization method is set to a callable or a user-provided array, the _validate_center_shape function is invoked to ensure that the provided centers are valid before proceeding with the clustering process.\n\nAdditionally, the _validate_center_shape function is also utilized in the fit methods of both the KMeans and MiniBatchKMeans classes. In these methods, it validates the initial cluster centers when they are provided as an array. This ensures that the clustering algorithms operate with valid configurations, preventing runtime errors that could arise from incompatible shapes.\n\n**Note**: It is essential to ensure that the initial centers provided to the clustering algorithm conform to the expected dimensions to avoid exceptions during the clustering process. Users should be aware of the shape requirements when initializing cluster centers, especially when using custom initialization methods."
      ],
      "code_start_line": 938,
      "code_end_line": 949,
      "params": [
        "self",
        "X",
        "centers"
      ],
      "have_return": false,
      "code_content": "    def _validate_center_shape(self, X, centers):\n        \"\"\"Check if centers is compatible with X and n_clusters.\"\"\"\n        if centers.shape[0] != self.n_clusters:\n            raise ValueError(\n                f\"The shape of the initial centers {centers.shape} does not \"\n                f\"match the number of clusters {self.n_clusters}.\"\n            )\n        if centers.shape[1] != X.shape[1]:\n            raise ValueError(\n                f\"The shape of the initial centers {centers.shape} does not \"\n                f\"match the number of features of the data {X.shape[1]}.\"\n            )\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "dataset/_kmeans.py/_BaseKMeans/_init_centroids",
        "dataset/_kmeans.py/KMeans/fit",
        "dataset/_kmeans.py/MiniBatchKMeans/fit",
        "dataset/_kmeans.py/MiniBatchKMeans/partial_fit"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "_check_test_data",
      "md_content": [
        "**_check_test_data**: The function of _check_test_data is to validate and prepare the input data for further processing.\n\n**parameters**: The parameters of this Function.\n· X: {array-like, sparse matrix} of shape (n_samples, n_features) - The input data that needs to be validated.\n\n**Code Description**: The _check_test_data function is responsible for ensuring that the input data X is in an acceptable format before it is used in other methods of the _BaseKMeans class. It utilizes the _validate_data method to perform several checks and transformations on the input data. The parameters passed to _validate_data specify that the function accepts sparse matrices in the Compressed Sparse Row (CSR) format, allows for both float64 and float32 data types, and does not reset any internal state. Additionally, it ensures that large sparse matrices are not accepted.\n\nThis function is called by several other methods within the _BaseKMeans class, including predict, transform, and score. In each of these methods, _check_test_data is invoked to validate the input data before any clustering operations are performed. This ensures that the data being processed is in the correct format and meets the necessary requirements, thereby preventing potential errors during execution.\n\n**Note**: It is important to ensure that the input data X conforms to the specified data types and formats, as violations may lead to exceptions or incorrect behavior in the clustering algorithms.\n\n**Output Example**: A possible return value of the function could be a validated NumPy array of shape (n_samples, n_features) containing the input data in the specified format, ready for further processing in the K-means clustering methods."
      ],
      "code_start_line": 951,
      "code_end_line": 960,
      "params": [
        "self",
        "X"
      ],
      "have_return": true,
      "code_content": "    def _check_test_data(self, X):\n        X = self._validate_data(\n            X,\n            accept_sparse=\"csr\",\n            reset=False,\n            dtype=[np.float64, np.float32],\n            order=\"C\",\n            accept_large_sparse=False,\n        )\n        return X\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "dataset/_kmeans.py/_BaseKMeans/predict",
        "dataset/_kmeans.py/_BaseKMeans/transform",
        "dataset/_kmeans.py/_BaseKMeans/score"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "_init_centroids",
      "md_content": [
        "**_init_centroids**: The function of _init_centroids is to compute the initial centroids for the k-means clustering algorithm based on the specified initialization method.\n\n**parameters**: The parameters of this Function.\n· X: {ndarray, sparse matrix} of shape (n_samples, n_features) - The input samples for which the centroids are to be initialized.\n· x_squared_norms: ndarray of shape (n_samples,) - Squared Euclidean norm of each data point. This can be passed directly to avoid recomputation.\n· init: {'k-means++', 'random'}, callable or ndarray of shape (n_clusters, n_features) - Method for initialization of centroids.\n· random_state: RandomState instance - Controls the randomness of centroid initialization.\n· sample_weight: ndarray of shape (n_samples,) - The weights for each observation in X. This is not used during initialization if `init` is a callable or a user-provided array.\n· init_size: int, default=None - Number of samples to randomly sample for speeding up the initialization process.\n· n_centroids: int, default=None - Number of centroids to initialize. If None, it defaults to the number of clusters specified in the model.\n\n**Code Description**: The _init_centroids function is responsible for calculating the initial cluster centers for the k-means clustering algorithm. It begins by determining the number of samples and the number of clusters to initialize. If an `init_size` is provided and is less than the total number of samples, a random subset of samples is selected to speed up the initialization process.\n\nThe function supports multiple methods for initializing centroids:\n1. If `init` is set to \"k-means++\", it calls the _kmeans_plusplus function, which selects initial centers in a way that is expected to improve the convergence speed of the k-means algorithm.\n2. If `init` is set to \"random\", it randomly selects samples from the input data as the initial centroids, weighted by the provided sample weights.\n3. If `init` is an array-like structure, it uses the provided array as the initial centers.\n4. If `init` is a callable function, it invokes this function to generate the initial centers.\n\nAfter determining the initial centers, the function checks if the centers are in a sparse format and converts them to a dense array if necessary. The computed centers are then returned.\n\nThis function is called by the fit methods of both the KMeans and MiniBatchKMeans classes. In these methods, it is used to initialize the centroids before running the k-means algorithm. The fit methods validate the input data and parameters, and they ensure that the initialization is performed correctly, leveraging the _init_centroids function to set up the initial state for clustering.\n\n**Note**: It is crucial to ensure that the number of samples in `X` is greater than or equal to the number of clusters specified. Users should also be aware that the choice of initialization method can significantly impact the performance and outcome of the k-means clustering process.\n\n**Output Example**: A possible return value of the function could be:\ncenters: array([[10, 2],\n                [1, 0]])"
      ],
      "code_start_line": 962,
      "code_end_line": 1046,
      "params": [
        "self",
        "X",
        "x_squared_norms",
        "init",
        "random_state",
        "sample_weight",
        "init_size",
        "n_centroids"
      ],
      "have_return": true,
      "code_content": "    def _init_centroids(\n        self,\n        X,\n        x_squared_norms,\n        init,\n        random_state,\n        sample_weight,\n        init_size=None,\n        n_centroids=None,\n    ):\n        \"\"\"Compute the initial centroids.\n\n        Parameters\n        ----------\n        X : {ndarray, sparse matrix} of shape (n_samples, n_features)\n            The input samples.\n\n        x_squared_norms : ndarray of shape (n_samples,)\n            Squared euclidean norm of each data point. Pass it if you have it\n            at hands already to avoid it being recomputed here.\n\n        init : {'k-means++', 'random'}, callable or ndarray of shape \\\n                (n_clusters, n_features)\n            Method for initialization.\n\n        random_state : RandomState instance\n            Determines random number generation for centroid initialization.\n            See :term:`Glossary <random_state>`.\n\n        sample_weight : ndarray of shape (n_samples,)\n            The weights for each observation in X. `sample_weight` is not used\n            during initialization if `init` is a callable or a user provided\n            array.\n\n        init_size : int, default=None\n            Number of samples to randomly sample for speeding up the\n            initialization (sometimes at the expense of accuracy).\n\n        n_centroids : int, default=None\n            Number of centroids to initialize.\n            If left to 'None' the number of centroids will be equal to\n            number of clusters to form (self.n_clusters).\n\n        Returns\n        -------\n        centers : ndarray of shape (n_clusters, n_features)\n            Initial centroids of clusters.\n        \"\"\"\n        n_samples = X.shape[0]\n        n_clusters = self.n_clusters if n_centroids is None else n_centroids\n\n        if init_size is not None and init_size < n_samples:\n            init_indices = random_state.randint(0, n_samples, init_size)\n            X = X[init_indices]\n            x_squared_norms = x_squared_norms[init_indices]\n            n_samples = X.shape[0]\n            sample_weight = sample_weight[init_indices]\n\n        if isinstance(init, str) and init == \"k-means++\":\n            centers, _ = _kmeans_plusplus(\n                X,\n                n_clusters,\n                random_state=random_state,\n                x_squared_norms=x_squared_norms,\n                sample_weight=sample_weight,\n            )\n        elif isinstance(init, str) and init == \"random\":\n            seeds = random_state.choice(\n                n_samples,\n                size=n_clusters,\n                replace=False,\n                p=sample_weight / sample_weight.sum(),\n            )\n            centers = X[seeds]\n        elif _is_arraylike_not_scalar(self.init):\n            centers = init\n        elif callable(init):\n            centers = init(X, n_clusters, random_state=random_state)\n            centers = check_array(centers, dtype=X.dtype, copy=False, order=\"C\")\n            self._validate_center_shape(X, centers)\n\n        if sp.issparse(centers):\n            centers = centers.toarray()\n\n        return centers\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "dataset/_kmeans.py/KMeans/fit",
        "dataset/_kmeans.py/MiniBatchKMeans/fit",
        "dataset/_kmeans.py/MiniBatchKMeans/partial_fit"
      ],
      "reference_who": [
        "dataset/_kmeans.py/_kmeans_plusplus",
        "dataset/_kmeans.py/_BaseKMeans/_validate_center_shape"
      ],
      "special_reference_type": [
        false,
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "fit_predict",
      "md_content": [
        "**fit_predict**: The function of fit_predict is to compute cluster centers and predict the cluster index for each sample.\n\n**parameters**: The parameters of this Function.\n· X: {array-like, sparse matrix} of shape (n_samples, n_features) - New data to transform.\n· y: Ignored - Not used, present here for API consistency by convention.\n· sample_weight: array-like of shape (n_samples,), default=None - The weights for each observation in X. If None, all observations are assigned equal weight.\n\n**Code Description**: The fit_predict function serves as a convenience method that combines two operations: fitting the model to the provided data and predicting the cluster indices for that data. It first calls the fit method on the input data X, which computes the necessary parameters for clustering, such as the cluster centers. The sample_weight parameter allows users to assign different weights to each observation, which can influence the clustering outcome. After fitting the model, the function retrieves the labels of the clusters assigned to each sample from the fitted model and returns these labels as an ndarray of shape (n_samples,). This method is particularly useful for users who want to perform clustering and obtain the results in a single step.\n\n**Note**: It is important to note that the parameter y is not utilized in this function; it is included solely for maintaining consistency with the API conventions. Users should ensure that the input data X is appropriately shaped and formatted to avoid errors during execution.\n\n**Output Example**: A possible return value of the fit_predict function could be an array like [0, 1, 0, 2, 1], indicating the cluster index for each of the samples in the input data."
      ],
      "code_start_line": 1048,
      "code_end_line": 1071,
      "params": [
        "self",
        "X",
        "y",
        "sample_weight"
      ],
      "have_return": true,
      "code_content": "    def fit_predict(self, X, y=None, sample_weight=None):\n        \"\"\"Compute cluster centers and predict cluster index for each sample.\n\n        Convenience method; equivalent to calling fit(X) followed by\n        predict(X).\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            New data to transform.\n\n        y : Ignored\n            Not used, present here for API consistency by convention.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            The weights for each observation in X. If None, all observations\n            are assigned equal weight.\n\n        Returns\n        -------\n        labels : ndarray of shape (n_samples,)\n            Index of the cluster each sample belongs to.\n        \"\"\"\n        return self.fit(X, sample_weight=sample_weight).labels_\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "predict",
      "md_content": [
        "**predict**: The function of predict is to determine the closest cluster for each sample in the input data X.\n\n**parameters**: The parameters of this Function.\n· X: {array-like, sparse matrix} of shape (n_samples, n_features) - New data to predict the cluster labels for.  \n· sample_weight: array-like of shape (n_samples,), default=\"deprecated\" - The weights for each observation in X. If None, all observations are assigned equal weight. This parameter is deprecated and will be removed in future versions.\n\n**Code Description**: The predict function is designed to assign each sample in the input data X to the nearest cluster based on the cluster centers established during the fitting process. Initially, the function checks if the model has been fitted using the check_is_fitted method, ensuring that the necessary parameters are available for making predictions.\n\nNext, the input data X is validated and prepared for processing through the _check_test_data method, which ensures that the data is in the correct format and meets the requirements for further analysis. The function also handles the sample_weight parameter, which is marked as deprecated. If the sample_weight is not set to the deprecated string, a warning is issued to inform the user of its impending removal in future versions. The sample weights are then validated using the _check_sample_weight function, which ensures that they are compatible with the input data.\n\nThe core functionality of the predict method is executed by calling the _labels_inertia_threadpool_limit function. This function computes the labels for the samples in X by determining which cluster center is closest to each sample. It operates within a controlled thread pool context to optimize performance and resource usage. The labels returned indicate the index of the cluster each sample belongs to.\n\nThe predict method is integral to the K-means clustering process, as it allows users to classify new data points based on the clusters identified during the fitting phase. It is commonly used in scenarios where new observations need to be categorized according to previously established clusters.\n\n**Note**: Users should be aware that the sample_weight parameter is deprecated and should avoid using it in future implementations. Additionally, it is crucial to ensure that the input data X is formatted correctly to prevent errors during execution.\n\n**Output Example**: A possible return value of the function when called with a dataset of 5 samples might look like this:  \nLabels: [0, 1, 0, 2, 1]"
      ],
      "code_start_line": 1073,
      "code_end_line": 1121,
      "params": [
        "self",
        "X",
        "sample_weight"
      ],
      "have_return": true,
      "code_content": "    def predict(self, X, sample_weight=\"deprecated\"):\n        \"\"\"Predict the closest cluster each sample in X belongs to.\n\n        In the vector quantization literature, `cluster_centers_` is called\n        the code book and each value returned by `predict` is the index of\n        the closest code in the code book.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            New data to predict.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            The weights for each observation in X. If None, all observations\n            are assigned equal weight.\n\n            .. deprecated:: 1.3\n               The parameter `sample_weight` is deprecated in version 1.3\n               and will be removed in 1.5.\n\n        Returns\n        -------\n        labels : ndarray of shape (n_samples,)\n            Index of the cluster each sample belongs to.\n        \"\"\"\n        check_is_fitted(self)\n\n        X = self._check_test_data(X)\n        if not (isinstance(sample_weight, str) and sample_weight == \"deprecated\"):\n            warnings.warn(\n                (\n                    \"'sample_weight' was deprecated in version 1.3 and \"\n                    \"will be removed in 1.5.\"\n                ),\n                FutureWarning,\n            )\n            sample_weight = _check_sample_weight(sample_weight, X, dtype=X.dtype)\n        else:\n            sample_weight = _check_sample_weight(None, X, dtype=X.dtype)\n\n        labels = _labels_inertia_threadpool_limit(\n            X,\n            sample_weight,\n            self.cluster_centers_,\n            n_threads=self._n_threads,\n            return_inertia=False,\n        )\n\n        return labels\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "dataset/_kmeans.py/_labels_inertia_threadpool_limit",
        "dataset/_kmeans.py/_BaseKMeans/_check_test_data"
      ],
      "special_reference_type": [
        false,
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "fit_transform",
      "md_content": [
        "**fit_transform**: The function of fit_transform is to compute clustering and transform the input data X into a cluster-distance space efficiently.\n\n**parameters**: The parameters of this Function.\n· X: {array-like, sparse matrix} of shape (n_samples, n_features) - New data to transform.\n· y: Ignored - Not used, present here for API consistency by convention.\n· sample_weight: array-like of shape (n_samples,), default=None - The weights for each observation in X. If None, all observations are assigned equal weight.\n\n**Code Description**: The fit_transform method is designed to perform clustering on the input data X and subsequently transform it into a new space that reflects the distances to the identified clusters. This method is equivalent to first fitting the model to the data using the fit method and then transforming the data using the transform method. However, it is implemented in a more efficient manner to optimize performance. The input parameter X represents the data to be clustered, while the parameter y is included for consistency with other methods but is not utilized in this function. The sample_weight parameter allows for the specification of weights for each observation, enabling the user to influence the clustering process based on the importance of individual samples. If sample_weight is not provided, all samples are treated equally. The method returns an ndarray of shape (n_samples, n_clusters), which contains the transformed data in the new cluster-distance space.\n\n**Note**: It is important to ensure that the input data X is in the correct format (array-like or sparse matrix) and has the appropriate shape. The sample_weight parameter is optional, but when used, it should match the number of samples in X.\n\n**Output Example**: A possible return value of the fit_transform method could be an ndarray such as:\n[[0.1, 0.9, 0.0],\n [0.2, 0.8, 0.0],\n [0.0, 0.0, 1.0]] \nThis output represents the distances of each sample to the respective clusters in the transformed space."
      ],
      "code_start_line": 1123,
      "code_end_line": 1145,
      "params": [
        "self",
        "X",
        "y",
        "sample_weight"
      ],
      "have_return": true,
      "code_content": "    def fit_transform(self, X, y=None, sample_weight=None):\n        \"\"\"Compute clustering and transform X to cluster-distance space.\n\n        Equivalent to fit(X).transform(X), but more efficiently implemented.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            New data to transform.\n\n        y : Ignored\n            Not used, present here for API consistency by convention.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            The weights for each observation in X. If None, all observations\n            are assigned equal weight.\n\n        Returns\n        -------\n        X_new : ndarray of shape (n_samples, n_clusters)\n            X transformed in the new space.\n        \"\"\"\n        return self.fit(X, sample_weight=sample_weight)._transform(X)\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "transform",
      "md_content": [
        "**transform**: The function of transform is to convert input data into a new space representing distances to cluster centers.\n\n**parameters**: The parameters of this Function.\n· X: {array-like, sparse matrix} of shape (n_samples, n_features) - New data to transform.\n\n**Code Description**: The transform method is a key function within the _BaseKMeans class that facilitates the transformation of input data X into a cluster-distance space. In this transformed space, each dimension corresponds to the distance from the data points to the identified cluster centers. The method begins by ensuring that the model has been fitted with the training data through the call to check_is_fitted(self). This is crucial as it verifies that the necessary cluster centers are available for distance calculations.\n\nFollowing this check, the method invokes _check_test_data(X) to validate and prepare the input data. This function ensures that the input data X is in an acceptable format, allowing for both dense and sparse representations. Once the data has been validated, the method proceeds to call _transform(X), which is responsible for computing the actual distances between the input data points and the cluster centers.\n\nThe _transform function operates without performing any input validation, relying on the prior checks conducted by the transform method. It utilizes the euclidean_distances function to calculate the distances, returning a new array where each row corresponds to a sample from X and each column corresponds to a cluster center.\n\nThis structured approach, where transform manages data validation and preparation while _transform focuses solely on distance computation, ensures a clear separation of responsibilities within the code. As a result, users can confidently transform their data into a format suitable for further analysis or clustering operations.\n\n**Note**: It is essential to ensure that the model is fitted before invoking the transform method. Additionally, the input data X must conform to the specified formats and types to avoid potential errors during execution.\n\n**Output Example**: A possible return value of the transform function could be a 2D NumPy array where each row represents a sample from X and each column corresponds to a cluster center, with values indicating the distances. For example, if there are 3 samples and 2 clusters, the output might look like:\n```\narray([[1.5, 2.0],\n       [0.5, 1.2],\n       [3.1, 0.8]])\n```"
      ],
      "code_start_line": 1147,
      "code_end_line": 1167,
      "params": [
        "self",
        "X"
      ],
      "have_return": true,
      "code_content": "    def transform(self, X):\n        \"\"\"Transform X to a cluster-distance space.\n\n        In the new space, each dimension is the distance to the cluster\n        centers. Note that even if X is sparse, the array returned by\n        `transform` will typically be dense.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            New data to transform.\n\n        Returns\n        -------\n        X_new : ndarray of shape (n_samples, n_clusters)\n            X transformed in the new space.\n        \"\"\"\n        check_is_fitted(self)\n\n        X = self._check_test_data(X)\n        return self._transform(X)\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "dataset/_kmeans.py/_BaseKMeans/_check_test_data",
        "dataset/_kmeans.py/_BaseKMeans/_transform"
      ],
      "special_reference_type": [
        false,
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "_transform",
      "md_content": [
        "**_transform**: The function of _transform is to compute the distances between input data points and the cluster centers.\n\n**parameters**: The parameters of this Function.\n· X: array-like or sparse matrix of shape (n_samples, n_features) representing the new data to transform.\n\n**Code Description**: The _transform function is a core component of the clustering algorithm that calculates the Euclidean distances from each data point in the input array X to the cluster centers stored in the attribute self.cluster_centers_. This function does not perform any input validation, which means it assumes that the input data X has already been checked and is in the correct format. \n\nThe _transform function is called by the transform method of the _BaseKMeans class. The transform method serves as a public interface for users to convert their data into a new space where each dimension corresponds to the distance from the data points to the identified cluster centers. Before invoking _transform, the transform method ensures that the model has been fitted with the training data by calling check_is_fitted(self) and also checks the test data format with _check_test_data(X). This layered approach allows for a clean separation of concerns, where _transform focuses solely on the distance computation, while transform handles data validation and preparation.\n\n**Note**: It is important to ensure that the model is fitted before calling the transform method, as the _transform function relies on the cluster centers being available.\n\n**Output Example**: A possible return value of the _transform function could be a 2D NumPy array where each row corresponds to a sample from X and each column corresponds to a cluster center, with the values representing the distances. For instance, if there are 3 samples and 2 clusters, the output might look like:\n```\narray([[1.5, 2.0],\n       [0.5, 1.2],\n       [3.1, 0.8]])\n```"
      ],
      "code_start_line": 1169,
      "code_end_line": 1171,
      "params": [
        "self",
        "X"
      ],
      "have_return": true,
      "code_content": "    def _transform(self, X):\n        \"\"\"Guts of transform method; no input validation.\"\"\"\n        return euclidean_distances(X, self.cluster_centers_)\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "dataset/_kmeans.py/_BaseKMeans/transform"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "score",
      "md_content": [
        "**score**: The function of score is to compute the opposite of the value of X on the K-means objective.\n\n**parameters**: The parameters of this Function.\n· X: {array-like, sparse matrix} of shape (n_samples, n_features) - New data to evaluate against the K-means model.  \n· y: Ignored - This parameter is not used and is present for API consistency by convention.  \n· sample_weight: array-like of shape (n_samples,), default=None - The weights for each observation in X. If None, all observations are assigned equal weight.\n\n**Code Description**: The score function is designed to evaluate the performance of the K-means clustering algorithm on a given dataset X. It begins by ensuring that the K-means model has been fitted to the data through the call to check_is_fitted(self). This is a crucial step as it verifies that the model has learned the cluster centers from the training data before any scoring can occur.\n\nNext, the function validates the input data X by calling the _check_test_data method. This method ensures that the data is in an acceptable format, either as a dense array or a sparse matrix in CSR format, and prepares it for further processing. The sample weights are also checked and validated through the _check_sample_weight function, which ensures that they are appropriately defined and match the shape of the input data.\n\nThe core of the scoring mechanism is executed by the _labels_inertia_threadpool_limit function. This function computes the labels and inertia for the input data X based on the cluster centers learned during the fitting process. It operates within a controlled threading context to optimize performance while managing resource usage. The function returns the inertia score, which quantifies the compactness of the clusters formed by the K-means algorithm.\n\nFinally, the score function returns the negative of the computed inertia score. This is because, in the context of K-means, a lower inertia indicates a better clustering solution, and thus the function provides the opposite value to reflect this relationship.\n\nThe score function is typically called after fitting the K-means model to new data, allowing users to evaluate how well the model performs on unseen samples.\n\n**Note**: It is essential to ensure that the input data X is formatted correctly and that the sample weights are defined if used. The function is optimized for performance but requires that the model has been fitted prior to invocation.\n\n**Output Example**: A possible return value of the function when called with a dataset of 5 samples might look like this:  \nScore: -12.34"
      ],
      "code_start_line": 1173,
      "code_end_line": 1201,
      "params": [
        "self",
        "X",
        "y",
        "sample_weight"
      ],
      "have_return": true,
      "code_content": "    def score(self, X, y=None, sample_weight=None):\n        \"\"\"Opposite of the value of X on the K-means objective.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            New data.\n\n        y : Ignored\n            Not used, present here for API consistency by convention.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            The weights for each observation in X. If None, all observations\n            are assigned equal weight.\n\n        Returns\n        -------\n        score : float\n            Opposite of the value of X on the K-means objective.\n        \"\"\"\n        check_is_fitted(self)\n\n        X = self._check_test_data(X)\n        sample_weight = _check_sample_weight(sample_weight, X, dtype=X.dtype)\n\n        _, scores = _labels_inertia_threadpool_limit(\n            X, sample_weight, self.cluster_centers_, self._n_threads\n        )\n        return -scores\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "dataset/_kmeans.py/_labels_inertia_threadpool_limit",
        "dataset/_kmeans.py/_BaseKMeans/_check_test_data"
      ],
      "special_reference_type": [
        false,
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "_more_tags",
      "md_content": [
        "**_more_tags**: The function of _more_tags is to provide additional metadata regarding the behavior of the class, specifically related to sample weight invariance checks.\n\n**parameters**: The parameters of this Function.\n· There are no parameters for this function.\n\n**Code Description**: The _more_tags function returns a dictionary containing specific tags that provide insights into the behavior of the class it belongs to. In this case, it includes a key \"_xfail_checks\" which indicates that there is a known issue with the handling of sample weights. The dictionary specifies that the check for sample weight invariance is expected to fail, with a description stating that \"zero sample_weight is not equivalent to removing samples.\" This suggests that when sample weights are set to zero, the model does not behave as if those samples were entirely excluded from the dataset, which is an important consideration for users implementing this functionality.\n\n**Note**: It is important for users to be aware of this behavior when using the class, as it may affect the results of their model training and evaluation. Understanding this limitation can help in making informed decisions regarding data preprocessing and model configuration.\n\n**Output Example**: The return value of the _more_tags function would appear as follows:\n{\n    \"_xfail_checks\": {\n        \"check_sample_weights_invariance\": (\n            \"zero sample_weight is not equivalent to removing samples\"\n        ),\n    },\n}"
      ],
      "code_start_line": 1203,
      "code_end_line": 1210,
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def _more_tags(self):\n        return {\n            \"_xfail_checks\": {\n                \"check_sample_weights_invariance\": (\n                    \"zero sample_weight is not equivalent to removing samples\"\n                ),\n            },\n        }\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "ClassDef",
      "name": "KMeans",
      "md_content": [
        "**KMeans**: The function of KMeans is to perform K-Means clustering, a popular algorithm used for partitioning a dataset into distinct groups based on feature similarity.\n\n**attributes**: The attributes of this Class.\n· n_clusters: The number of clusters to form as well as the number of centroids to generate.\n· init: Method for initialization of cluster centers.\n· n_init: Number of times the k-means algorithm will be run with different centroid seeds.\n· max_iter: Maximum number of iterations of the k-means algorithm for a single run.\n· tol: Relative tolerance with regards to Frobenius norm of the difference in the cluster centers of two consecutive iterations to declare convergence.\n· verbose: Verbosity mode.\n· random_state: Determines random number generation for centroid initialization.\n· copy_x: When pre-computing distances, indicates whether to modify the original data.\n· algorithm: K-means algorithm to use, either \"lloyd\" or \"elkan\".\n\n**Code Description**: The KMeans class is an implementation of the K-Means clustering algorithm, inheriting from the _BaseKMeans class, which provides foundational functionality and parameter validation. The KMeans class allows users to specify the number of clusters (n_clusters) they wish to form, the method for initializing cluster centers (init), and various other parameters that control the behavior of the algorithm.\n\nUpon initialization, the KMeans class sets up the parameters and checks them against the input data using the inherited methods from _BaseKMeans. The fit method computes the K-Means clustering by iterating through the data, adjusting the cluster centers based on the assigned labels until convergence is achieved or the maximum number of iterations is reached.\n\nThe KMeans class also includes methods for predicting cluster labels for new data points and transforming the data into a cluster-distance space. It maintains attributes that store the final cluster centers, labels for each data point, the inertia (a measure of how tightly the clusters are packed), and the number of iterations run.\n\nThe KMeans class is called by the k_means function, which serves as a convenient interface for users to perform clustering without directly interacting with the class. The k_means function initializes an instance of KMeans with the specified parameters and calls its fit method to perform the clustering. The results, including the final centroids, labels, and inertia, are returned to the user.\n\n**Note**: When using the KMeans class, it is essential to ensure that the input data meets the specified constraints, particularly regarding the number of samples and clusters. Users should also be aware of the initialization methods and their implications on the clustering results, as well as the potential for the algorithm to converge to local minima.\n\n**Output Example**: A possible return value from the fit method could be:\n```\ncluster_centers_: array([[10.,  2.],\n                          [ 1.,  2.]])\nlabels_: array([1, 1, 1, 0, 0, 0], dtype=int32)\ninertia_: 16.0\nn_iter_: 5\n``` \nThis output indicates the coordinates of the cluster centers, the cluster assignment for each sample in the input data, the final inertia value, and the number of iterations taken to converge."
      ],
      "code_start_line": 1213,
      "code_end_line": 1580,
      "params": [],
      "have_return": true,
      "code_content": "class KMeans(_BaseKMeans):\n    \"\"\"K-Means clustering.\n\n    Read more in the :ref:`User Guide <k_means>`.\n\n    Parameters\n    ----------\n\n    n_clusters : int, default=8\n        The number of clusters to form as well as the number of\n        centroids to generate.\n\n        For an example of how to choose an optimal value for `n_clusters` refer to\n        :ref:`sphx_glr_auto_examples_cluster_plot_kmeans_silhouette_analysis.py`.\n\n    init : {'k-means++', 'random'}, callable or array-like of shape \\\n            (n_clusters, n_features), default='k-means++'\n        Method for initialization:\n\n        * 'k-means++' : selects initial cluster centroids using sampling \\\n            based on an empirical probability distribution of the points' \\\n            contribution to the overall inertia. This technique speeds up \\\n            convergence. The algorithm implemented is \"greedy k-means++\". It \\\n            differs from the vanilla k-means++ by making several trials at \\\n            each sampling step and choosing the best centroid among them.\n\n        * 'random': choose `n_clusters` observations (rows) at random from \\\n        data for the initial centroids.\n\n        * If an array is passed, it should be of shape (n_clusters, n_features)\\\n        and gives the initial centers.\n\n        * If a callable is passed, it should take arguments X, n_clusters and a\\\n        random state and return an initialization.\n\n        For an example of how to use the different `init` strategy, see the example\n        entitled :ref:`sphx_glr_auto_examples_cluster_plot_kmeans_digits.py`.\n\n    n_init : 'auto' or int, default='auto'\n        Number of times the k-means algorithm is run with different centroid\n        seeds. The final results is the best output of `n_init` consecutive runs\n        in terms of inertia. Several runs are recommended for sparse\n        high-dimensional problems (see :ref:`kmeans_sparse_high_dim`).\n\n        When `n_init='auto'`, the number of runs depends on the value of init:\n        10 if using `init='random'` or `init` is a callable;\n        1 if using `init='k-means++'` or `init` is an array-like.\n\n        .. versionadded:: 1.2\n           Added 'auto' option for `n_init`.\n\n        .. versionchanged:: 1.4\n           Default value for `n_init` changed to `'auto'`.\n\n    max_iter : int, default=300\n        Maximum number of iterations of the k-means algorithm for a\n        single run.\n\n    tol : float, default=1e-4\n        Relative tolerance with regards to Frobenius norm of the difference\n        in the cluster centers of two consecutive iterations to declare\n        convergence.\n\n    verbose : int, default=0\n        Verbosity mode.\n\n    random_state : int, RandomState instance or None, default=None\n        Determines random number generation for centroid initialization. Use\n        an int to make the randomness deterministic.\n        See :term:`Glossary <random_state>`.\n\n    copy_x : bool, default=True\n        When pre-computing distances it is more numerically accurate to center\n        the data first. If copy_x is True (default), then the original data is\n        not modified. If False, the original data is modified, and put back\n        before the function returns, but small numerical differences may be\n        introduced by subtracting and then adding the data mean. Note that if\n        the original data is not C-contiguous, a copy will be made even if\n        copy_x is False. If the original data is sparse, but not in CSR format,\n        a copy will be made even if copy_x is False.\n\n    algorithm : {\"lloyd\", \"elkan\"}, default=\"lloyd\"\n        K-means algorithm to use. The classical EM-style algorithm is `\"lloyd\"`.\n        The `\"elkan\"` variation can be more efficient on some datasets with\n        well-defined clusters, by using the triangle inequality. However it's\n        more memory intensive due to the allocation of an extra array of shape\n        `(n_samples, n_clusters)`.\n\n        .. versionchanged:: 0.18\n            Added Elkan algorithm\n\n        .. versionchanged:: 1.1\n            Renamed \"full\" to \"lloyd\", and deprecated \"auto\" and \"full\".\n            Changed \"auto\" to use \"lloyd\" instead of \"elkan\".\n\n    Attributes\n    ----------\n    cluster_centers_ : ndarray of shape (n_clusters, n_features)\n        Coordinates of cluster centers. If the algorithm stops before fully\n        converging (see ``tol`` and ``max_iter``), these will not be\n        consistent with ``labels_``.\n\n    labels_ : ndarray of shape (n_samples,)\n        Labels of each point\n\n    inertia_ : float\n        Sum of squared distances of samples to their closest cluster center,\n        weighted by the sample weights if provided.\n\n    n_iter_ : int\n        Number of iterations run.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    MiniBatchKMeans : Alternative online implementation that does incremental\n        updates of the centers positions using mini-batches.\n        For large scale learning (say n_samples > 10k) MiniBatchKMeans is\n        probably much faster than the default batch implementation.\n\n    Notes\n    -----\n    The k-means problem is solved using either Lloyd's or Elkan's algorithm.\n\n    The average complexity is given by O(k n T), where n is the number of\n    samples and T is the number of iteration.\n\n    The worst case complexity is given by O(n^(k+2/p)) with\n    n = n_samples, p = n_features.\n    Refer to :doi:`\"How slow is the k-means method?\" D. Arthur and S. Vassilvitskii -\n    SoCG2006.<10.1145/1137856.1137880>` for more details.\n\n    In practice, the k-means algorithm is very fast (one of the fastest\n    clustering algorithms available), but it falls in local minima. That's why\n    it can be useful to restart it several times.\n\n    If the algorithm stops before fully converging (because of ``tol`` or\n    ``max_iter``), ``labels_`` and ``cluster_centers_`` will not be consistent,\n    i.e. the ``cluster_centers_`` will not be the means of the points in each\n    cluster. Also, the estimator will reassign ``labels_`` after the last\n    iteration to make ``labels_`` consistent with ``predict`` on the training\n    set.\n\n    Examples\n    --------\n\n    >>> from sklearn.cluster import KMeans\n    >>> import numpy as np\n    >>> X = np.array([[1, 2], [1, 4], [1, 0],\n    ...               [10, 2], [10, 4], [10, 0]])\n    >>> kmeans = KMeans(n_clusters=2, random_state=0, n_init=\"auto\").fit(X)\n    >>> kmeans.labels_\n    array([1, 1, 1, 0, 0, 0], dtype=int32)\n    >>> kmeans.predict([[0, 0], [12, 3]])\n    array([1, 0], dtype=int32)\n    >>> kmeans.cluster_centers_\n    array([[10.,  2.],\n           [ 1.,  2.]])\n\n    For a more detailed example of K-Means using the iris dataset see\n    :ref:`sphx_glr_auto_examples_cluster_plot_cluster_iris.py`.\n\n    For examples of common problems with K-Means and how to address them see\n    :ref:`sphx_glr_auto_examples_cluster_plot_kmeans_assumptions.py`.\n\n    For an example of how to use K-Means to perform color quantization see\n    :ref:`sphx_glr_auto_examples_cluster_plot_color_quantization.py`.\n\n    For a demonstration of how K-Means can be used to cluster text documents see\n    :ref:`sphx_glr_auto_examples_text_plot_document_clustering.py`.\n\n    For a comparison between K-Means and MiniBatchKMeans refer to example\n    :ref:`sphx_glr_auto_examples_cluster_plot_mini_batch_kmeans.py`.\n    \"\"\"\n\n    _parameter_constraints: dict = {\n        **_BaseKMeans._parameter_constraints,\n        \"copy_x\": [\"boolean\"],\n        \"algorithm\": [StrOptions({\"lloyd\", \"elkan\"})],\n    }\n\n    def __init__(\n        self,\n        n_clusters=8,\n        *,\n        init=\"k-means++\",\n        n_init=\"auto\",\n        max_iter=300,\n        tol=1e-4,\n        verbose=0,\n        random_state=None,\n        copy_x=True,\n        algorithm=\"lloyd\",\n    ):\n        super().__init__(\n            n_clusters=n_clusters,\n            init=init,\n            n_init=n_init,\n            max_iter=max_iter,\n            tol=tol,\n            verbose=verbose,\n            random_state=random_state,\n        )\n\n        self.copy_x = copy_x\n        self.algorithm = algorithm\n\n    def _check_params_vs_input(self, X):\n        super()._check_params_vs_input(X, default_n_init=10)\n\n        self._algorithm = self.algorithm\n        if self._algorithm == \"elkan\" and self.n_clusters == 1:\n            warnings.warn(\n                (\n                    \"algorithm='elkan' doesn't make sense for a single \"\n                    \"cluster. Using 'lloyd' instead.\"\n                ),\n                RuntimeWarning,\n            )\n            self._algorithm = \"lloyd\"\n\n    def _warn_mkl_vcomp(self, n_active_threads):\n        \"\"\"Warn when vcomp and mkl are both present\"\"\"\n        warnings.warn(\n            \"KMeans is known to have a memory leak on Windows \"\n            \"with MKL, when there are less chunks than available \"\n            \"threads. You can avoid it by setting the environment\"\n            f\" variable OMP_NUM_THREADS={n_active_threads}.\"\n        )\n\n    @_fit_context(prefer_skip_nested_validation=True)\n    def fit(self, X, y=None, sample_weight=None):\n        \"\"\"Compute k-means clustering.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Training instances to cluster. It must be noted that the data\n            will be converted to C ordering, which will cause a memory\n            copy if the given data is not C-contiguous.\n            If a sparse matrix is passed, a copy will be made if it's not in\n            CSR format.\n\n        y : Ignored\n            Not used, present here for API consistency by convention.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            The weights for each observation in X. If None, all observations\n            are assigned equal weight. `sample_weight` is not used during\n            initialization if `init` is a callable or a user provided array.\n\n            .. versionadded:: 0.20\n\n        Returns\n        -------\n        self : object\n            Fitted estimator.\n        \"\"\"\n        X = self._validate_data(\n            X,\n            accept_sparse=\"csr\",\n            dtype=[np.float64, np.float32],\n            order=\"C\",\n            copy=self.copy_x,\n            accept_large_sparse=False,\n        )\n\n        self._check_params_vs_input(X)\n\n        random_state = check_random_state(self.random_state)\n        sample_weight = _check_sample_weight(sample_weight, X, dtype=X.dtype)\n        self._n_threads = _openmp_effective_n_threads()\n\n        # Validate init array\n        init = self.init\n        init_is_array_like = _is_arraylike_not_scalar(init)\n        if init_is_array_like:\n            init = check_array(init, dtype=X.dtype, copy=True, order=\"C\")\n            self._validate_center_shape(X, init)\n\n        # subtract of mean of x for more accurate distance computations\n        if not sp.issparse(X):\n            X_mean = X.mean(axis=0)\n            # The copy was already done above\n            X -= X_mean\n\n            if init_is_array_like:\n                init -= X_mean\n\n        # precompute squared norms of data points\n        x_squared_norms = row_norms(X, squared=True)\n\n        if self._algorithm == \"elkan\":\n            kmeans_single = _kmeans_single_elkan\n        else:\n            kmeans_single = _kmeans_single_lloyd\n            self._check_mkl_vcomp(X, X.shape[0])\n\n        best_inertia, best_labels = None, None\n\n        for i in range(self._n_init):\n            # Initialize centers\n            centers_init = self._init_centroids(\n                X,\n                x_squared_norms=x_squared_norms,\n                init=init,\n                random_state=random_state,\n                sample_weight=sample_weight,\n            )\n            if self.verbose:\n                print(\"Initialization complete\")\n\n            # run a k-means once\n            labels, inertia, centers, n_iter_ = kmeans_single(\n                X,\n                sample_weight,\n                centers_init,\n                max_iter=self.max_iter,\n                verbose=self.verbose,\n                tol=self._tol,\n                n_threads=self._n_threads,\n            )\n\n            # determine if these results are the best so far\n            # we chose a new run if it has a better inertia and the clustering is\n            # different from the best so far (it's possible that the inertia is\n            # slightly better even if the clustering is the same with potentially\n            # permuted labels, due to rounding errors)\n            if best_inertia is None or (\n                inertia < best_inertia\n                and not _is_same_clustering(labels, best_labels, self.n_clusters)\n            ):\n                best_labels = labels\n                best_centers = centers\n                best_inertia = inertia\n                best_n_iter = n_iter_\n\n        if not sp.issparse(X):\n            if not self.copy_x:\n                X += X_mean\n            best_centers += X_mean\n\n        distinct_clusters = len(set(best_labels))\n        if distinct_clusters < self.n_clusters:\n            warnings.warn(\n                \"Number of distinct clusters ({}) found smaller than \"\n                \"n_clusters ({}). Possibly due to duplicate points \"\n                \"in X.\".format(distinct_clusters, self.n_clusters),\n                ConvergenceWarning,\n                stacklevel=2,\n            )\n\n        self.cluster_centers_ = best_centers\n        self._n_features_out = self.cluster_centers_.shape[0]\n        self.labels_ = best_labels\n        self.inertia_ = best_inertia\n        self.n_iter_ = best_n_iter\n        return self\n",
      "name_column": 6,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "dataset/_kmeans.py/k_means"
      ],
      "reference_who": [
        "dataset/_kmeans.py/_BaseKMeans"
      ],
      "special_reference_type": [
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [
        "**__init__**: The function of __init__ is to initialize an instance of the KMeans class with specified parameters for clustering.\n\n**parameters**: The parameters of this Function.\n· n_clusters: The number of clusters to form as well as the number of centroids to generate. Default is 8.  \n· init: Method for initialization. Default is \"k-means++\", which helps in selecting initial cluster centers in a smart way to speed up convergence.  \n· n_init: Number of times the k-means algorithm will be run with different centroid seeds. Default is \"auto\".  \n· max_iter: Maximum number of iterations of the k-means algorithm for a single run. Default is 300.  \n· tol: Relative tolerance with regards to inertia to declare convergence. Default is 1e-4.  \n· verbose: Verbosity mode. Default is 0, which means no output.  \n· random_state: Determines random number generation for centroid initialization. Use an int for reproducibility. Default is None.  \n· copy_x: If True, the original data will be copied; else, it may be overwritten. Default is True.  \n· algorithm: K-means algorithm to use. Default is \"lloyd\", which is the standard algorithm.\n\n**Code Description**: The __init__ function serves as the constructor for the KMeans class, allowing users to create an instance of the KMeans clustering algorithm with customizable parameters. The function begins by calling the constructor of the parent class using `super().__init__`, passing essential parameters such as `n_clusters`, `init`, `n_init`, `max_iter`, `tol`, `verbose`, and `random_state`. This ensures that the base class is properly initialized with these values. The function then sets additional attributes specific to the KMeans class: `copy_x`, which determines whether to copy the input data or not, and `algorithm`, which specifies the algorithm to be used for clustering. This initialization process is crucial for configuring the behavior of the KMeans instance according to the user's requirements.\n\n**Note**: It is important to choose the number of clusters (`n_clusters`) wisely, as it significantly affects the clustering results. Additionally, setting `random_state` can help in achieving reproducible results when running the algorithm multiple times."
      ],
      "code_start_line": 1404,
      "code_end_line": 1428,
      "params": [
        "self",
        "n_clusters"
      ],
      "have_return": false,
      "code_content": "    def __init__(\n        self,\n        n_clusters=8,\n        *,\n        init=\"k-means++\",\n        n_init=\"auto\",\n        max_iter=300,\n        tol=1e-4,\n        verbose=0,\n        random_state=None,\n        copy_x=True,\n        algorithm=\"lloyd\",\n    ):\n        super().__init__(\n            n_clusters=n_clusters,\n            init=init,\n            n_init=n_init,\n            max_iter=max_iter,\n            tol=tol,\n            verbose=verbose,\n            random_state=random_state,\n        )\n\n        self.copy_x = copy_x\n        self.algorithm = algorithm\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "_check_params_vs_input",
      "md_content": [
        "**_check_params_vs_input**: The function of _check_params_vs_input is to validate the input parameters against the expected configuration for the KMeans clustering algorithm.\n\n**parameters**: The parameters of this Function.\n· X: {array-like, sparse matrix} of shape (n_samples, n_features) - The input data to be validated, which contains the training instances for clustering.\n\n**Code Description**: The _check_params_vs_input function is responsible for ensuring that the input data X is compatible with the parameters set for the KMeans algorithm. It first calls the superclass method to perform initial checks on the input data, passing a default value for n_init (number of initializations) set to 10. This ensures that the input data adheres to the expected format and structure required by the KMeans algorithm.\n\nFollowing the superclass validation, the function checks the algorithm specified for clustering. If the algorithm is set to \"elkan\" and the number of clusters (n_clusters) is equal to 1, a warning is issued. This warning indicates that using the \"elkan\" algorithm is not appropriate for a single cluster scenario, and the algorithm is automatically switched to \"lloyd\". This adjustment is crucial because the \"elkan\" algorithm is optimized for scenarios with multiple clusters and may not function correctly with only one cluster.\n\nThe _check_params_vs_input function is called within the fit method of the KMeans class. The fit method is responsible for computing the k-means clustering by validating the input data, initializing centroids, and running the clustering algorithm. By validating the parameters against the input data, _check_params_vs_input ensures that the clustering process is set up correctly before any computations are performed, thus preventing potential errors during execution.\n\n**Note**: It is important to ensure that the input data X is in the correct format and structure before calling the fit method, as this will directly affect the performance and outcome of the clustering process. Additionally, users should be aware of the implications of using different algorithms based on the number of clusters specified."
      ],
      "code_start_line": 1430,
      "code_end_line": 1442,
      "params": [
        "self",
        "X"
      ],
      "have_return": false,
      "code_content": "    def _check_params_vs_input(self, X):\n        super()._check_params_vs_input(X, default_n_init=10)\n\n        self._algorithm = self.algorithm\n        if self._algorithm == \"elkan\" and self.n_clusters == 1:\n            warnings.warn(\n                (\n                    \"algorithm='elkan' doesn't make sense for a single \"\n                    \"cluster. Using 'lloyd' instead.\"\n                ),\n                RuntimeWarning,\n            )\n            self._algorithm = \"lloyd\"\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "dataset/_kmeans.py/KMeans/fit"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "_warn_mkl_vcomp",
      "md_content": [
        "**_warn_mkl_vcomp**: The function of _warn_mkl_vcomp is to issue a warning regarding potential memory leaks when using KMeans with MKL on Windows.\n\n**parameters**: The parameters of this Function.\n· n_active_threads: An integer representing the number of active threads that are available for processing.\n\n**Code Description**: The _warn_mkl_vcomp function is designed to alert users when both the Intel Math Kernel Library (MKL) and the vcomp library are present in the environment while using the KMeans algorithm. This situation can lead to a memory leak on Windows systems if the number of chunks processed is less than the number of available threads. The function takes a single parameter, n_active_threads, which indicates how many threads are actively being utilized. When invoked, the function generates a warning message that informs the user of the potential issue and provides a recommendation to set the environment variable OMP_NUM_THREADS to the value of n_active_threads. This adjustment is suggested as a means to mitigate the risk of memory leaks during the execution of the KMeans algorithm.\n\n**Note**: It is important for users to heed the warning generated by this function, especially when running KMeans on Windows with MKL. Proper configuration of the OMP_NUM_THREADS environment variable can help prevent performance degradation and memory-related issues."
      ],
      "code_start_line": 1444,
      "code_end_line": 1451,
      "params": [
        "self",
        "n_active_threads"
      ],
      "have_return": false,
      "code_content": "    def _warn_mkl_vcomp(self, n_active_threads):\n        \"\"\"Warn when vcomp and mkl are both present\"\"\"\n        warnings.warn(\n            \"KMeans is known to have a memory leak on Windows \"\n            \"with MKL, when there are less chunks than available \"\n            \"threads. You can avoid it by setting the environment\"\n            f\" variable OMP_NUM_THREADS={n_active_threads}.\"\n        )\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "fit",
      "md_content": [
        "**fit**: The function of fit is to compute k-means clustering.\n\n**parameters**: The parameters of this Function.\n· X: {array-like, sparse matrix} of shape (n_samples, n_features) - Training instances to cluster. The data will be converted to C ordering, which may cause a memory copy if the data is not C-contiguous. If a sparse matrix is passed, a copy will be made if it's not in CSR format.\n· y: Ignored - Not used, present here for API consistency by convention.\n· sample_weight: array-like of shape (n_samples,), default=None - The weights for each observation in X. If None, all observations are assigned equal weight. `sample_weight` is not used during initialization if `init` is a callable or a user-provided array.\n\n**Code Description**: The fit method is responsible for executing the k-means clustering algorithm. It begins by validating the input data X using the _validate_data method, ensuring that the data is in an acceptable format and type. The method checks for the compatibility of the input data with the parameters set for the KMeans algorithm through the _check_params_vs_input method.\n\nOnce the data is validated, the method initializes the random state for reproducibility and checks the sample weights using the _check_sample_weight function. It also determines the number of effective threads for computation.\n\nThe method then validates the initial cluster centers if provided, ensuring they conform to the expected shape using the _validate_center_shape method. If the input data is dense, it subtracts the mean of X to enhance the accuracy of distance computations.\n\nThe fit method precomputes the squared norms of the data points and selects the appropriate k-means algorithm (either Elkan or Lloyd) based on the specified parameters. It then enters a loop to perform multiple initializations of the cluster centers, running the k-means algorithm for each initialization.\n\nDuring each iteration, the method initializes the cluster centers and calls the selected k-means function (_kmeans_single_elkan or _kmeans_single_lloyd) to perform the clustering. After each run, it evaluates the results based on inertia and updates the best labels and centers if the current run yields better results.\n\nFinally, the method checks for distinct clusters and raises a warning if the number of distinct clusters found is smaller than the specified number of clusters. It assigns the best cluster centers, labels, inertia, and the number of iterations to the respective attributes of the KMeans instance and returns the fitted estimator.\n\nThe fit method is called by the k_means function, which serves as a high-level interface for performing k-means clustering. The k_means function initializes a KMeans instance and calls its fit method to execute the clustering process.\n\n**Note**: It is essential to ensure that the input data X is properly formatted and that the sample weights are correctly specified. The choice of initialization method and the number of initializations can significantly impact the performance and outcome of the k-means clustering process.\n\n**Output Example**: A possible return value from the fit method could be:\n```python\nKMeans(n_clusters=3, init='k-means++', n_init=10, max_iter=300, random_state=42)\n```\nThis output indicates that the KMeans instance has been fitted with the specified parameters, ready for further analysis or predictions."
      ],
      "code_start_line": 1454,
      "code_end_line": 1580,
      "params": [
        "self",
        "X",
        "y",
        "sample_weight"
      ],
      "have_return": true,
      "code_content": "    def fit(self, X, y=None, sample_weight=None):\n        \"\"\"Compute k-means clustering.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Training instances to cluster. It must be noted that the data\n            will be converted to C ordering, which will cause a memory\n            copy if the given data is not C-contiguous.\n            If a sparse matrix is passed, a copy will be made if it's not in\n            CSR format.\n\n        y : Ignored\n            Not used, present here for API consistency by convention.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            The weights for each observation in X. If None, all observations\n            are assigned equal weight. `sample_weight` is not used during\n            initialization if `init` is a callable or a user provided array.\n\n            .. versionadded:: 0.20\n\n        Returns\n        -------\n        self : object\n            Fitted estimator.\n        \"\"\"\n        X = self._validate_data(\n            X,\n            accept_sparse=\"csr\",\n            dtype=[np.float64, np.float32],\n            order=\"C\",\n            copy=self.copy_x,\n            accept_large_sparse=False,\n        )\n\n        self._check_params_vs_input(X)\n\n        random_state = check_random_state(self.random_state)\n        sample_weight = _check_sample_weight(sample_weight, X, dtype=X.dtype)\n        self._n_threads = _openmp_effective_n_threads()\n\n        # Validate init array\n        init = self.init\n        init_is_array_like = _is_arraylike_not_scalar(init)\n        if init_is_array_like:\n            init = check_array(init, dtype=X.dtype, copy=True, order=\"C\")\n            self._validate_center_shape(X, init)\n\n        # subtract of mean of x for more accurate distance computations\n        if not sp.issparse(X):\n            X_mean = X.mean(axis=0)\n            # The copy was already done above\n            X -= X_mean\n\n            if init_is_array_like:\n                init -= X_mean\n\n        # precompute squared norms of data points\n        x_squared_norms = row_norms(X, squared=True)\n\n        if self._algorithm == \"elkan\":\n            kmeans_single = _kmeans_single_elkan\n        else:\n            kmeans_single = _kmeans_single_lloyd\n            self._check_mkl_vcomp(X, X.shape[0])\n\n        best_inertia, best_labels = None, None\n\n        for i in range(self._n_init):\n            # Initialize centers\n            centers_init = self._init_centroids(\n                X,\n                x_squared_norms=x_squared_norms,\n                init=init,\n                random_state=random_state,\n                sample_weight=sample_weight,\n            )\n            if self.verbose:\n                print(\"Initialization complete\")\n\n            # run a k-means once\n            labels, inertia, centers, n_iter_ = kmeans_single(\n                X,\n                sample_weight,\n                centers_init,\n                max_iter=self.max_iter,\n                verbose=self.verbose,\n                tol=self._tol,\n                n_threads=self._n_threads,\n            )\n\n            # determine if these results are the best so far\n            # we chose a new run if it has a better inertia and the clustering is\n            # different from the best so far (it's possible that the inertia is\n            # slightly better even if the clustering is the same with potentially\n            # permuted labels, due to rounding errors)\n            if best_inertia is None or (\n                inertia < best_inertia\n                and not _is_same_clustering(labels, best_labels, self.n_clusters)\n            ):\n                best_labels = labels\n                best_centers = centers\n                best_inertia = inertia\n                best_n_iter = n_iter_\n\n        if not sp.issparse(X):\n            if not self.copy_x:\n                X += X_mean\n            best_centers += X_mean\n\n        distinct_clusters = len(set(best_labels))\n        if distinct_clusters < self.n_clusters:\n            warnings.warn(\n                \"Number of distinct clusters ({}) found smaller than \"\n                \"n_clusters ({}). Possibly due to duplicate points \"\n                \"in X.\".format(distinct_clusters, self.n_clusters),\n                ConvergenceWarning,\n                stacklevel=2,\n            )\n\n        self.cluster_centers_ = best_centers\n        self._n_features_out = self.cluster_centers_.shape[0]\n        self.labels_ = best_labels\n        self.inertia_ = best_inertia\n        self.n_iter_ = best_n_iter\n        return self\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "dataset/_kmeans.py/k_means"
      ],
      "reference_who": [
        "dataset/_kmeans.py/_kmeans_single_elkan",
        "dataset/_kmeans.py/_kmeans_single_lloyd",
        "dataset/_kmeans.py/_BaseKMeans/_check_mkl_vcomp",
        "dataset/_kmeans.py/_BaseKMeans/_validate_center_shape",
        "dataset/_kmeans.py/_BaseKMeans/_init_centroids",
        "dataset/_kmeans.py/KMeans/_check_params_vs_input"
      ],
      "special_reference_type": [
        false,
        false,
        false,
        false,
        false,
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "_mini_batch_step",
      "md_content": [
        "**_mini_batch_step**: The function of _mini_batch_step is to perform an incremental update of the centers for the Minibatch K-Means algorithm.\n\n**parameters**: The parameters of this Function.\n· X: {ndarray, sparse matrix} of shape (n_samples, n_features) - The original data array. If sparse, must be in CSR format.  \n· sample_weight: ndarray of shape (n_samples,) - The weights for each observation in `X`.  \n· centers: ndarray of shape (n_clusters, n_features) - The cluster centers before the current iteration.  \n· centers_new: ndarray of shape (n_clusters, n_features) - The cluster centers after the current iteration. Modified in-place.  \n· weight_sums: ndarray of shape (n_clusters,) - The vector in which we keep track of the numbers of points in a cluster. This array is modified in place.  \n· random_state: RandomState instance - Determines random number generation for low count centers reassignment.  \n· random_reassign: boolean, default=False - If True, centers with very low counts are randomly reassigned to observations.  \n· reassignment_ratio: float, default=0.01 - Control the fraction of the maximum number of counts for a center to be reassigned.  \n· verbose: bool, default=False - Controls the verbosity of the output.  \n· n_threads: int, default=1 - The number of OpenMP threads to use for the computation.\n\n**Code Description**: The _mini_batch_step function is a key component of the Minibatch K-Means clustering algorithm, designed to efficiently update cluster centers based on a mini-batch of data. The function begins by assigning labels to the input samples (X) based on their proximity to the current cluster centers using the _labels_inertia function. This function computes both the labels and the inertia, which is the sum of squared distances from each sample to its nearest cluster center.\n\nAfter label assignment, the function updates the cluster centers. Depending on whether the input data is sparse or dense, it calls either _minibatch_update_sparse or _minibatch_update_dense to perform the update. This ensures that the algorithm can handle different data formats efficiently.\n\nThe function also includes a mechanism for reassignment of cluster centers that have very low counts. If the random_reassign parameter is set to True, centers with counts below a certain threshold (defined by the reassignment_ratio) are randomly reassigned to new observations from the dataset. This is intended to improve the clustering quality by preventing centers from becoming stagnant.\n\nThe _mini_batch_step function is called by the fit and partial_fit methods of the MiniBatchKMeans class. In these methods, it is used to iteratively update the cluster centers as new mini-batches of data are processed. This iterative approach allows the algorithm to converge more quickly and efficiently on the optimal cluster centers.\n\n**Note**: It is important to ensure that the input data X is in the correct format (ndarray or CSR sparse matrix) and that the sample weights are appropriately defined. The function is optimized for performance with the option to utilize multiple threads for computation.\n\n**Output Example**: A possible return value of the function when called with a dataset of 5 samples and 3 clusters might look like this:  \nInertia: 12.34"
      ],
      "code_start_line": 1583,
      "code_end_line": 1701,
      "params": [
        "X",
        "sample_weight",
        "centers",
        "centers_new",
        "weight_sums",
        "random_state",
        "random_reassign",
        "reassignment_ratio",
        "verbose",
        "n_threads"
      ],
      "have_return": true,
      "code_content": "def _mini_batch_step(\n    X,\n    sample_weight,\n    centers,\n    centers_new,\n    weight_sums,\n    random_state,\n    random_reassign=False,\n    reassignment_ratio=0.01,\n    verbose=False,\n    n_threads=1,\n):\n    \"\"\"Incremental update of the centers for the Minibatch K-Means algorithm.\n\n    Parameters\n    ----------\n\n    X : {ndarray, sparse matrix} of shape (n_samples, n_features)\n        The original data array. If sparse, must be in CSR format.\n\n    x_squared_norms : ndarray of shape (n_samples,)\n        Squared euclidean norm of each data point.\n\n    sample_weight : ndarray of shape (n_samples,)\n        The weights for each observation in `X`.\n\n    centers : ndarray of shape (n_clusters, n_features)\n        The cluster centers before the current iteration\n\n    centers_new : ndarray of shape (n_clusters, n_features)\n        The cluster centers after the current iteration. Modified in-place.\n\n    weight_sums : ndarray of shape (n_clusters,)\n        The vector in which we keep track of the numbers of points in a\n        cluster. This array is modified in place.\n\n    random_state : RandomState instance\n        Determines random number generation for low count centers reassignment.\n        See :term:`Glossary <random_state>`.\n\n    random_reassign : boolean, default=False\n        If True, centers with very low counts are randomly reassigned\n        to observations.\n\n    reassignment_ratio : float, default=0.01\n        Control the fraction of the maximum number of counts for a\n        center to be reassigned. A higher value means that low count\n        centers are more likely to be reassigned, which means that the\n        model will take longer to converge, but should converge in a\n        better clustering.\n\n    verbose : bool, default=False\n        Controls the verbosity.\n\n    n_threads : int, default=1\n        The number of OpenMP threads to use for the computation.\n\n    Returns\n    -------\n    inertia : float\n        Sum of squared distances of samples to their closest cluster center.\n        The inertia is computed after finding the labels and before updating\n        the centers.\n    \"\"\"\n    # Perform label assignment to nearest centers\n    # For better efficiency, it's better to run _mini_batch_step in a\n    # threadpool_limit context than using _labels_inertia_threadpool_limit here\n    labels, inertia = _labels_inertia(X, sample_weight, centers, n_threads=n_threads)\n\n    # Update centers according to the labels\n    if sp.issparse(X):\n        _minibatch_update_sparse(\n            X, sample_weight, centers, centers_new, weight_sums, labels, n_threads\n        )\n    else:\n        _minibatch_update_dense(\n            X,\n            sample_weight,\n            centers,\n            centers_new,\n            weight_sums,\n            labels,\n            n_threads,\n        )\n\n    # Reassign clusters that have very low weight\n    if random_reassign and reassignment_ratio > 0:\n        to_reassign = weight_sums < reassignment_ratio * weight_sums.max()\n\n        # pick at most .5 * batch_size samples as new centers\n        if to_reassign.sum() > 0.5 * X.shape[0]:\n            indices_dont_reassign = np.argsort(weight_sums)[int(0.5 * X.shape[0]) :]\n            to_reassign[indices_dont_reassign] = False\n        n_reassigns = to_reassign.sum()\n\n        if n_reassigns:\n            # Pick new clusters amongst observations with uniform probability\n            new_centers = random_state.choice(\n                X.shape[0], replace=False, size=n_reassigns\n            )\n            if verbose:\n                print(f\"[MiniBatchKMeans] Reassigning {n_reassigns} cluster centers.\")\n\n            if sp.issparse(X):\n                assign_rows_csr(\n                    X,\n                    new_centers.astype(np.intp, copy=False),\n                    np.where(to_reassign)[0].astype(np.intp, copy=False),\n                    centers_new,\n                )\n            else:\n                centers_new[to_reassign] = X[new_centers]\n\n        # reset counts of reassigned centers, but don't reset them too small\n        # to avoid instant reassignment. This is a pretty dirty hack as it\n        # also modifies the learning rates.\n        weight_sums[to_reassign] = np.min(weight_sums[~to_reassign])\n\n    return inertia\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "dataset/_kmeans.py/MiniBatchKMeans/fit",
        "dataset/_kmeans.py/MiniBatchKMeans/partial_fit"
      ],
      "reference_who": [
        "dataset/_kmeans.py/_labels_inertia"
      ],
      "special_reference_type": [
        false
      ]
    },
    {
      "type": "ClassDef",
      "name": "MiniBatchKMeans",
      "md_content": [
        "**MiniBatchKMeans**: The function of MiniBatchKMeans is to perform clustering on large datasets using a mini-batch approach to optimize the K-Means algorithm.\n\n**attributes**: The attributes of this Class.\n· n_clusters: The number of clusters to form as well as the number of centroids to generate.\n· init: Method for initialization of cluster centers, which can be 'k-means++', 'random', or a user-defined callable or array.\n· max_iter: Maximum number of iterations over the complete dataset before stopping.\n· batch_size: Size of the mini batches used in the optimization process.\n· verbose: Verbosity mode to control the level of output during the fitting process.\n· compute_labels: A boolean indicating whether to compute label assignments and inertia for the complete dataset after convergence.\n· random_state: Determines random number generation for centroid initialization and random reassignment.\n· tol: Control early stopping based on the relative center changes.\n· max_no_improvement: Control early stopping based on the consecutive number of mini batches that do not yield an improvement on the inertia.\n· init_size: Number of samples to randomly sample for speeding up the initialization.\n· n_init: Number of random initializations that are tried.\n· reassignment_ratio: Control the fraction of the maximum number of counts for a center to be reassigned.\n\n**Code Description**: The MiniBatchKMeans class is an implementation of the K-Means clustering algorithm optimized for large datasets by processing data in smaller batches. It inherits from the _BaseKMeans class, which provides foundational functionality and parameter validation. The class is designed to handle clustering tasks efficiently by minimizing memory usage and computational time.\n\nThe constructor initializes several parameters, including the number of clusters, initialization method, maximum iterations, batch size, and others. The class includes methods for fitting the model to the data, predicting cluster assignments, and updating the model incrementally with new data through the `partial_fit` method.\n\nThe `_check_params_vs_input` method validates the input parameters against the provided data, ensuring that the number of clusters does not exceed the number of samples and that the initialization method is appropriate. The `_mini_batch_convergence` method implements early stopping logic based on the convergence of the clustering process, allowing the algorithm to terminate when no significant improvements are observed.\n\nThe `fit` method computes the centroids on the input data by chunking it into mini-batches, while the `partial_fit` method allows for updating the K-Means estimate on a single mini-batch. The class also provides attributes to store the resulting cluster centers, labels, inertia, and the number of iterations and steps processed during fitting.\n\nOverall, MiniBatchKMeans is particularly useful for applications involving large datasets where traditional K-Means may be computationally expensive and memory-intensive.\n\n**Note**: When using the MiniBatchKMeans class, it is essential to ensure that the input data meets the specified constraints, particularly regarding the number of samples and clusters. Additionally, users should be aware of the initialization methods and their implications on the clustering results.\n\n**Output Example**: A possible return value from the `fit` method could be an object containing the fitted model, with attributes such as:\n```\ncluster_centers_: array([[3.55102041, 2.48979592],\n                          [1.06896552, 1.        ]])\nlabels_: array([1, 0, 0, 1, 0, 1], dtype=int32)\ninertia_: 5.123456789\nn_iter_: 5\nn_steps_: 10\n``` \nThis output indicates the cluster centers, the labels assigned to each sample, the inertia value, and the number of iterations and steps taken during the fitting process."
      ],
      "code_start_line": 1704,
      "code_end_line": 2318,
      "params": [],
      "have_return": true,
      "code_content": "class MiniBatchKMeans(_BaseKMeans):\n    \"\"\"\n    Mini-Batch K-Means clustering.\n\n    Read more in the :ref:`User Guide <mini_batch_kmeans>`.\n\n    Parameters\n    ----------\n\n    n_clusters : int, default=8\n        The number of clusters to form as well as the number of\n        centroids to generate.\n\n    init : {'k-means++', 'random'}, callable or array-like of shape \\\n            (n_clusters, n_features), default='k-means++'\n        Method for initialization:\n\n        'k-means++' : selects initial cluster centroids using sampling based on\n        an empirical probability distribution of the points' contribution to the\n        overall inertia. This technique speeds up convergence. The algorithm\n        implemented is \"greedy k-means++\". It differs from the vanilla k-means++\n        by making several trials at each sampling step and choosing the best centroid\n        among them.\n\n        'random': choose `n_clusters` observations (rows) at random from data\n        for the initial centroids.\n\n        If an array is passed, it should be of shape (n_clusters, n_features)\n        and gives the initial centers.\n\n        If a callable is passed, it should take arguments X, n_clusters and a\n        random state and return an initialization.\n\n    max_iter : int, default=100\n        Maximum number of iterations over the complete dataset before\n        stopping independently of any early stopping criterion heuristics.\n\n    batch_size : int, default=1024\n        Size of the mini batches.\n        For faster computations, you can set the ``batch_size`` greater than\n        256 * number of cores to enable parallelism on all cores.\n\n        .. versionchanged:: 1.0\n           `batch_size` default changed from 100 to 1024.\n\n    verbose : int, default=0\n        Verbosity mode.\n\n    compute_labels : bool, default=True\n        Compute label assignment and inertia for the complete dataset\n        once the minibatch optimization has converged in fit.\n\n    random_state : int, RandomState instance or None, default=None\n        Determines random number generation for centroid initialization and\n        random reassignment. Use an int to make the randomness deterministic.\n        See :term:`Glossary <random_state>`.\n\n    tol : float, default=0.0\n        Control early stopping based on the relative center changes as\n        measured by a smoothed, variance-normalized of the mean center\n        squared position changes. This early stopping heuristics is\n        closer to the one used for the batch variant of the algorithms\n        but induces a slight computational and memory overhead over the\n        inertia heuristic.\n\n        To disable convergence detection based on normalized center\n        change, set tol to 0.0 (default).\n\n    max_no_improvement : int, default=10\n        Control early stopping based on the consecutive number of mini\n        batches that does not yield an improvement on the smoothed inertia.\n\n        To disable convergence detection based on inertia, set\n        max_no_improvement to None.\n\n    init_size : int, default=None\n        Number of samples to randomly sample for speeding up the\n        initialization (sometimes at the expense of accuracy): the\n        only algorithm is initialized by running a batch KMeans on a\n        random subset of the data. This needs to be larger than n_clusters.\n\n        If `None`, the heuristic is `init_size = 3 * batch_size` if\n        `3 * batch_size < n_clusters`, else `init_size = 3 * n_clusters`.\n\n    n_init : 'auto' or int, default=\"auto\"\n        Number of random initializations that are tried.\n        In contrast to KMeans, the algorithm is only run once, using the best of\n        the `n_init` initializations as measured by inertia. Several runs are\n        recommended for sparse high-dimensional problems (see\n        :ref:`kmeans_sparse_high_dim`).\n\n        When `n_init='auto'`, the number of runs depends on the value of init:\n        3 if using `init='random'` or `init` is a callable;\n        1 if using `init='k-means++'` or `init` is an array-like.\n\n        .. versionadded:: 1.2\n           Added 'auto' option for `n_init`.\n\n        .. versionchanged:: 1.4\n           Default value for `n_init` changed to `'auto'` in version.\n\n    reassignment_ratio : float, default=0.01\n        Control the fraction of the maximum number of counts for a center to\n        be reassigned. A higher value means that low count centers are more\n        easily reassigned, which means that the model will take longer to\n        converge, but should converge in a better clustering. However, too high\n        a value may cause convergence issues, especially with a small batch\n        size.\n\n    Attributes\n    ----------\n\n    cluster_centers_ : ndarray of shape (n_clusters, n_features)\n        Coordinates of cluster centers.\n\n    labels_ : ndarray of shape (n_samples,)\n        Labels of each point (if compute_labels is set to True).\n\n    inertia_ : float\n        The value of the inertia criterion associated with the chosen\n        partition if compute_labels is set to True. If compute_labels is set to\n        False, it's an approximation of the inertia based on an exponentially\n        weighted average of the batch inertiae.\n        The inertia is defined as the sum of square distances of samples to\n        their cluster center, weighted by the sample weights if provided.\n\n    n_iter_ : int\n        Number of iterations over the full dataset.\n\n    n_steps_ : int\n        Number of minibatches processed.\n\n        .. versionadded:: 1.0\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    KMeans : The classic implementation of the clustering method based on the\n        Lloyd's algorithm. It consumes the whole set of input data at each\n        iteration.\n\n    Notes\n    -----\n    See https://www.eecs.tufts.edu/~dsculley/papers/fastkmeans.pdf\n\n    When there are too few points in the dataset, some centers may be\n    duplicated, which means that a proper clustering in terms of the number\n    of requesting clusters and the number of returned clusters will not\n    always match. One solution is to set `reassignment_ratio=0`, which\n    prevents reassignments of clusters that are too small.\n\n    Examples\n    --------\n    >>> from sklearn.cluster import MiniBatchKMeans\n    >>> import numpy as np\n    >>> X = np.array([[1, 2], [1, 4], [1, 0],\n    ...               [4, 2], [4, 0], [4, 4],\n    ...               [4, 5], [0, 1], [2, 2],\n    ...               [3, 2], [5, 5], [1, -1]])\n    >>> # manually fit on batches\n    >>> kmeans = MiniBatchKMeans(n_clusters=2,\n    ...                          random_state=0,\n    ...                          batch_size=6,\n    ...                          n_init=\"auto\")\n    >>> kmeans = kmeans.partial_fit(X[0:6,:])\n    >>> kmeans = kmeans.partial_fit(X[6:12,:])\n    >>> kmeans.cluster_centers_\n    array([[3.375, 3.  ],\n           [0.75 , 0.5 ]])\n    >>> kmeans.predict([[0, 0], [4, 4]])\n    array([1, 0], dtype=int32)\n    >>> # fit on the whole data\n    >>> kmeans = MiniBatchKMeans(n_clusters=2,\n    ...                          random_state=0,\n    ...                          batch_size=6,\n    ...                          max_iter=10,\n    ...                          n_init=\"auto\").fit(X)\n    >>> kmeans.cluster_centers_\n    array([[3.55102041, 2.48979592],\n           [1.06896552, 1.        ]])\n    >>> kmeans.predict([[0, 0], [4, 4]])\n    array([1, 0], dtype=int32)\n    \"\"\"\n\n    _parameter_constraints: dict = {\n        **_BaseKMeans._parameter_constraints,\n        \"batch_size\": [Interval(Integral, 1, None, closed=\"left\")],\n        \"compute_labels\": [\"boolean\"],\n        \"max_no_improvement\": [Interval(Integral, 0, None, closed=\"left\"), None],\n        \"init_size\": [Interval(Integral, 1, None, closed=\"left\"), None],\n        \"reassignment_ratio\": [Interval(Real, 0, None, closed=\"left\")],\n    }\n\n    def __init__(\n        self,\n        n_clusters=8,\n        *,\n        init=\"k-means++\",\n        max_iter=100,\n        batch_size=1024,\n        verbose=0,\n        compute_labels=True,\n        random_state=None,\n        tol=0.0,\n        max_no_improvement=10,\n        init_size=None,\n        n_init=\"auto\",\n        reassignment_ratio=0.01,\n    ):\n        super().__init__(\n            n_clusters=n_clusters,\n            init=init,\n            max_iter=max_iter,\n            verbose=verbose,\n            random_state=random_state,\n            tol=tol,\n            n_init=n_init,\n        )\n\n        self.max_no_improvement = max_no_improvement\n        self.batch_size = batch_size\n        self.compute_labels = compute_labels\n        self.init_size = init_size\n        self.reassignment_ratio = reassignment_ratio\n\n    def _check_params_vs_input(self, X):\n        super()._check_params_vs_input(X, default_n_init=3)\n\n        self._batch_size = min(self.batch_size, X.shape[0])\n\n        # init_size\n        self._init_size = self.init_size\n        if self._init_size is None:\n            self._init_size = 3 * self._batch_size\n            if self._init_size < self.n_clusters:\n                self._init_size = 3 * self.n_clusters\n        elif self._init_size < self.n_clusters:\n            warnings.warn(\n                (\n                    f\"init_size={self._init_size} should be larger than \"\n                    f\"n_clusters={self.n_clusters}. Setting it to \"\n                    \"min(3*n_clusters, n_samples)\"\n                ),\n                RuntimeWarning,\n                stacklevel=2,\n            )\n            self._init_size = 3 * self.n_clusters\n        self._init_size = min(self._init_size, X.shape[0])\n\n        # reassignment_ratio\n        if self.reassignment_ratio < 0:\n            raise ValueError(\n                \"reassignment_ratio should be >= 0, got \"\n                f\"{self.reassignment_ratio} instead.\"\n            )\n\n    def _warn_mkl_vcomp(self, n_active_threads):\n        \"\"\"Warn when vcomp and mkl are both present\"\"\"\n        warnings.warn(\n            \"MiniBatchKMeans is known to have a memory leak on \"\n            \"Windows with MKL, when there are less chunks than \"\n            \"available threads. You can prevent it by setting \"\n            f\"batch_size >= {self._n_threads * CHUNK_SIZE} or by \"\n            \"setting the environment variable \"\n            f\"OMP_NUM_THREADS={n_active_threads}\"\n        )\n\n    def _mini_batch_convergence(\n        self, step, n_steps, n_samples, centers_squared_diff, batch_inertia\n    ):\n        \"\"\"Helper function to encapsulate the early stopping logic\"\"\"\n        # Normalize inertia to be able to compare values when\n        # batch_size changes\n        batch_inertia /= self._batch_size\n\n        # count steps starting from 1 for user friendly verbose mode.\n        step = step + 1\n\n        # Ignore first iteration because it's inertia from initialization.\n        if step == 1:\n            if self.verbose:\n                print(\n                    f\"Minibatch step {step}/{n_steps}: mean batch \"\n                    f\"inertia: {batch_inertia}\"\n                )\n            return False\n\n        # Compute an Exponentially Weighted Average of the inertia to\n        # monitor the convergence while discarding minibatch-local stochastic\n        # variability: https://en.wikipedia.org/wiki/Moving_average\n        if self._ewa_inertia is None:\n            self._ewa_inertia = batch_inertia\n        else:\n            alpha = self._batch_size * 2.0 / (n_samples + 1)\n            alpha = min(alpha, 1)\n            self._ewa_inertia = self._ewa_inertia * (1 - alpha) + batch_inertia * alpha\n\n        # Log progress to be able to monitor convergence\n        if self.verbose:\n            print(\n                f\"Minibatch step {step}/{n_steps}: mean batch inertia: \"\n                f\"{batch_inertia}, ewa inertia: {self._ewa_inertia}\"\n            )\n\n        # Early stopping based on absolute tolerance on squared change of\n        # centers position\n        if self._tol > 0.0 and centers_squared_diff <= self._tol:\n            if self.verbose:\n                print(f\"Converged (small centers change) at step {step}/{n_steps}\")\n            return True\n\n        # Early stopping heuristic due to lack of improvement on smoothed\n        # inertia\n        if self._ewa_inertia_min is None or self._ewa_inertia < self._ewa_inertia_min:\n            self._no_improvement = 0\n            self._ewa_inertia_min = self._ewa_inertia\n        else:\n            self._no_improvement += 1\n\n        if (\n            self.max_no_improvement is not None\n            and self._no_improvement >= self.max_no_improvement\n        ):\n            if self.verbose:\n                print(\n                    \"Converged (lack of improvement in inertia) at step \"\n                    f\"{step}/{n_steps}\"\n                )\n            return True\n\n        return False\n\n    def _random_reassign(self):\n        \"\"\"Check if a random reassignment needs to be done.\n\n        Do random reassignments each time 10 * n_clusters samples have been\n        processed.\n\n        If there are empty clusters we always want to reassign.\n        \"\"\"\n        self._n_since_last_reassign += self._batch_size\n        if (self._counts == 0).any() or self._n_since_last_reassign >= (\n            10 * self.n_clusters\n        ):\n            self._n_since_last_reassign = 0\n            return True\n        return False\n\n    @_fit_context(prefer_skip_nested_validation=True)\n    def fit(self, X, y=None, sample_weight=None):\n        \"\"\"Compute the centroids on X by chunking it into mini-batches.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Training instances to cluster. It must be noted that the data\n            will be converted to C ordering, which will cause a memory copy\n            if the given data is not C-contiguous.\n            If a sparse matrix is passed, a copy will be made if it's not in\n            CSR format.\n\n        y : Ignored\n            Not used, present here for API consistency by convention.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            The weights for each observation in X. If None, all observations\n            are assigned equal weight. `sample_weight` is not used during\n            initialization if `init` is a callable or a user provided array.\n\n            .. versionadded:: 0.20\n\n        Returns\n        -------\n        self : object\n            Fitted estimator.\n        \"\"\"\n        X = self._validate_data(\n            X,\n            accept_sparse=\"csr\",\n            dtype=[np.float64, np.float32],\n            order=\"C\",\n            accept_large_sparse=False,\n        )\n\n        self._check_params_vs_input(X)\n        random_state = check_random_state(self.random_state)\n        sample_weight = _check_sample_weight(sample_weight, X, dtype=X.dtype)\n        self._n_threads = _openmp_effective_n_threads()\n        n_samples, n_features = X.shape\n\n        # Validate init array\n        init = self.init\n        if _is_arraylike_not_scalar(init):\n            init = check_array(init, dtype=X.dtype, copy=True, order=\"C\")\n            self._validate_center_shape(X, init)\n\n        self._check_mkl_vcomp(X, self._batch_size)\n\n        # precompute squared norms of data points\n        x_squared_norms = row_norms(X, squared=True)\n\n        # Validation set for the init\n        validation_indices = random_state.randint(0, n_samples, self._init_size)\n        X_valid = X[validation_indices]\n        sample_weight_valid = sample_weight[validation_indices]\n\n        # perform several inits with random subsets\n        best_inertia = None\n        for init_idx in range(self._n_init):\n            if self.verbose:\n                print(f\"Init {init_idx + 1}/{self._n_init} with method {init}\")\n\n            # Initialize the centers using only a fraction of the data as we\n            # expect n_samples to be very large when using MiniBatchKMeans.\n            cluster_centers = self._init_centroids(\n                X,\n                x_squared_norms=x_squared_norms,\n                init=init,\n                random_state=random_state,\n                init_size=self._init_size,\n                sample_weight=sample_weight,\n            )\n\n            # Compute inertia on a validation set.\n            _, inertia = _labels_inertia_threadpool_limit(\n                X_valid,\n                sample_weight_valid,\n                cluster_centers,\n                n_threads=self._n_threads,\n            )\n\n            if self.verbose:\n                print(f\"Inertia for init {init_idx + 1}/{self._n_init}: {inertia}\")\n            if best_inertia is None or inertia < best_inertia:\n                init_centers = cluster_centers\n                best_inertia = inertia\n\n        centers = init_centers\n        centers_new = np.empty_like(centers)\n\n        # Initialize counts\n        self._counts = np.zeros(self.n_clusters, dtype=X.dtype)\n\n        # Attributes to monitor the convergence\n        self._ewa_inertia = None\n        self._ewa_inertia_min = None\n        self._no_improvement = 0\n\n        # Initialize number of samples seen since last reassignment\n        self._n_since_last_reassign = 0\n\n        n_steps = (self.max_iter * n_samples) // self._batch_size\n\n        with threadpool_limits(limits=1, user_api=\"blas\"):\n            # Perform the iterative optimization until convergence\n            for i in range(n_steps):\n                # Sample a minibatch from the full dataset\n                minibatch_indices = random_state.randint(0, n_samples, self._batch_size)\n\n                # Perform the actual update step on the minibatch data\n                batch_inertia = _mini_batch_step(\n                    X=X[minibatch_indices],\n                    sample_weight=sample_weight[minibatch_indices],\n                    centers=centers,\n                    centers_new=centers_new,\n                    weight_sums=self._counts,\n                    random_state=random_state,\n                    random_reassign=self._random_reassign(),\n                    reassignment_ratio=self.reassignment_ratio,\n                    verbose=self.verbose,\n                    n_threads=self._n_threads,\n                )\n\n                if self._tol > 0.0:\n                    centers_squared_diff = np.sum((centers_new - centers) ** 2)\n                else:\n                    centers_squared_diff = 0\n\n                centers, centers_new = centers_new, centers\n\n                # Monitor convergence and do early stopping if necessary\n                if self._mini_batch_convergence(\n                    i, n_steps, n_samples, centers_squared_diff, batch_inertia\n                ):\n                    break\n\n        self.cluster_centers_ = centers\n        self._n_features_out = self.cluster_centers_.shape[0]\n\n        self.n_steps_ = i + 1\n        self.n_iter_ = int(np.ceil(((i + 1) * self._batch_size) / n_samples))\n\n        if self.compute_labels:\n            self.labels_, self.inertia_ = _labels_inertia_threadpool_limit(\n                X,\n                sample_weight,\n                self.cluster_centers_,\n                n_threads=self._n_threads,\n            )\n        else:\n            self.inertia_ = self._ewa_inertia * n_samples\n\n        return self\n\n    @_fit_context(prefer_skip_nested_validation=True)\n    def partial_fit(self, X, y=None, sample_weight=None):\n        \"\"\"Update k means estimate on a single mini-batch X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Training instances to cluster. It must be noted that the data\n            will be converted to C ordering, which will cause a memory copy\n            if the given data is not C-contiguous.\n            If a sparse matrix is passed, a copy will be made if it's not in\n            CSR format.\n\n        y : Ignored\n            Not used, present here for API consistency by convention.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            The weights for each observation in X. If None, all observations\n            are assigned equal weight. `sample_weight` is not used during\n            initialization if `init` is a callable or a user provided array.\n\n        Returns\n        -------\n        self : object\n            Return updated estimator.\n        \"\"\"\n        has_centers = hasattr(self, \"cluster_centers_\")\n\n        X = self._validate_data(\n            X,\n            accept_sparse=\"csr\",\n            dtype=[np.float64, np.float32],\n            order=\"C\",\n            accept_large_sparse=False,\n            reset=not has_centers,\n        )\n\n        self._random_state = getattr(\n            self, \"_random_state\", check_random_state(self.random_state)\n        )\n        sample_weight = _check_sample_weight(sample_weight, X, dtype=X.dtype)\n        self.n_steps_ = getattr(self, \"n_steps_\", 0)\n\n        # precompute squared norms of data points\n        x_squared_norms = row_norms(X, squared=True)\n\n        if not has_centers:\n            # this instance has not been fitted yet (fit or partial_fit)\n            self._check_params_vs_input(X)\n            self._n_threads = _openmp_effective_n_threads()\n\n            # Validate init array\n            init = self.init\n            if _is_arraylike_not_scalar(init):\n                init = check_array(init, dtype=X.dtype, copy=True, order=\"C\")\n                self._validate_center_shape(X, init)\n\n            self._check_mkl_vcomp(X, X.shape[0])\n\n            # initialize the cluster centers\n            self.cluster_centers_ = self._init_centroids(\n                X,\n                x_squared_norms=x_squared_norms,\n                init=init,\n                random_state=self._random_state,\n                init_size=self._init_size,\n                sample_weight=sample_weight,\n            )\n\n            # Initialize counts\n            self._counts = np.zeros(self.n_clusters, dtype=X.dtype)\n\n            # Initialize number of samples seen since last reassignment\n            self._n_since_last_reassign = 0\n\n        with threadpool_limits(limits=1, user_api=\"blas\"):\n            _mini_batch_step(\n                X,\n                sample_weight=sample_weight,\n                centers=self.cluster_centers_,\n                centers_new=self.cluster_centers_,\n                weight_sums=self._counts,\n                random_state=self._random_state,\n                random_reassign=self._random_reassign(),\n                reassignment_ratio=self.reassignment_ratio,\n                verbose=self.verbose,\n                n_threads=self._n_threads,\n            )\n\n        if self.compute_labels:\n            self.labels_, self.inertia_ = _labels_inertia_threadpool_limit(\n                X,\n                sample_weight,\n                self.cluster_centers_,\n                n_threads=self._n_threads,\n            )\n\n        self.n_steps_ += 1\n        self._n_features_out = self.cluster_centers_.shape[0]\n\n        return self\n",
      "name_column": 6,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "dataset/_kmeans.py/_BaseKMeans"
      ],
      "special_reference_type": [
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [
        "**__init__**: The function of __init__ is to initialize an instance of the MiniBatchKMeans class with specified parameters for clustering.\n\n**parameters**: The parameters of this Function.\n· n_clusters: The number of clusters to form, default is 8.  \n· init: Method for initialization, default is \"k-means++\".  \n· max_iter: Maximum number of iterations for a single run, default is 100.  \n· batch_size: Size of the mini-batches, default is 1024.  \n· verbose: Verbosity mode, default is 0 (no output).  \n· compute_labels: Whether to compute labels for the clusters, default is True.  \n· random_state: Seed for random number generation, default is None.  \n· tol: Tolerance for convergence, default is 0.0.  \n· max_no_improvement: Maximum number of iterations with no improvement before stopping, default is 10.  \n· init_size: Size of the initialization set, default is None.  \n· n_init: Number of time the k-means algorithm will be run with different centroid seeds, default is \"auto\".  \n· reassignment_ratio: The ratio of reassignment for the clusters, default is 0.01.  \n\n**Code Description**: The __init__ function is a constructor for the MiniBatchKMeans class, which is a variant of the KMeans clustering algorithm designed to handle large datasets efficiently by using mini-batches. The function accepts several parameters that allow users to customize the clustering process. The n_clusters parameter specifies how many clusters the algorithm should find. The init parameter determines the method used to initialize the cluster centers, with \"k-means++\" being a popular choice for better convergence. The max_iter parameter sets the upper limit on the number of iterations for the algorithm to run, ensuring that it does not run indefinitely. The batch_size parameter controls how many samples are processed in each iteration, which can significantly affect performance and memory usage.\n\nThe verbose parameter allows users to control the amount of information printed during the execution of the algorithm, which can be useful for debugging or monitoring progress. The compute_labels parameter indicates whether the algorithm should compute and return the labels of the clusters for each data point. The random_state parameter is used to seed the random number generator for reproducibility of results. The tol parameter sets the tolerance level for convergence, while max_no_improvement defines how many iterations without improvement are allowed before the algorithm stops. The init_size parameter can be specified to control the size of the initial sample used for initializing the cluster centers, and n_init determines how many times the algorithm will be run with different initializations to ensure a good solution. Finally, the reassignment_ratio parameter specifies the fraction of points that can be reassigned to different clusters during the clustering process.\n\n**Note**: It is important to choose the parameters wisely based on the dataset and the specific requirements of the clustering task. The default values are provided for convenience, but they may need to be adjusted for optimal performance in different scenarios."
      ],
      "code_start_line": 1907,
      "code_end_line": 1937,
      "params": [
        "self",
        "n_clusters"
      ],
      "have_return": false,
      "code_content": "    def __init__(\n        self,\n        n_clusters=8,\n        *,\n        init=\"k-means++\",\n        max_iter=100,\n        batch_size=1024,\n        verbose=0,\n        compute_labels=True,\n        random_state=None,\n        tol=0.0,\n        max_no_improvement=10,\n        init_size=None,\n        n_init=\"auto\",\n        reassignment_ratio=0.01,\n    ):\n        super().__init__(\n            n_clusters=n_clusters,\n            init=init,\n            max_iter=max_iter,\n            verbose=verbose,\n            random_state=random_state,\n            tol=tol,\n            n_init=n_init,\n        )\n\n        self.max_no_improvement = max_no_improvement\n        self.batch_size = batch_size\n        self.compute_labels = compute_labels\n        self.init_size = init_size\n        self.reassignment_ratio = reassignment_ratio\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "_check_params_vs_input",
      "md_content": [
        "**_check_params_vs_input**: The function of _check_params_vs_input is to validate and adjust parameters based on the input data provided for the MiniBatchKMeans clustering algorithm.\n\n**parameters**: The parameters of this Function.\n· X: {array-like, sparse matrix} of shape (n_samples, n_features) - The input data to be clustered, which must be validated for compatibility with the clustering algorithm.\n\n**Code Description**: The _check_params_vs_input function performs several critical checks and adjustments related to the parameters used in the MiniBatchKMeans algorithm. Initially, it calls the superclass method to ensure that the input data X meets the expected criteria, while also setting a default value for n_init if not specified. \n\nNext, the function determines the batch size by taking the minimum of the specified batch_size and the number of samples in X. This ensures that the batch size does not exceed the available data points. \n\nThe function then checks and sets the _init_size parameter, which dictates how many samples will be used to initialize the cluster centers. If _init_size is not provided, it defaults to three times the batch size, with a further check to ensure it is at least three times the number of clusters. If _init_size is less than the number of clusters, a warning is issued, and _init_size is adjusted accordingly. The function also ensures that _init_size does not exceed the number of samples in X.\n\nAdditionally, the function validates the reassignment_ratio parameter, raising a ValueError if it is negative, as this would be invalid for the clustering process.\n\nThis function is called within the fit and partial_fit methods of the MiniBatchKMeans class. In the fit method, it is invoked after validating the input data to ensure that all parameters are correctly set before proceeding with the clustering process. In the partial_fit method, it is called when initializing the cluster centers for the first time, ensuring that the parameters are appropriately configured for subsequent updates to the clustering model.\n\n**Note**: It is important to ensure that the input data X is in the correct format and that all parameters are set appropriately before calling the fit or partial_fit methods to avoid runtime errors and ensure optimal clustering performance."
      ],
      "code_start_line": 1939,
      "code_end_line": 1968,
      "params": [
        "self",
        "X"
      ],
      "have_return": false,
      "code_content": "    def _check_params_vs_input(self, X):\n        super()._check_params_vs_input(X, default_n_init=3)\n\n        self._batch_size = min(self.batch_size, X.shape[0])\n\n        # init_size\n        self._init_size = self.init_size\n        if self._init_size is None:\n            self._init_size = 3 * self._batch_size\n            if self._init_size < self.n_clusters:\n                self._init_size = 3 * self.n_clusters\n        elif self._init_size < self.n_clusters:\n            warnings.warn(\n                (\n                    f\"init_size={self._init_size} should be larger than \"\n                    f\"n_clusters={self.n_clusters}. Setting it to \"\n                    \"min(3*n_clusters, n_samples)\"\n                ),\n                RuntimeWarning,\n                stacklevel=2,\n            )\n            self._init_size = 3 * self.n_clusters\n        self._init_size = min(self._init_size, X.shape[0])\n\n        # reassignment_ratio\n        if self.reassignment_ratio < 0:\n            raise ValueError(\n                \"reassignment_ratio should be >= 0, got \"\n                f\"{self.reassignment_ratio} instead.\"\n            )\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "dataset/_kmeans.py/MiniBatchKMeans/fit",
        "dataset/_kmeans.py/MiniBatchKMeans/partial_fit"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "_warn_mkl_vcomp",
      "md_content": [
        "**_warn_mkl_vcomp**: The function of _warn_mkl_vcomp is to issue a warning regarding potential memory leaks when using MiniBatchKMeans with MKL on Windows.\n\n**parameters**: The parameters of this Function.\n· n_active_threads: An integer representing the number of active threads that are being utilized.\n\n**Code Description**: The _warn_mkl_vcomp function is designed to alert users when both the Intel Math Kernel Library (MKL) and the vcomp library are present in the environment. This situation can lead to a memory leak issue specifically on Windows systems when the number of chunks processed is less than the number of available threads. The function takes one parameter, n_active_threads, which indicates how many threads are actively being used during the execution of the MiniBatchKMeans algorithm. \n\nWhen the function is called, it triggers a warning message that informs the user about the potential memory leak. The warning suggests two possible solutions to mitigate the issue: either increase the batch size to be greater than or equal to the product of the number of threads and a predefined chunk size (self._n_threads * CHUNK_SIZE), or set the environment variable OMP_NUM_THREADS to the value of n_active_threads. This guidance helps users adjust their configurations to avoid performance degradation due to memory management issues.\n\n**Note**: It is important for users to heed this warning when using MiniBatchKMeans in environments where both MKL and vcomp are present, especially on Windows, to ensure optimal performance and prevent memory leaks."
      ],
      "code_start_line": 1970,
      "code_end_line": 1979,
      "params": [
        "self",
        "n_active_threads"
      ],
      "have_return": false,
      "code_content": "    def _warn_mkl_vcomp(self, n_active_threads):\n        \"\"\"Warn when vcomp and mkl are both present\"\"\"\n        warnings.warn(\n            \"MiniBatchKMeans is known to have a memory leak on \"\n            \"Windows with MKL, when there are less chunks than \"\n            \"available threads. You can prevent it by setting \"\n            f\"batch_size >= {self._n_threads * CHUNK_SIZE} or by \"\n            \"setting the environment variable \"\n            f\"OMP_NUM_THREADS={n_active_threads}\"\n        )\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "_mini_batch_convergence",
      "md_content": [
        "**_mini_batch_convergence**: The function of _mini_batch_convergence is to implement early stopping logic for the MiniBatchKMeans clustering algorithm based on convergence criteria.\n\n**parameters**: The parameters of this Function.\n· step: An integer representing the current iteration step in the minibatch process, starting from 0.\n· n_steps: An integer indicating the total number of steps to be performed during the fitting process.\n· n_samples: An integer representing the total number of samples in the dataset.\n· centers_squared_diff: A float representing the squared difference in the positions of the cluster centers between iterations.\n· batch_inertia: A float representing the inertia (or within-cluster sum of squares) calculated for the current minibatch.\n\n**Code Description**: The _mini_batch_convergence function is a helper function designed to monitor the convergence of the MiniBatchKMeans algorithm during its iterative fitting process. It normalizes the batch inertia by dividing it by the batch size to ensure comparability across different batch sizes. The function begins by incrementing the step count for user-friendly output. It ignores the first iteration since it only contains inertia from initialization.\n\nThe function computes an Exponentially Weighted Average (EWA) of the batch inertia to smooth out the stochastic variability inherent in minibatch processing. This is done using a formula that incorporates a decay factor (alpha), which is derived from the batch size and the total number of samples. The EWA inertia is then logged if verbosity is enabled.\n\nThe function checks for convergence based on two criteria: \n1. If the absolute change in the position of the cluster centers (centers_squared_diff) is less than a predefined tolerance (_tol), it indicates convergence due to small changes in the centers.\n2. If there is no improvement in the smoothed inertia over a specified number of iterations (max_no_improvement), it also indicates convergence.\n\nIf either of these conditions is met, the function returns True, signaling that the algorithm can stop early. If neither condition is satisfied, it returns False, allowing the fitting process to continue.\n\nThis function is called within the fit method of the MiniBatchKMeans class, specifically during the iterative optimization loop. After each minibatch update, the fit method invokes _mini_batch_convergence to assess whether the algorithm has converged based on the current step's inertia and the change in cluster centers. This integration ensures that the fitting process is efficient and can terminate early when appropriate, enhancing performance and reducing unnecessary computations.\n\n**Note**: It is important to ensure that the parameters passed to this function are correctly calculated and represent the current state of the fitting process to avoid incorrect convergence signals.\n\n**Output Example**: The function may return a boolean value, such as True or False, indicating whether the algorithm has converged. For instance, if the function detects convergence due to small changes in cluster centers, it would return True."
      ],
      "code_start_line": 1981,
      "code_end_line": 2044,
      "params": [
        "self",
        "step",
        "n_steps",
        "n_samples",
        "centers_squared_diff",
        "batch_inertia"
      ],
      "have_return": true,
      "code_content": "    def _mini_batch_convergence(\n        self, step, n_steps, n_samples, centers_squared_diff, batch_inertia\n    ):\n        \"\"\"Helper function to encapsulate the early stopping logic\"\"\"\n        # Normalize inertia to be able to compare values when\n        # batch_size changes\n        batch_inertia /= self._batch_size\n\n        # count steps starting from 1 for user friendly verbose mode.\n        step = step + 1\n\n        # Ignore first iteration because it's inertia from initialization.\n        if step == 1:\n            if self.verbose:\n                print(\n                    f\"Minibatch step {step}/{n_steps}: mean batch \"\n                    f\"inertia: {batch_inertia}\"\n                )\n            return False\n\n        # Compute an Exponentially Weighted Average of the inertia to\n        # monitor the convergence while discarding minibatch-local stochastic\n        # variability: https://en.wikipedia.org/wiki/Moving_average\n        if self._ewa_inertia is None:\n            self._ewa_inertia = batch_inertia\n        else:\n            alpha = self._batch_size * 2.0 / (n_samples + 1)\n            alpha = min(alpha, 1)\n            self._ewa_inertia = self._ewa_inertia * (1 - alpha) + batch_inertia * alpha\n\n        # Log progress to be able to monitor convergence\n        if self.verbose:\n            print(\n                f\"Minibatch step {step}/{n_steps}: mean batch inertia: \"\n                f\"{batch_inertia}, ewa inertia: {self._ewa_inertia}\"\n            )\n\n        # Early stopping based on absolute tolerance on squared change of\n        # centers position\n        if self._tol > 0.0 and centers_squared_diff <= self._tol:\n            if self.verbose:\n                print(f\"Converged (small centers change) at step {step}/{n_steps}\")\n            return True\n\n        # Early stopping heuristic due to lack of improvement on smoothed\n        # inertia\n        if self._ewa_inertia_min is None or self._ewa_inertia < self._ewa_inertia_min:\n            self._no_improvement = 0\n            self._ewa_inertia_min = self._ewa_inertia\n        else:\n            self._no_improvement += 1\n\n        if (\n            self.max_no_improvement is not None\n            and self._no_improvement >= self.max_no_improvement\n        ):\n            if self.verbose:\n                print(\n                    \"Converged (lack of improvement in inertia) at step \"\n                    f\"{step}/{n_steps}\"\n                )\n            return True\n\n        return False\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "dataset/_kmeans.py/MiniBatchKMeans/fit"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "_random_reassign",
      "md_content": [
        "_random_reassign: The function of _random_reassign is to determine whether a random reassignment of clusters should occur based on the number of processed samples and the presence of empty clusters.\n\nparameters: The parameters of this Function.\n· None\n\nCode Description: The _random_reassign function is a private method within the MiniBatchKMeans class that checks if a random reassignment of data points to clusters is necessary. This function is crucial for maintaining the effectiveness of the clustering algorithm, particularly in scenarios where some clusters may become empty or when a significant number of samples have been processed since the last reassignment.\n\nThe function operates by first incrementing the _n_since_last_reassign attribute by the size of the current batch (_batch_size). It then evaluates two conditions to decide if a reassignment should take place:\n1. It checks if any clusters are empty by evaluating if the _counts array (which tracks the number of samples assigned to each cluster) contains any zeros. If any cluster is empty, a reassignment is warranted.\n2. It also checks if the number of samples processed since the last reassignment (_n_since_last_reassign) has reached or exceeded ten times the number of clusters (10 * n_clusters). This ensures that reassignments occur at regular intervals, promoting better convergence of the algorithm.\n\nIf either condition is met, the function resets _n_since_last_reassign to zero and returns True, indicating that a reassignment should occur. If neither condition is satisfied, it returns False.\n\nThe _random_reassign function is called within the fit and partial_fit methods of the MiniBatchKMeans class. In the fit method, it is invoked during the iterative optimization process, where mini-batches of data are processed. The result of _random_reassign informs the _mini_batch_step function whether to perform a random reassignment of data points to clusters. Similarly, in the partial_fit method, it is used to determine if a reassignment is needed when updating the clustering model with a new mini-batch of data. This integration ensures that the clustering algorithm remains adaptive and responsive to the distribution of data points across clusters.\n\nNote: It is important to ensure that the MiniBatchKMeans instance has been initialized correctly before invoking the fit or partial_fit methods, as the behavior of _random_reassign relies on the state of the clustering model.\n\nOutput Example: The function does not return a value in the traditional sense but returns a boolean indicating whether a reassignment is needed. For example, it may return True if a reassignment is warranted or False if it is not."
      ],
      "code_start_line": 2046,
      "code_end_line": 2060,
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def _random_reassign(self):\n        \"\"\"Check if a random reassignment needs to be done.\n\n        Do random reassignments each time 10 * n_clusters samples have been\n        processed.\n\n        If there are empty clusters we always want to reassign.\n        \"\"\"\n        self._n_since_last_reassign += self._batch_size\n        if (self._counts == 0).any() or self._n_since_last_reassign >= (\n            10 * self.n_clusters\n        ):\n            self._n_since_last_reassign = 0\n            return True\n        return False\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "dataset/_kmeans.py/MiniBatchKMeans/fit",
        "dataset/_kmeans.py/MiniBatchKMeans/partial_fit"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "fit",
      "md_content": [
        "**fit**: The function of fit is to compute the centroids on the input data X by chunking it into mini-batches.\n\n**parameters**: The parameters of this Function.\n· X: {array-like, sparse matrix} of shape (n_samples, n_features) - Training instances to cluster. The data will be converted to C ordering, which may cause a memory copy if the data is not C-contiguous. If a sparse matrix is passed, a copy will be made if it's not in CSR format.  \n· y: Ignored - Not used, present here for API consistency by convention.  \n· sample_weight: array-like of shape (n_samples,), default=None - The weights for each observation in X. If None, all observations are assigned equal weight. `sample_weight` is not used during initialization if `init` is a callable or a user-provided array.\n\n**Code Description**: The fit function is a core method of the MiniBatchKMeans class, responsible for performing the clustering operation on the provided dataset X. It begins by validating the input data through the _validate_data method, ensuring that the data is in an acceptable format and meets the requirements for clustering. The function checks for the compatibility of the input parameters with the data, including the initialization of cluster centers and the handling of sample weights.\n\nThe function then initializes several variables, including the number of samples and features in the dataset. It validates the initial cluster centers if provided, ensuring they conform to the expected shape relative to the number of clusters and features in the data. The function also precomputes the squared norms of the data points to optimize subsequent calculations.\n\nThe main clustering process is executed in an iterative loop, where mini-batches of data are sampled and processed. For each mini-batch, the function updates the cluster centers based on the assigned labels using the _mini_batch_step method. This method performs the actual update of the centers and computes the inertia, which quantifies the compactness of the clusters.\n\nThe function monitors convergence through the _mini_batch_convergence method, which checks if the algorithm has reached a satisfactory solution based on the change in cluster centers and the inertia values. If convergence criteria are met, the fitting process is terminated early to enhance efficiency.\n\nFinally, the function returns the fitted estimator, which includes the computed cluster centers and other relevant attributes, such as labels and inertia, if requested. The fit method is integral to the MiniBatchKMeans algorithm, allowing it to efficiently handle large datasets by processing them in smaller, manageable chunks.\n\n**Note**: It is crucial to ensure that the input data X is in the correct format and that all parameters are appropriately set before invoking the fit method to avoid runtime errors and ensure optimal clustering performance.\n\n**Output Example**: A possible return value of the function when called with a dataset of 5 samples and 3 clusters might look like this:  \nCluster Centers: [[1.5, 2.0], [3.0, 4.5], [5.0, 6.0]]  \nLabels: [0, 1, 0, 2, 1]  \nInertia: 10.56"
      ],
      "code_start_line": 2063,
      "code_end_line": 2216,
      "params": [
        "self",
        "X",
        "y",
        "sample_weight"
      ],
      "have_return": true,
      "code_content": "    def fit(self, X, y=None, sample_weight=None):\n        \"\"\"Compute the centroids on X by chunking it into mini-batches.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Training instances to cluster. It must be noted that the data\n            will be converted to C ordering, which will cause a memory copy\n            if the given data is not C-contiguous.\n            If a sparse matrix is passed, a copy will be made if it's not in\n            CSR format.\n\n        y : Ignored\n            Not used, present here for API consistency by convention.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            The weights for each observation in X. If None, all observations\n            are assigned equal weight. `sample_weight` is not used during\n            initialization if `init` is a callable or a user provided array.\n\n            .. versionadded:: 0.20\n\n        Returns\n        -------\n        self : object\n            Fitted estimator.\n        \"\"\"\n        X = self._validate_data(\n            X,\n            accept_sparse=\"csr\",\n            dtype=[np.float64, np.float32],\n            order=\"C\",\n            accept_large_sparse=False,\n        )\n\n        self._check_params_vs_input(X)\n        random_state = check_random_state(self.random_state)\n        sample_weight = _check_sample_weight(sample_weight, X, dtype=X.dtype)\n        self._n_threads = _openmp_effective_n_threads()\n        n_samples, n_features = X.shape\n\n        # Validate init array\n        init = self.init\n        if _is_arraylike_not_scalar(init):\n            init = check_array(init, dtype=X.dtype, copy=True, order=\"C\")\n            self._validate_center_shape(X, init)\n\n        self._check_mkl_vcomp(X, self._batch_size)\n\n        # precompute squared norms of data points\n        x_squared_norms = row_norms(X, squared=True)\n\n        # Validation set for the init\n        validation_indices = random_state.randint(0, n_samples, self._init_size)\n        X_valid = X[validation_indices]\n        sample_weight_valid = sample_weight[validation_indices]\n\n        # perform several inits with random subsets\n        best_inertia = None\n        for init_idx in range(self._n_init):\n            if self.verbose:\n                print(f\"Init {init_idx + 1}/{self._n_init} with method {init}\")\n\n            # Initialize the centers using only a fraction of the data as we\n            # expect n_samples to be very large when using MiniBatchKMeans.\n            cluster_centers = self._init_centroids(\n                X,\n                x_squared_norms=x_squared_norms,\n                init=init,\n                random_state=random_state,\n                init_size=self._init_size,\n                sample_weight=sample_weight,\n            )\n\n            # Compute inertia on a validation set.\n            _, inertia = _labels_inertia_threadpool_limit(\n                X_valid,\n                sample_weight_valid,\n                cluster_centers,\n                n_threads=self._n_threads,\n            )\n\n            if self.verbose:\n                print(f\"Inertia for init {init_idx + 1}/{self._n_init}: {inertia}\")\n            if best_inertia is None or inertia < best_inertia:\n                init_centers = cluster_centers\n                best_inertia = inertia\n\n        centers = init_centers\n        centers_new = np.empty_like(centers)\n\n        # Initialize counts\n        self._counts = np.zeros(self.n_clusters, dtype=X.dtype)\n\n        # Attributes to monitor the convergence\n        self._ewa_inertia = None\n        self._ewa_inertia_min = None\n        self._no_improvement = 0\n\n        # Initialize number of samples seen since last reassignment\n        self._n_since_last_reassign = 0\n\n        n_steps = (self.max_iter * n_samples) // self._batch_size\n\n        with threadpool_limits(limits=1, user_api=\"blas\"):\n            # Perform the iterative optimization until convergence\n            for i in range(n_steps):\n                # Sample a minibatch from the full dataset\n                minibatch_indices = random_state.randint(0, n_samples, self._batch_size)\n\n                # Perform the actual update step on the minibatch data\n                batch_inertia = _mini_batch_step(\n                    X=X[minibatch_indices],\n                    sample_weight=sample_weight[minibatch_indices],\n                    centers=centers,\n                    centers_new=centers_new,\n                    weight_sums=self._counts,\n                    random_state=random_state,\n                    random_reassign=self._random_reassign(),\n                    reassignment_ratio=self.reassignment_ratio,\n                    verbose=self.verbose,\n                    n_threads=self._n_threads,\n                )\n\n                if self._tol > 0.0:\n                    centers_squared_diff = np.sum((centers_new - centers) ** 2)\n                else:\n                    centers_squared_diff = 0\n\n                centers, centers_new = centers_new, centers\n\n                # Monitor convergence and do early stopping if necessary\n                if self._mini_batch_convergence(\n                    i, n_steps, n_samples, centers_squared_diff, batch_inertia\n                ):\n                    break\n\n        self.cluster_centers_ = centers\n        self._n_features_out = self.cluster_centers_.shape[0]\n\n        self.n_steps_ = i + 1\n        self.n_iter_ = int(np.ceil(((i + 1) * self._batch_size) / n_samples))\n\n        if self.compute_labels:\n            self.labels_, self.inertia_ = _labels_inertia_threadpool_limit(\n                X,\n                sample_weight,\n                self.cluster_centers_,\n                n_threads=self._n_threads,\n            )\n        else:\n            self.inertia_ = self._ewa_inertia * n_samples\n\n        return self\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "dataset/_kmeans.py/_labels_inertia_threadpool_limit",
        "dataset/_kmeans.py/_BaseKMeans/_check_mkl_vcomp",
        "dataset/_kmeans.py/_BaseKMeans/_validate_center_shape",
        "dataset/_kmeans.py/_BaseKMeans/_init_centroids",
        "dataset/_kmeans.py/_mini_batch_step",
        "dataset/_kmeans.py/MiniBatchKMeans/_check_params_vs_input",
        "dataset/_kmeans.py/MiniBatchKMeans/_mini_batch_convergence",
        "dataset/_kmeans.py/MiniBatchKMeans/_random_reassign"
      ],
      "special_reference_type": [
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "partial_fit",
      "md_content": [
        "**partial_fit**: The function of partial_fit is to update the k-means estimate on a single mini-batch of data.\n\n**parameters**: The parameters of this Function.\n· X: {array-like, sparse matrix} of shape (n_samples, n_features) - Training instances to cluster. The data will be converted to C ordering, which may cause a memory copy if the data is not C-contiguous. If a sparse matrix is passed, a copy will be made if it's not in CSR format.  \n· y: Ignored - Not used, present here for API consistency by convention.  \n· sample_weight: array-like of shape (n_samples,), default=None - The weights for each observation in X. If None, all observations are assigned equal weight. `sample_weight` is not used during initialization if `init` is a callable or a user-provided array.\n\n**Code Description**: The partial_fit function is a core method of the MiniBatchKMeans class, designed to incrementally update the clustering model with new data. It begins by checking if the model has already been fitted by examining the presence of cluster centers. If the model has not been fitted, it initializes the cluster centers based on the provided data and specified initialization method.\n\nThe function first validates the input data X, ensuring it is in an acceptable format (either dense or sparse) and conforms to the expected data types. It also checks the sample weights to ensure they are appropriately defined. The squared norms of the data points are precomputed to facilitate efficient distance calculations during the clustering process.\n\nIf the model is being fitted for the first time, the function validates the initialization parameters and computes the initial cluster centers using the _init_centroids method. It also initializes the counts of samples assigned to each cluster and tracks the number of samples seen since the last reassignment.\n\nThe core of the function involves performing a mini-batch step, where the _mini_batch_step function is called to update the cluster centers based on the current mini-batch of data. This function handles the assignment of samples to the nearest cluster centers and updates the centers accordingly.\n\nAdditionally, if the compute_labels attribute is set to True, the function computes the labels and inertia for the current mini-batch using the _labels_inertia_threadpool_limit function. This provides insights into the clustering quality and helps in monitoring the convergence of the algorithm.\n\nThe function concludes by incrementing the number of steps taken and returning the updated estimator, allowing for further incremental updates with additional data.\n\nThe partial_fit method is integral to the MiniBatchKMeans algorithm, enabling it to process large datasets in smaller, manageable chunks while continuously refining the clustering model.\n\n**Note**: It is essential to ensure that the input data X is in the correct format and that all parameters are set appropriately before calling the partial_fit method to avoid runtime errors and ensure optimal clustering performance.\n\n**Output Example**: A possible return value of the function could be the updated MiniBatchKMeans instance itself, reflecting the changes made during the fitting process. For example:  \nMiniBatchKMeans(n_clusters=3, n_steps=1, cluster_centers_=array([[1.5, 2.5], [3.0, 4.0], [5.0, 6.0]]))"
      ],
      "code_start_line": 2219,
      "code_end_line": 2318,
      "params": [
        "self",
        "X",
        "y",
        "sample_weight"
      ],
      "have_return": true,
      "code_content": "    def partial_fit(self, X, y=None, sample_weight=None):\n        \"\"\"Update k means estimate on a single mini-batch X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Training instances to cluster. It must be noted that the data\n            will be converted to C ordering, which will cause a memory copy\n            if the given data is not C-contiguous.\n            If a sparse matrix is passed, a copy will be made if it's not in\n            CSR format.\n\n        y : Ignored\n            Not used, present here for API consistency by convention.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            The weights for each observation in X. If None, all observations\n            are assigned equal weight. `sample_weight` is not used during\n            initialization if `init` is a callable or a user provided array.\n\n        Returns\n        -------\n        self : object\n            Return updated estimator.\n        \"\"\"\n        has_centers = hasattr(self, \"cluster_centers_\")\n\n        X = self._validate_data(\n            X,\n            accept_sparse=\"csr\",\n            dtype=[np.float64, np.float32],\n            order=\"C\",\n            accept_large_sparse=False,\n            reset=not has_centers,\n        )\n\n        self._random_state = getattr(\n            self, \"_random_state\", check_random_state(self.random_state)\n        )\n        sample_weight = _check_sample_weight(sample_weight, X, dtype=X.dtype)\n        self.n_steps_ = getattr(self, \"n_steps_\", 0)\n\n        # precompute squared norms of data points\n        x_squared_norms = row_norms(X, squared=True)\n\n        if not has_centers:\n            # this instance has not been fitted yet (fit or partial_fit)\n            self._check_params_vs_input(X)\n            self._n_threads = _openmp_effective_n_threads()\n\n            # Validate init array\n            init = self.init\n            if _is_arraylike_not_scalar(init):\n                init = check_array(init, dtype=X.dtype, copy=True, order=\"C\")\n                self._validate_center_shape(X, init)\n\n            self._check_mkl_vcomp(X, X.shape[0])\n\n            # initialize the cluster centers\n            self.cluster_centers_ = self._init_centroids(\n                X,\n                x_squared_norms=x_squared_norms,\n                init=init,\n                random_state=self._random_state,\n                init_size=self._init_size,\n                sample_weight=sample_weight,\n            )\n\n            # Initialize counts\n            self._counts = np.zeros(self.n_clusters, dtype=X.dtype)\n\n            # Initialize number of samples seen since last reassignment\n            self._n_since_last_reassign = 0\n\n        with threadpool_limits(limits=1, user_api=\"blas\"):\n            _mini_batch_step(\n                X,\n                sample_weight=sample_weight,\n                centers=self.cluster_centers_,\n                centers_new=self.cluster_centers_,\n                weight_sums=self._counts,\n                random_state=self._random_state,\n                random_reassign=self._random_reassign(),\n                reassignment_ratio=self.reassignment_ratio,\n                verbose=self.verbose,\n                n_threads=self._n_threads,\n            )\n\n        if self.compute_labels:\n            self.labels_, self.inertia_ = _labels_inertia_threadpool_limit(\n                X,\n                sample_weight,\n                self.cluster_centers_,\n                n_threads=self._n_threads,\n            )\n\n        self.n_steps_ += 1\n        self._n_features_out = self.cluster_centers_.shape[0]\n\n        return self\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "dataset/_kmeans.py/_labels_inertia_threadpool_limit",
        "dataset/_kmeans.py/_BaseKMeans/_check_mkl_vcomp",
        "dataset/_kmeans.py/_BaseKMeans/_validate_center_shape",
        "dataset/_kmeans.py/_BaseKMeans/_init_centroids",
        "dataset/_kmeans.py/_mini_batch_step",
        "dataset/_kmeans.py/MiniBatchKMeans/_check_params_vs_input",
        "dataset/_kmeans.py/MiniBatchKMeans/_random_reassign"
      ],
      "special_reference_type": [
        false,
        false,
        false,
        false,
        false,
        false,
        false
      ]
    }
  ],
  "dataset/verify.py": [
    {
      "type": "ClassDef",
      "name": "ReturnCode",
      "md_content": [
        "**ReturnCode**: The function of ReturnCode is to define a set of constants representing the various return statuses for file verification operations.\n\n**attributes**: The attributes of this Class.\n· SUCCESS: Indicates that the operation completed successfully.  \n· INTEGRITY_FAILURE: Indicates that a file's integrity check has failed.  \n· FILE_GET_FAILED: Indicates that the attempt to retrieve a file was unsuccessful.  \n· FILE_MISSING_FROM_ONE_HOST: Indicates that a file was not found on one of the specified hosts.  \n· FILES_NOT_EQUAL: Indicates that the files being compared are not identical.  \n· NO_BINARIES_MATCH: Indicates that no binaries matched the specified criteria.  \n· NOT_ENOUGH_GOOD_SIGS: Indicates that there are not enough trusted signatures to meet the required threshold.  \n· BINARY_DOWNLOAD_FAILED: Indicates that the download of a binary file has failed.  \n· BAD_VERSION: Indicates that the version provided is not acceptable or is incorrectly formatted.  \n\n**Code Description**: The ReturnCode class is an enumeration that extends the functionality of the built-in IntEnum class from the enum module. It provides a clear and organized way to represent various return codes that can be used throughout the dataset verification process. Each attribute corresponds to a specific outcome of operations related to file retrieval, integrity checks, and signature verification.\n\nThis class is utilized in several functions within the dataset/verify.py module, including get_files_from_hosts_and_compare, verify_shasums_signature, verify_binary_hashes, verify_published_handler, and verify_binaries_handler. Each of these functions returns a ReturnCode value to indicate the result of their operations. For instance, if a file cannot be retrieved from a host, the function will return ReturnCode.FILE_GET_FAILED. Similarly, if the integrity of a file fails, ReturnCode.INTEGRITY_FAILURE will be returned.\n\nThe use of ReturnCode enhances code readability and maintainability by providing meaningful names for return values instead of using arbitrary integers. This allows developers to quickly understand the outcome of operations without needing to reference documentation or comments extensively.\n\n**Note**: When using the ReturnCode class, it is essential to handle each return value appropriately in the calling functions to ensure that errors are logged and managed correctly. This practice helps maintain the robustness of the verification process and provides clear feedback to users regarding the status of their operations."
      ],
      "code_start_line": 56,
      "code_end_line": 65,
      "params": [],
      "have_return": false,
      "code_content": "class ReturnCode(enum.IntEnum):\n    SUCCESS = 0\n    INTEGRITY_FAILURE = 1\n    FILE_GET_FAILED = 4\n    FILE_MISSING_FROM_ONE_HOST = 5\n    FILES_NOT_EQUAL = 6\n    NO_BINARIES_MATCH = 7\n    NOT_ENOUGH_GOOD_SIGS = 9\n    BINARY_DOWNLOAD_FAILED = 10\n    BAD_VERSION = 11\n",
      "name_column": 6,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "dataset/verify.py/get_files_from_hosts_and_compare",
        "dataset/verify.py/verify_shasums_signature",
        "dataset/verify.py/verify_binary_hashes",
        "dataset/verify.py/verify_published_handler",
        "dataset/verify.py/verify_binaries_handler"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "set_up_logger",
      "md_content": [
        "**set_up_logger**: The function of set_up_logger is to configure a logger that outputs log messages to standard error (stderr).\n\n**parameters**: The parameters of this Function.\n· is_verbose: A boolean value that determines the logging level. If set to True, the logger will log informational messages; if set to False, it will log warnings and above.\n\n**Code Description**: The set_up_logger function initializes a logger using Python's logging module. It first retrieves a logger instance associated with the current module using `logging.getLogger(__name__)`. The logging level is set based on the is_verbose parameter: if is_verbose is True, the logging level is set to INFO, allowing informational messages to be logged; if False, the level is set to WARNING, which restricts logging to warning messages and errors only. \n\nNext, a StreamHandler is created to direct log messages to standard error (stderr). This handler is set to DEBUG level, meaning it will process all messages at this level and above. A formatter is then defined to structure the log messages, which will display the log level and the message in the format '[LEVEL] message'. The formatter is applied to the console handler, and the handler is added to the logger. Finally, the configured logger instance is returned for use in other parts of the application.\n\n**Note**: It is important to ensure that the logging configuration does not conflict with other logging setups in the application. The is_verbose parameter allows for easy toggling between detailed and concise logging output.\n\n**Output Example**: When is_verbose is set to True and a log message is generated, the output might appear as:\n```\n[INFO] This is an informational message.\n```\nIf is_verbose is set to False, the output for a warning message would be:\n```\n[WARNING] This is a warning message.\n```"
      ],
      "code_start_line": 68,
      "code_end_line": 77,
      "params": [
        "is_verbose"
      ],
      "have_return": true,
      "code_content": "def set_up_logger(is_verbose: bool = True) -> logging.Logger:\n    \"\"\"Set up a logger that writes to stderr.\"\"\"\n    log = logging.getLogger(__name__)\n    log.setLevel(logging.INFO if is_verbose else logging.WARNING)\n    console = logging.StreamHandler(sys.stderr)  # log to stderr\n    console.setLevel(logging.DEBUG)\n    formatter = logging.Formatter('[%(levelname)s] %(message)s')\n    console.setFormatter(formatter)\n    log.addHandler(console)\n    return log\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "indent",
      "md_content": [
        "**indent**: The function of indent is to add indentation to a given string output.\n\n**parameters**: The parameters of this Function.\n· output: A string that represents the text to which indentation will be added.\n\n**Code Description**: The indent function utilizes the textwrap module's indent method to prepend a specified string (in this case, two spaces) to each line of the provided output string. This function is particularly useful for formatting text output, making it more readable by visually distinguishing it from other text. \n\nThe indent function is called in several other functions within the dataset/verify.py module. For instance, in the files_are_equal function, it is used to format the output of the diff between two files when they are found to be unequal. This enhances the clarity of the log messages by ensuring that the differences are easily identifiable.\n\nSimilarly, in the get_files_from_hosts_and_compare function, the indent function is employed to format the output from the wget command when a file download fails. This ensures that error messages are presented in a structured manner, making it easier for developers to diagnose issues.\n\nIn the check_multisig function, the indent function is used to format the output from the GPG verification process when verbose logging is enabled. This allows users to see the GPG output in a more organized way, improving the readability of the logs.\n\nLastly, in the verify_shasums_signature function, the indent function formats the GPG output when an integrity failure occurs. This consistent use of indentation across various functions helps maintain a uniform logging style throughout the module.\n\n**Note**: When using the indent function, ensure that the output string is properly formatted to avoid unexpected results. The function assumes that the input is a string and does not handle cases where the input may be of a different type.\n\n**Output Example**: If the input to the indent function is:\n```\n\"Line 1\\nLine 2\\nLine 3\"\n```\nThe output will be:\n```\n\"  Line 1\\n  Line 2\\n  Line 3\"\n```"
      ],
      "code_start_line": 83,
      "code_end_line": 84,
      "params": [
        "output"
      ],
      "have_return": true,
      "code_content": "def indent(output: str) -> str:\n    return textwrap.indent(output, '  ')\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "dataset/verify.py/files_are_equal",
        "dataset/verify.py/get_files_from_hosts_and_compare",
        "dataset/verify.py/check_multisig",
        "dataset/verify.py/verify_shasums_signature",
        "dataset/verify.py/verify_published_handler"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "bool_from_env",
      "md_content": [
        "**bool_from_env**: The function of bool_from_env is to retrieve a boolean value from the environment variables based on a specified key.\n\n**parameters**: The parameters of this Function.\n· parameter1: key - A string representing the name of the environment variable to check.\n· parameter2: default - A boolean value that serves as the fallback if the specified key is not found in the environment variables. The default value is set to False.\n\n**Code Description**: The bool_from_env function checks if a specified key exists in the environment variables. If the key is not present, it returns the default value provided as a parameter. If the key is found, it retrieves the corresponding value and converts it to lowercase for comparison. The function recognizes the strings '1' and 'true' as True, while '0' and 'false' are interpreted as False. If the value does not match any of these recognized strings, the function raises a ValueError, indicating that the environment variable contains an unrecognized value.\n\nThis function is utilized within the main function of the dataset/verify.py module. It is called multiple times to set default values for various command-line arguments based on the corresponding environment variables. For instance, the verbosity level, quiet mode, and JSON output options are all determined by the values of environment variables such as 'BINVERIFY_VERBOSE', 'BINVERIFY_QUIET', and 'BINVERIFY_JSON', respectively. This design allows for flexible configuration of the program's behavior through environment variables, enhancing usability and adaptability in different execution contexts.\n\n**Note**: It is important to ensure that the environment variable values are strictly '1', 'true', '0', or 'false' to avoid triggering the ValueError. Users should be aware of the expected formats when setting environment variables to prevent runtime errors.\n\n**Output Example**: If the environment variable 'BINVERIFY_VERBOSE' is set to 'true', the function call bool_from_env('BINVERIFY_VERBOSE') will return True. If the variable is not set, it will return False as the default value."
      ],
      "code_start_line": 87,
      "code_end_line": 96,
      "params": [
        "key",
        "default"
      ],
      "have_return": true,
      "code_content": "def bool_from_env(key, default=False) -> bool:\n    if key not in os.environ:\n        return default\n    raw = os.environ[key]\n\n    if raw.lower() in ('1', 'true'):\n        return True\n    elif raw.lower() in ('0', 'false'):\n        return False\n    raise ValueError(f\"Unrecognized environment value {key}={raw!r}\")\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "dataset/verify.py/main"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "parse_version_string",
      "md_content": [
        "**parse_version_string**: The function of parse_version_string is to parse a version string into its base version, release candidate (if applicable), and operating system/platform information.\n\n**parameters**: The parameters of this Function.\n· version_str: A string representing the version, which may include a release candidate suffix and/or platform information.\n\n**Code Description**: The parse_version_string function takes a version string formatted in a specific way and splits it into three components: the base version, the release candidate (if present), and the operating system or platform information. The function first splits the input string using the hyphen ('-') as a delimiter. The first part of the split string is always considered the base version. Depending on the number of parts obtained from the split, the function determines whether there is a release candidate or platform information. \n\nIf the input string contains two parts, it checks if the second part includes \"rc\" to identify it as a release candidate; otherwise, it is treated as platform information. If there are three parts, the second part is assigned to the release candidate and the third part to the platform information. The function then returns a tuple containing the base version, release candidate, and platform information.\n\nThis function is called within the verify_published_handler function, which is responsible for verifying published binaries based on the provided version string. The parse_version_string function is crucial in this context as it extracts the necessary components from the version string to determine the appropriate remote directory for fetching binaries and signatures. The successful parsing of the version string is essential for the subsequent operations in verify_published_handler, such as constructing the remote directory path and managing the verification process.\n\n**Note**: It is important to ensure that the version string provided to the parse_version_string function adheres to the expected format, as deviations may lead to exceptions being raised during parsing.\n\n**Output Example**: For an input string \"1.0.0-rc1-linux\", the function would return the tuple: (\"1.0.0\", \"rc1\", \"linux\")."
      ],
      "code_start_line": 102,
      "code_end_line": 116,
      "params": [
        "version_str"
      ],
      "have_return": true,
      "code_content": "def parse_version_string(version_str):\n    parts = version_str.split('-')\n    version_base = parts[0]\n    version_rc = \"\"\n    version_os = \"\"\n    if len(parts) == 2:  # \"<version>-rcN\" or \"version-platform\"\n        if \"rc\" in parts[1]:\n            version_rc = parts[1]\n        else:\n            version_os = parts[1]\n    elif len(parts) == 3:  # \"<version>-rcN-platform\"\n        version_rc = parts[1]\n        version_os = parts[2]\n\n    return version_base, version_rc, version_os\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "dataset/verify.py/verify_published_handler"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "download_with_wget",
      "md_content": [
        "**download_with_wget**: The function of download_with_wget is to download a file from a specified remote location and save it to a local file path using the wget command-line utility.\n\n**parameters**: The parameters of this Function.\n· remote_file: A string representing the URL of the file to be downloaded.\n· local_file: A string representing the path where the downloaded file will be saved locally.\n\n**Code Description**: The download_with_wget function utilizes the subprocess module to execute the wget command, which is a widely used utility for downloading files from the web. The function constructs a command that includes the remote file URL and the local file path where the downloaded content should be stored. It runs this command and captures both the standard output and standard error. The function returns a tuple containing a boolean indicating the success of the download operation and the decoded output from the wget command, stripped of any trailing whitespace.\n\nThis function is called by other functions in the project, specifically get_files_from_hosts_and_compare and verify_published_handler. In get_files_from_hosts_and_compare, download_with_wget is used to retrieve files from multiple hosts, ensuring that the files are identical across these sources. The success of the download is critical for the subsequent comparison of file contents. In verify_published_handler, download_with_wget is employed to download binary files after verifying their signatures and checksums, ensuring that the correct files are obtained for further verification processes. The successful execution of download_with_wget is essential for the overall integrity and reliability of the file verification workflow in the project.\n\n**Note**: It is important to ensure that the wget utility is installed and accessible in the environment where this function is executed. Additionally, the remote URL must be valid and reachable to avoid download failures.\n\n**Output Example**: A possible return value of the function could be (True, \"Downloaded 1234 bytes in 0.5 seconds\"), indicating that the download was successful and providing information about the download size and time."
      ],
      "code_start_line": 119,
      "code_end_line": 122,
      "params": [
        "remote_file",
        "local_file"
      ],
      "have_return": true,
      "code_content": "def download_with_wget(remote_file, local_file):\n    result = subprocess.run(['wget', '-O', local_file, remote_file],\n                            stderr=subprocess.STDOUT, stdout=subprocess.PIPE)\n    return result.returncode == 0, result.stdout.decode().rstrip()\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "dataset/verify.py/get_files_from_hosts_and_compare",
        "dataset/verify.py/verify_published_handler"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "download_lines_with_urllib",
      "md_content": [
        "**download_lines_with_urllib**: The function of download_lines_with_urllib is to retrieve text lines from a specified URL over HTTP.\n\n**parameters**: The parameters of this Function.\n· url: A string representing the URL from which to download the text lines.\n\n**Code Description**: The download_lines_with_urllib function attempts to open a specified URL and read its content line by line. It utilizes the urllib library to perform an HTTP request. The function returns a tuple containing a boolean value and a list of strings. If the request is successful, the boolean value is True, and the list contains the stripped lines of text retrieved from the URL. Each line is decoded from bytes to a string format. If an HTTP error occurs, such as a 404 or 500 status code, the function catches the urllib.error.HTTPError exception and logs a warning message indicating the failure. Additionally, if any other exception occurs during the request, it is caught, and a warning is logged as well. In both cases of failure, the function returns a tuple with the boolean value set to False and an empty list.\n\n**Note**: It is important to ensure that the URL provided is valid and accessible. The function handles exceptions gracefully, logging warnings for any errors encountered during the HTTP request.\n\n**Output Example**: A successful call to download_lines_with_urllib(\"http://example.com/file.txt\") might return:\n(True, ['First line of text', 'Second line of text', 'Third line of text']) \n\nConversely, if the URL is invalid or an error occurs, it might return:\n(False, [])"
      ],
      "code_start_line": 125,
      "code_end_line": 134,
      "params": [
        "url"
      ],
      "have_return": true,
      "code_content": "def download_lines_with_urllib(url) -> t.Tuple[bool, t.List[str]]:\n    \"\"\"Get (success, text lines of a file) over HTTP.\"\"\"\n    try:\n        return (True, [\n            line.strip().decode() for line in urllib.request.urlopen(url).readlines()])\n    except urllib.error.HTTPError as e:\n        log.warning(f\"HTTP request to {url} failed (HTTPError): {e}\")\n    except Exception as e:\n        log.warning(f\"HTTP request to {url} failed ({e})\")\n    return (False, [])\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "verify_with_gpg",
      "md_content": [
        "**verify_with_gpg**: The function of verify_with_gpg is to verify the authenticity of a file using GPG signatures.\n\n**parameters**: The parameters of this Function.\n· filename: The path to the file that needs to be verified.\n· signature_filename: The path to the GPG signature file associated with the file being verified.\n· output_filename: An optional parameter specifying the path where the output should be written. If not provided, the output will not be written to a file.\n\n**Code Description**: The verify_with_gpg function utilizes the GPG (GNU Privacy Guard) command-line tool to verify the signature of a specified file. It constructs a command with the necessary arguments to invoke GPG, including options to handle the verification process and specify the output format. The function creates a temporary file to capture the status output from GPG during the verification process.\n\nThe function begins by creating a temporary file using `tempfile.NamedTemporaryFile()` to store the status messages generated by GPG. It then constructs the command-line arguments for the GPG verification, including options to display only the primary UID and to specify the output file for the verification results. The environment variable `LANGUAGE` is set to 'en' to ensure that the output is in English.\n\nThe subprocess module is used to execute the GPG command, capturing both standard output and error output. After the command execution, the status file is read to obtain the GPG output, which is then decoded and stripped of any trailing whitespace.\n\nThe function logs the return code and the output from GPG for debugging purposes and returns a tuple containing the GPG return code and the status output. A return code of '0' typically indicates a successful verification, while other codes indicate various error states.\n\nThis function is called by the check_multisig function, which is responsible for checking the signatures of multiple files. In check_multisig, verify_with_gpg is invoked to validate the signature of a sums file against a provided signature file. The output from verify_with_gpg is then parsed to determine the status of the signatures (good, unknown, or bad). If there are unknown signatures and the user opts to import keys, the function attempts to retrieve the necessary keys before re-verifying the signatures.\n\n**Note**: It is important to ensure that GPG is installed and properly configured on the system where this function is executed. Additionally, the output_filename parameter should be used with caution, as providing an invalid path may lead to errors during execution.\n\n**Output Example**: A possible return value from the function could be (0, \"gpg: Good signature from 'John Doe <john@example.com>'\"), indicating a successful verification with a message confirming the good signature."
      ],
      "code_start_line": 137,
      "code_end_line": 154,
      "params": [
        "filename",
        "signature_filename",
        "output_filename"
      ],
      "have_return": true,
      "code_content": "def verify_with_gpg(\n    filename,\n    signature_filename,\n    output_filename: t.Optional[str] = None\n) -> t.Tuple[int, str]:\n    with tempfile.NamedTemporaryFile() as status_file:\n        args = [\n            'gpg', '--yes', '--verify', '--verify-options', 'show-primary-uid-only', \"--status-file\", status_file.name,\n            '--output', output_filename if output_filename else '', signature_filename, filename]\n\n        env = dict(os.environ, LANGUAGE='en')\n        result = subprocess.run(args, stderr=subprocess.STDOUT, stdout=subprocess.PIPE, env=env)\n\n        gpg_data = status_file.read().decode().rstrip()\n\n    log.debug(f'Result from GPG ({result.returncode}): {result.stdout.decode()}')\n    log.debug(f\"{gpg_data}\")\n    return result.returncode, gpg_data\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "dataset/verify.py/check_multisig"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "remove_files",
      "md_content": [
        "**remove_files**: The function of remove_files is to delete files specified in a list of filenames.\n\n**parameters**: The parameters of this Function.\n· parameter1: filenames - A list of strings, where each string represents the name of a file to be removed from the filesystem.\n\n**Code Description**: The remove_files function iterates over a list of filenames provided as an argument. For each filename in the list, it calls the os.remove() function to delete the corresponding file from the filesystem. This function assumes that the filenames provided are valid and that the files exist; otherwise, an exception will be raised if a file cannot be found or accessed.\n\nThe function does not return any value. It performs the operation of file deletion directly. It is important to ensure that the list of filenames does not contain any unintended files, as this operation is irreversible and will permanently remove the specified files from the system.\n\n**Note**: Points to note about the use of the code\n- Ensure that the filenames provided are correct and that the files exist to avoid exceptions.\n- Consider implementing error handling to manage cases where a file cannot be deleted, such as using try-except blocks around the os.remove() call.\n- Be cautious when using this function, as it will permanently delete files without any confirmation or recovery option."
      ],
      "code_start_line": 157,
      "code_end_line": 159,
      "params": [
        "filenames"
      ],
      "have_return": false,
      "code_content": "def remove_files(filenames):\n    for filename in filenames:\n        os.remove(filename)\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "ClassDef",
      "name": "SigData",
      "md_content": [
        "**SigData**: The function of SigData is to represent GPG signature data parsed from GPG stdout.\n\n**attributes**: The attributes of this Class.\n· key: Represents the unique identifier of the GPG key associated with the signature. It can be None if not set.\n· name: A string that holds the name of the entity associated with the signature.\n· trusted: A boolean indicating whether the signature is trusted or not.\n· status: A string that describes the status of the signature, such as \"expired\" or \"revoked\".\n\n**Code Description**: The SigData class is designed to encapsulate the details of a GPG signature, including the key, name, trust status, and any relevant status messages. The constructor initializes the attributes to default values, with `key` set to None, `name` as an empty string, `trusted` as False, and `status` as an empty string. \n\nThe class includes a `__bool__` method that allows instances of SigData to be evaluated in a boolean context. This method returns True if the `key` attribute is not None, indicating that the signature data is valid. The `__repr__` method provides a string representation of the SigData instance, which includes the values of its attributes, formatted for clarity.\n\nThe SigData class is utilized within the context of signature verification processes in the project. It is primarily called by functions such as `parse_gpg_result`, `check_multisig`, and `verify_shasums_signature`. The `parse_gpg_result` function processes the output from GPG, creating instances of SigData for good, unknown, and bad signatures. These instances are then returned as lists to the calling functions, which further handle the verification logic, including checking the trustworthiness of the signatures and logging the results.\n\n**Note**: When using the SigData class, it is important to ensure that the `key` attribute is set appropriately to reflect the actual GPG key being represented. The trust status and signature status should also be updated based on the results of the GPG verification process.\n\n**Output Example**: An instance of SigData might be represented as follows:\nSigData('A1B2C3D4', 'John Doe', trusted=True, status='')"
      ],
      "code_start_line": 162,
      "code_end_line": 176,
      "params": [],
      "have_return": true,
      "code_content": "class SigData:\n    \"\"\"GPG signature data as parsed from GPG stdout.\"\"\"\n    def __init__(self):\n        self.key = None\n        self.name = \"\"\n        self.trusted = False\n        self.status = \"\"\n\n    def __bool__(self):\n        return self.key is not None\n\n    def __repr__(self):\n        return (\n            \"SigData(%r, %r, trusted=%s, status=%r)\" %\n            (self.key, self.name, self.trusted, self.status))\n",
      "name_column": 6,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "dataset/verify.py/parse_gpg_result",
        "dataset/verify.py/check_multisig",
        "dataset/verify.py/verify_shasums_signature"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [
        "**__init__**: The function of __init__ is to initialize an instance of the SigData class with default attribute values.\n\n**parameters**: The parameters of this Function.\n· There are no parameters for this function.\n\n**Code Description**: The __init__ function is a constructor method that is automatically called when an instance of the SigData class is created. This function initializes four attributes of the class: \n- `self.key`: This attribute is set to None, indicating that it does not hold any value upon initialization. It is likely intended to store a key value that may be assigned later in the object's lifecycle.\n- `self.name`: This attribute is initialized as an empty string. It is intended to hold the name associated with the SigData instance, which can be populated with a meaningful string later.\n- `self.trusted`: This attribute is set to False, indicating that the instance is not trusted by default. This boolean value may be used to determine the trustworthiness of the data represented by the instance.\n- `self.status`: This attribute is also initialized as an empty string. It is meant to represent the current status of the SigData instance, which can be updated as needed.\n\nOverall, the __init__ function establishes a clean state for the SigData object, ensuring that all attributes are defined and ready for use.\n\n**Note**: It is important to remember that this constructor does not take any parameters, meaning that any specific values for the attributes must be set after the object is instantiated. Users of this class should ensure that they assign appropriate values to these attributes to avoid operating with default states that may not be suitable for their use case."
      ],
      "code_start_line": 164,
      "code_end_line": 168,
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def __init__(self):\n        self.key = None\n        self.name = \"\"\n        self.trusted = False\n        self.status = \"\"\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "__bool__",
      "md_content": [
        "**__bool__**: The function of __bool__ is to determine the truthiness of an instance of the SigData class.\n\n**parameters**: The parameters of this Function.\n· There are no parameters for this function.\n\n**Code Description**: The __bool__ function is a special method in Python that is used to define the truth value of an object. In this implementation, the function checks if the attribute `key` of the instance is not `None`. If `self.key` is not `None`, the function returns `True`, indicating that the instance is considered \"truthy\". Conversely, if `self.key` is `None`, the function returns `False`, indicating that the instance is considered \"falsy\". This behavior allows instances of the SigData class to be used in conditional statements and boolean contexts, providing a clear and intuitive way to evaluate the state of the object based on the presence of the `key` attribute.\n\n**Note**: It is important to ensure that the `key` attribute is properly initialized before using this method, as its value directly affects the truthiness of the instance. If `key` is expected to be `None` at certain times, the behavior of the instance in boolean contexts should be understood accordingly.\n\n**Output Example**: \n- If an instance of SigData has `key` set to a valid value (e.g., `key = \"some_value\"`), calling `bool(instance)` will return `True`.\n- If `key` is set to `None` (e.g., `key = None`), calling `bool(instance)` will return `False`."
      ],
      "code_start_line": 170,
      "code_end_line": 171,
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def __bool__(self):\n        return self.key is not None\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "__repr__",
      "md_content": [
        "**__repr__**: The function of __repr__ is to provide a string representation of the SigData object.\n\n**parameters**: The parameters of this Function.\n· There are no parameters for this function.\n\n**Code Description**: The __repr__ method is a special method in Python that is used to define a string representation for an instance of a class. In this implementation, the method returns a formatted string that includes the key attributes of the SigData object. Specifically, it returns a string that contains the class name \"SigData\" followed by the values of the object's attributes: `key`, `name`, `trusted`, and `status`. The attributes are formatted using the `%r` format specifier, which calls the `repr()` function on the attributes, ensuring that they are represented in a way that is unambiguous and suitable for debugging. The `trusted` attribute is formatted as a boolean value, while the others are represented in their respective formats.\n\n**Note**: It is important to ensure that the attributes `key`, `name`, `trusted`, and `status` are defined within the SigData class for this method to function correctly. This method is particularly useful for debugging and logging, as it provides a clear and concise representation of the object.\n\n**Output Example**: An example of the output from this method could be:\n\"SigData('12345', 'Sample Data', trusted=True, status='active')\""
      ],
      "code_start_line": 173,
      "code_end_line": 176,
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def __repr__(self):\n        return (\n            \"SigData(%r, %r, trusted=%s, status=%r)\" %\n            (self.key, self.name, self.trusted, self.status))\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "parse_gpg_result",
      "md_content": [
        "**parse_gpg_result**: The function of parse_gpg_result is to parse the output from GPG and return categorized lists of good, unknown, and bad signatures.\n\n**parameters**: The parameters of this Function.\n· output: A list of strings representing the lines of output from GPG.\n\n**Code Description**: The parse_gpg_result function processes the output from GPG, which is expected to contain information about various signatures associated with a file. It categorizes these signatures into three distinct lists: good signatures, unknown signatures, and bad signatures. The function begins by initializing three lists to hold instances of the SigData class, which encapsulates the details of each signature.\n\nThe function defines a nested helper function, line_begins_with, which checks if a given line starts with a specific pattern. This is crucial for ensuring that the parser only processes lines that are relevant and prevents malicious input from affecting the parsing logic.\n\nAs the function iterates through each line of the GPG output, it uses regular expressions to identify and categorize the signatures based on specific prefixes such as \"NEWSIG\", \"GOODSIG\", \"BADSIG\", and others. For each identified signature, it populates the attributes of a SigData instance, including the key, name, trust status, and any relevant status messages (e.g., \"expired\" or \"revoked\"). \n\nAt the end of the parsing process, the function checks that the total number of resolved signatures matches the number of signatures found in the output. If there is a discrepancy, it raises a RuntimeError to alert the caller of the issue.\n\nThe parse_gpg_result function is called by the check_multisig function, which is responsible for verifying signatures against a given file. After obtaining the output from GPG, check_multisig calls parse_gpg_result to categorize the signatures, allowing it to handle unknown signatures appropriately, such as prompting the user to retrieve missing keys.\n\n**Note**: When using parse_gpg_result, it is essential to ensure that the output provided is correctly formatted as expected by the function. Any deviations in the output format may lead to incorrect parsing or runtime errors.\n\n**Output Example**: The return value of parse_gpg_result could look like this:\n(\n    [SigData('A1B2C3D4', 'John Doe', trusted=True, status='')],\n    [SigData('E5F6G7H8', 'Unknown Entity', trusted=False, status='')],\n    [SigData('I9J0K1L2', 'Malicious Entity', trusted=False, status='revoked')]\n)"
      ],
      "code_start_line": 179,
      "code_end_line": 244,
      "params": [
        "output"
      ],
      "have_return": true,
      "code_content": "def parse_gpg_result(\n    output: t.List[str]\n) -> t.Tuple[t.List[SigData], t.List[SigData], t.List[SigData]]:\n    \"\"\"Returns good, unknown, and bad signatures from GPG stdout.\"\"\"\n    good_sigs: t.List[SigData] = []\n    unknown_sigs: t.List[SigData] = []\n    bad_sigs: t.List[SigData] = []\n    total_resolved_sigs = 0\n\n    # Ensure that all lines we match on include a prefix that prevents malicious input\n    # from fooling the parser.\n    def line_begins_with(patt: str, line: str) -> t.Optional[re.Match]:\n        return re.match(r'^(\\[GNUPG:\\])\\s+' + patt, line)\n\n    curr_sigs = unknown_sigs\n    curr_sigdata = SigData()\n\n    for line in output:\n        if line_begins_with(r\"NEWSIG(?:\\s|$)\", line):\n            total_resolved_sigs += 1\n            if curr_sigdata:\n                curr_sigs.append(curr_sigdata)\n                curr_sigdata = SigData()\n            newsig_split = line.split()\n            if len(newsig_split) == 3:\n                curr_sigdata.name = newsig_split[2]\n\n        elif line_begins_with(r\"GOODSIG(?:\\s|$)\", line):\n            curr_sigdata.key, curr_sigdata.name = line.split(maxsplit=3)[2:4]\n            curr_sigs = good_sigs\n\n        elif line_begins_with(r\"EXPKEYSIG(?:\\s|$)\", line):\n            curr_sigdata.key, curr_sigdata.name = line.split(maxsplit=3)[2:4]\n            curr_sigs = good_sigs\n            curr_sigdata.status = \"expired\"\n\n        elif line_begins_with(r\"REVKEYSIG(?:\\s|$)\", line):\n            curr_sigdata.key, curr_sigdata.name = line.split(maxsplit=3)[2:4]\n            curr_sigs = good_sigs\n            curr_sigdata.status = \"revoked\"\n\n        elif line_begins_with(r\"BADSIG(?:\\s|$)\", line):\n            curr_sigdata.key, curr_sigdata.name = line.split(maxsplit=3)[2:4]\n            curr_sigs = bad_sigs\n\n        elif line_begins_with(r\"ERRSIG(?:\\s|$)\", line):\n            curr_sigdata.key, _, _, _, _, _ = line.split()[2:8]\n            curr_sigs = unknown_sigs\n\n        elif line_begins_with(r\"TRUST_(UNDEFINED|NEVER)(?:\\s|$)\", line):\n            curr_sigdata.trusted = False\n\n        elif line_begins_with(r\"TRUST_(MARGINAL|FULLY|ULTIMATE)(?:\\s|$)\", line):\n            curr_sigdata.trusted = True\n\n    # The last one won't have been added, so add it now\n    assert curr_sigdata\n    curr_sigs.append(curr_sigdata)\n\n    all_found = len(good_sigs + bad_sigs + unknown_sigs)\n    if all_found != total_resolved_sigs:\n        raise RuntimeError(\n            f\"failed to evaluate all signatures: found {all_found} \"\n            f\"but expected {total_resolved_sigs}\")\n\n    return (good_sigs, unknown_sigs, bad_sigs)\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "dataset/verify.py/check_multisig"
      ],
      "reference_who": [
        "dataset/verify.py/SigData"
      ],
      "special_reference_type": [
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "line_begins_with",
      "md_content": [
        "**line_begins_with**: The function of line_begins_with is to determine if a given line starts with a specific pattern prefixed by the string \"[GNUPG:]\".\n\n**parameters**: The parameters of this Function.\n· parameter1: patt - A string representing the pattern that the line should match after the \"[GNUPG:]\" prefix.  \n· parameter2: line - A string that represents the line to be checked against the specified pattern.\n\n**Code Description**: The line_begins_with function utilizes the re.match method from the regular expression module (re) to check if the input line begins with the specified pattern. The function constructs a regular expression that looks for the exact string \"[GNUPG:]\" followed by one or more whitespace characters and then the provided pattern (patt). The caret (^) in the regular expression signifies that the match must occur at the start of the line. If the line matches this constructed pattern, re.match returns a match object; otherwise, it returns None. This function is particularly useful for parsing lines of text that are expected to follow a specific format, such as log entries from GnuPG.\n\n**Note**: It is important to ensure that the pattern provided in the patt parameter is a valid regular expression. Additionally, the function only checks for matches at the beginning of the line, so any content before \"[GNUPG:]\" will result in no match.\n\n**Output Example**: If the line is \"[GNUPG:] key 12345\" and the pattern is \"key\", the function will return a match object. If the line is \"key 12345\" (without the \"[GNUPG:]\" prefix), the function will return None."
      ],
      "code_start_line": 190,
      "code_end_line": 191,
      "params": [
        "patt",
        "line"
      ],
      "have_return": true,
      "code_content": "    def line_begins_with(patt: str, line: str) -> t.Optional[re.Match]:\n        return re.match(r'^(\\[GNUPG:\\])\\s+' + patt, line)\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "files_are_equal",
      "md_content": [
        "**files_are_equal**: The function of files_are_equal is to compare the contents of two files and determine if they are identical.\n\n**parameters**: The parameters of this Function.\n· filename1: A string representing the path to the first file to be compared.\n· filename2: A string representing the path to the second file to be compared.\n\n**Code Description**: The files_are_equal function opens two files in binary mode and reads their contents to compare them for equality. If the contents are identical, the function returns True. If the contents differ, it proceeds to open both files in text mode, reads their lines, and generates a unified diff of the differences using the difflib module. This diff is then indented for better readability and logged as a warning message, indicating the files that were found to be different. The function ultimately returns False in this case.\n\nThis function is called by the get_files_from_hosts_and_compare function, which is responsible for retrieving files from multiple hosts and ensuring that they are identical. After downloading files from the specified hosts, get_files_from_hosts_and_compare invokes files_are_equal to compare each downloaded file with the next one in the list. If any pair of files is found to be unequal, an error is logged, and the function returns a specific return code indicating that the files are not equal.\n\nThe use of files_are_equal is crucial in maintaining data integrity when files are fetched from different sources. By ensuring that the contents of the files match, it helps prevent issues that may arise from discrepancies in the data.\n\n**Note**: When using the files_are_equal function, it is important to ensure that the file paths provided are correct and that the files are accessible. The function assumes that the files can be opened without any permission issues. Additionally, the function handles files in both binary and text modes, which is essential for accurately comparing different types of files.\n\n**Output Example**: If the two files being compared are identical, the function will return:\n```\nTrue\n```\nIf the files differ, the function will log a warning and return:\n```\nFalse\n```"
      ],
      "code_start_line": 247,
      "code_end_line": 264,
      "params": [
        "filename1",
        "filename2"
      ],
      "have_return": true,
      "code_content": "def files_are_equal(filename1, filename2):\n    with open(filename1, 'rb') as file1:\n        contents1 = file1.read()\n    with open(filename2, 'rb') as file2:\n        contents2 = file2.read()\n    eq = contents1 == contents2\n\n    if not eq:\n        with open(filename1, 'r', encoding='utf-8') as f1, \\\n                open(filename2, 'r', encoding='utf-8') as f2:\n            f1lines = f1.readlines()\n            f2lines = f2.readlines()\n\n            diff = indent(\n                ''.join(difflib.unified_diff(f1lines, f2lines)))\n            log.warning(f\"found diff in files ({filename1}, {filename2}):\\n{diff}\\n\")\n\n    return eq\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "dataset/verify.py/get_files_from_hosts_and_compare"
      ],
      "reference_who": [
        "dataset/verify.py/indent"
      ],
      "special_reference_type": [
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "get_files_from_hosts_and_compare",
      "md_content": [
        "**get_files_from_hosts_and_compare**: The function of get_files_from_hosts_and_compare is to retrieve the same file from multiple hosts and ensure that they have identical contents.\n\n**parameters**: The parameters of this Function.\n· hosts: A list of strings representing the host addresses from which the file will be downloaded. The first host is treated as the primary host.\n· path: A string representing the path to the file on the remote hosts.\n· filename: A string representing the name of the file to be saved locally.\n· require_all: A boolean indicating whether all hosts must successfully provide the file for the operation to be considered successful. Defaults to False.\n\n**Code Description**: The get_files_from_hosts_and_compare function is designed to facilitate the retrieval of a specified file from multiple remote hosts and to verify that the contents of the downloaded files are identical. The function begins by asserting that more than one host is provided, as it requires at least one primary host and one or more additional hosts for comparison.\n\nThe primary host is defined as the first entry in the hosts list, and the function constructs a URL for the file to be downloaded from this host. It utilizes the download_with_wget function to attempt to download the file. If the download from the primary host fails, an error is logged, and the function returns a specific ReturnCode indicating the failure.\n\nSubsequently, the function iterates over the remaining hosts, downloading the same file from each. If the require_all parameter is set to True and any of the additional hosts fail to provide the file, the function logs an error and returns a ReturnCode indicating that a file is missing from one host. If a host fails to provide the file but require_all is False, a warning is logged, and the function continues with the files obtained from the primary host.\n\nOnce all files have been downloaded, the function compares the contents of the downloaded files using the files_are_equal function. If any pair of files is found to be different, an error is logged, and the function returns a ReturnCode indicating that the files are not equal. If all files are confirmed to be identical, the function returns a ReturnCode indicating success.\n\nThis function is called by the verify_published_handler function, which is responsible for orchestrating the verification of published binaries. In this context, get_files_from_hosts_and_compare is used to retrieve and compare signature files and checksum files from multiple hosts, ensuring that the verification process is based on consistent and reliable data.\n\n**Note**: When using the get_files_from_hosts_and_compare function, it is essential to ensure that the provided hosts are valid and reachable. Additionally, the path and filename parameters must be correctly specified to avoid download failures. The function assumes that the files can be accessed without permission issues on the remote hosts.\n\n**Output Example**: A possible return value of the function could be ReturnCode.SUCCESS, indicating that all files were successfully retrieved and verified as identical. If there was a failure in downloading from the primary host, the return value could be ReturnCode.FILE_GET_FAILED."
      ],
      "code_start_line": 267,
      "code_end_line": 326,
      "params": [
        "hosts",
        "path",
        "filename",
        "require_all"
      ],
      "have_return": true,
      "code_content": "def get_files_from_hosts_and_compare(\n    hosts: t.List[str], path: str, filename: str, require_all: bool = False\n) -> ReturnCode:\n    \"\"\"\n    Retrieve the same file from a number of hosts and ensure they have the same contents.\n    The first host given will be treated as the \"primary\" host, and is required to succeed.\n\n    Args:\n        filename: for writing the file locally.\n    \"\"\"\n    assert len(hosts) > 1\n    primary_host = hosts[0]\n    other_hosts = hosts[1:]\n    got_files = []\n\n    def join_url(host: str) -> str:\n        return host.rstrip('/') + '/' + path.lstrip('/')\n\n    url = join_url(primary_host)\n    success, output = download_with_wget(url, filename)\n    if not success:\n        log.error(\n            f\"couldn't fetch file ({url}). \"\n            \"Have you specified the version number in the following format?\\n\"\n            f\"{VERSION_FORMAT} \"\n            f\"(example: {VERSION_EXAMPLE})\\n\"\n            f\"wget output:\\n{indent(output)}\")\n        return ReturnCode.FILE_GET_FAILED\n    else:\n        log.info(f\"got file {url} as {filename}\")\n        got_files.append(filename)\n\n    for i, host in enumerate(other_hosts):\n        url = join_url(host)\n        fname = filename + f'.{i + 2}'\n        success, output = download_with_wget(url, fname)\n\n        if require_all and not success:\n            log.error(\n                f\"{host} failed to provide file ({url}), but {primary_host} did?\\n\"\n                f\"wget output:\\n{indent(output)}\")\n            return ReturnCode.FILE_MISSING_FROM_ONE_HOST\n        elif not success:\n            log.warning(\n                f\"{host} failed to provide file ({url}). \"\n                f\"Continuing based solely upon {primary_host}.\")\n        else:\n            log.info(f\"got file {url} as {fname}\")\n            got_files.append(fname)\n\n    for i, got_file in enumerate(got_files):\n        if got_file == got_files[-1]:\n            break  # break on last file, nothing after it to compare to\n\n        compare_to = got_files[i + 1]\n        if not files_are_equal(got_file, compare_to):\n            log.error(f\"files not equal: {got_file} and {compare_to}\")\n            return ReturnCode.FILES_NOT_EQUAL\n\n    return ReturnCode.SUCCESS\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "dataset/verify.py/verify_published_handler"
      ],
      "reference_who": [
        "dataset/verify.py/ReturnCode",
        "dataset/verify.py/indent",
        "dataset/verify.py/download_with_wget",
        "dataset/verify.py/files_are_equal"
      ],
      "special_reference_type": [
        false,
        false,
        false,
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "join_url",
      "md_content": [
        "**join_url**: The function of join_url is to concatenate a host URL with a specified path, ensuring proper formatting by removing any trailing slashes from the host and leading slashes from the path.\n\n**parameters**: The parameters of this Function.\n· host: A string representing the base URL or host to which the path will be appended.\n\n**Code Description**: The join_url function takes a single parameter, host, which is expected to be a string. The function performs two main operations to ensure that the resulting URL is correctly formatted. First, it uses the rstrip('/') method on the host string to remove any trailing slashes. This is important because having a trailing slash can lead to double slashes when concatenating with the path. Next, it concatenates the cleaned host with a forward slash ('/') and the path, which is processed using lstrip('/') to remove any leading slashes. This ensures that the path does not start with a slash, preventing the formation of an incorrect URL structure. The final result is a well-formed URL that combines the host and the path.\n\n**Note**: It is essential to ensure that the host parameter is a valid URL format. Additionally, the path variable should be defined in the scope where the join_url function is called, as it is not passed as a parameter. Users should be cautious about the input values to avoid unexpected URL formats.\n\n**Output Example**: If the host is \"http://example.com/\" and the path is \"/api/v1/resource\", the function will return \"http://example.com/api/v1/resource\"."
      ],
      "code_start_line": 282,
      "code_end_line": 283,
      "params": [
        "host"
      ],
      "have_return": true,
      "code_content": "    def join_url(host: str) -> str:\n        return host.rstrip('/') + '/' + path.lstrip('/')\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "check_multisig",
      "md_content": [
        "**check_multisig**: The function of check_multisig is to verify the signatures of a specified sums file against a provided GPG signature file and return the results of the verification process.\n\n**parameters**: The parameters of this Function.\n· sums_file: A string representing the path to the file containing the checksums that need to be verified.\n· sigfilename: A string representing the path to the GPG signature file associated with the sums file.\n· args: An instance of argparse.Namespace containing command-line arguments that influence the behavior of the function.\n\n**Code Description**: The check_multisig function is responsible for verifying the authenticity of signatures associated with a sums file using GPG (GNU Privacy Guard). The function begins by calling the verify_with_gpg function, which executes the GPG command to check the signatures and returns a status code along with the output from GPG. The output is then processed to categorize the signatures into three lists: good, unknown, and bad, using the parse_gpg_result function.\n\nIf there are any unknown signatures and the user has specified the option to import keys, the function prompts the user for confirmation to retrieve the unknown keys from a keyserver. If the user agrees, it attempts to retrieve the keys using a subprocess call to GPG. After retrieving the keys, the function re-verifies the signatures by calling verify_with_gpg again and updates the lists of good, unknown, and bad signatures.\n\nThe function returns a tuple containing the GPG return code, the output from GPG, and the categorized lists of good, unknown, and bad signatures. This structured output allows the calling function, such as verify_shasums_signature, to make informed decisions based on the verification results. In verify_shasums_signature, the results from check_multisig are used to determine if there are enough trusted signatures to meet a specified threshold, logging appropriate messages based on the verification outcome.\n\n**Note**: It is essential to ensure that the GPG environment is properly set up and that the necessary keys are accessible for the verification process to succeed. Additionally, the function assumes that the input files are correctly formatted and accessible.\n\n**Output Example**: A possible return value from the function could be:\n(0, \"gpg: Good signature from 'John Doe <john@example.com>'\", \n [SigData('A1B2C3D4', 'John Doe', trusted=True, status='')], \n [SigData('E5F6G7H8', 'Unknown Entity', trusted=False, status='')], \n [SigData('I9J0K1L2', 'Malicious Entity', trusted=False, status='revoked')])"
      ],
      "code_start_line": 329,
      "code_end_line": 356,
      "params": [
        "sums_file",
        "sigfilename",
        "args"
      ],
      "have_return": true,
      "code_content": "def check_multisig(sums_file: str, sigfilename: str, args: argparse.Namespace) -> t.Tuple[int, str, t.List[SigData], t.List[SigData], t.List[SigData]]:\n    # check signature\n    #\n    # We don't write output to a file because this command will almost certainly\n    # fail with GPG exit code '2' (and so not writing to --output) because of the\n    # likely presence of multiple untrusted signatures.\n    retval, output = verify_with_gpg(sums_file, sigfilename)\n\n    if args.verbose:\n        log.info(f\"gpg output:\\n{indent(output)}\")\n\n    good, unknown, bad = parse_gpg_result(output.splitlines())\n\n    if unknown and args.import_keys:\n        # Retrieve unknown keys and then try GPG again.\n        for unsig in unknown:\n            if prompt_yn(f\" ? Retrieve key {unsig.key} ({unsig.name})? (y/N) \"):\n                ran = subprocess.run(\n                    [\"gpg\", \"--keyserver\", args.keyserver, \"--recv-keys\", unsig.key])\n\n                if ran.returncode != 0:\n                    log.warning(f\"failed to retrieve key {unsig.key}\")\n\n        # Reparse the GPG output now that we have more keys\n        retval, output = verify_with_gpg(sums_file, sigfilename)\n        good, unknown, bad = parse_gpg_result(output.splitlines())\n\n    return retval, output, good, unknown, bad\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "dataset/verify.py/verify_shasums_signature"
      ],
      "reference_who": [
        "dataset/verify.py/indent",
        "dataset/verify.py/verify_with_gpg",
        "dataset/verify.py/SigData",
        "dataset/verify.py/parse_gpg_result",
        "dataset/verify.py/prompt_yn"
      ],
      "special_reference_type": [
        false,
        false,
        true,
        false,
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "prompt_yn",
      "md_content": [
        "**prompt_yn**: The function of prompt_yn is to prompt the user for a yes or no input and return a boolean value based on the user's response.\n\n**parameters**: The parameters of this Function.\n· prompt: A string that represents the message displayed to the user when asking for input.\n\n**Code Description**: The prompt_yn function is designed to solicit a yes ('y') or no ('n') response from the user. It begins by initializing a variable `got` to an empty string. The function then enters a while loop that continues until the user provides a valid input, which must be either 'y' or 'n'. Inside the loop, the function calls the input function with the provided prompt, converting the user's response to lowercase to ensure case insensitivity. Once a valid response is received, the function returns True if the response is 'y' and False if it is 'n'.\n\nThis function is utilized within the check_multisig function, which is responsible for verifying signatures associated with a file. Specifically, prompt_yn is called when the program encounters an unknown key during the verification process and the user has opted to import keys. The function prompts the user to confirm whether they wish to retrieve the unknown key from a keyserver. The boolean result from prompt_yn determines whether the program will attempt to retrieve the key using a subprocess call to GPG. This interaction illustrates how prompt_yn facilitates user decision-making in the signature verification workflow.\n\n**Note**: It is important to ensure that the prompt provided to the user is clear and concise, as this will directly affect the user's ability to respond correctly. Additionally, the function only accepts 'y' or 'n' as valid inputs, and any other input will result in the prompt being displayed again.\n\n**Output Example**: If the user inputs 'y', the function will return True. If the user inputs 'n', the function will return False. For any other input, the prompt will be displayed again until a valid response is received."
      ],
      "code_start_line": 359,
      "code_end_line": 364,
      "params": [
        "prompt"
      ],
      "have_return": true,
      "code_content": "def prompt_yn(prompt) -> bool:\n    \"\"\"Return true if the user inputs 'y'.\"\"\"\n    got = ''\n    while got not in ['y', 'n']:\n        got = input(prompt).lower()\n    return got == 'y'\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "dataset/verify.py/check_multisig"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "verify_shasums_signature",
      "md_content": [
        "**verify_shasums_signature**: The function of verify_shasums_signature is to verify the signatures of a SHA256SUMS file against a provided GPG signature file and ensure that the number of trusted signatures meets a specified threshold.\n\n**parameters**: The parameters of this Function.\n· signature_file_path: A string representing the path to the GPG signature file associated with the SHA256SUMS file.  \n· sums_file_path: A string representing the path to the file containing the checksums that need to be verified.  \n· args: An instance of argparse.Namespace containing command-line arguments that influence the behavior of the function, including the minimum number of good signatures required for verification.\n\n**Code Description**: The verify_shasums_signature function is responsible for validating the integrity of a SHA256SUMS file by checking its signatures using GPG (GNU Privacy Guard). It begins by defining the minimum number of good signatures required and the allowed GPG return codes for successful verification. The function then calls check_multisig, which executes the GPG command to verify the signatures and returns a status code along with categorized lists of good, unknown, and bad signatures.\n\nUpon receiving the results from check_multisig, the function checks the GPG return code against the allowed codes. If the return code indicates an integrity failure or an unexpected error, it logs the appropriate error messages and returns a corresponding ReturnCode indicating the failure.\n\nThe function then assesses which signatures are trusted based on the command-line arguments provided. It tallies the good signatures, distinguishing between trusted and untrusted signatures. If the number of trusted signatures is below the specified threshold, it logs a warning and returns a ReturnCode indicating that there are not enough good signatures.\n\nFor each signature, the function logs its status, including good signatures, expired keys, bad signatures, and unknown signatures. Finally, it returns a tuple containing the overall status of the verification process, along with lists of trusted good signatures, untrusted good signatures, unknown signatures, and bad signatures.\n\nThis function is called by other functions such as verify_published_handler and verify_binaries_handler, which handle the overall verification process for published binaries and specific binary files, respectively. These calling functions rely on verify_shasums_signature to ensure that the SHA256SUMS file is properly verified before proceeding with further operations, such as downloading binaries or verifying their hashes.\n\n**Note**: When using the verify_shasums_signature function, it is essential to ensure that the GPG environment is correctly configured and that the necessary keys are available for the verification process to succeed. Additionally, the function assumes that the input files are correctly formatted and accessible.\n\n**Output Example**: A possible return value from the function could be:\n(ReturnCode.SUCCESS, \n [SigData('A1B2C3D4', 'John Doe', trusted=True, status='')], \n [SigData('E5F6G7H8', 'Unknown Entity', trusted=False, status='')], \n [SigData('I9J0K1L2', 'Malicious Entity', trusted=False, status='revoked')])"
      ],
      "code_start_line": 366,
      "code_end_line": 429,
      "params": [
        "signature_file_path",
        "sums_file_path",
        "args"
      ],
      "have_return": true,
      "code_content": "def verify_shasums_signature(\n    signature_file_path: str, sums_file_path: str, args: argparse.Namespace\n) -> t.Tuple[\n   ReturnCode, t.List[SigData], t.List[SigData], t.List[SigData], t.List[SigData]\n]:\n    min_good_sigs = args.min_good_sigs\n    gpg_allowed_codes = [0, 2]  # 2 is returned when untrusted signatures are present.\n\n    gpg_retval, gpg_output, good, unknown, bad = check_multisig(sums_file_path, signature_file_path, args)\n\n    if gpg_retval not in gpg_allowed_codes:\n        if gpg_retval == 1:\n            log.critical(f\"Bad signature (code: {gpg_retval}).\")\n        else:\n            log.critical(f\"unexpected GPG exit code ({gpg_retval})\")\n\n        log.error(f\"gpg output:\\n{indent(gpg_output)}\")\n        return (ReturnCode.INTEGRITY_FAILURE, [], [], [], [])\n\n    # Decide which keys we trust, though not \"trust\" in the GPG sense, but rather\n    # which pubkeys convince us that this sums file is legitimate. In other words,\n    # which pubkeys within the Bitcoin community do we trust for the purposes of\n    # binary verification?\n    trusted_keys = set()\n    if args.trusted_keys:\n        trusted_keys |= set(args.trusted_keys.split(','))\n\n    # Tally signatures and make sure we have enough goods to fulfill\n    # our threshold.\n    good_trusted = [sig for sig in good if sig.trusted or sig.key in trusted_keys]\n    good_untrusted = [sig for sig in good if sig not in good_trusted]\n    num_trusted = len(good_trusted) + len(good_untrusted)\n    log.info(f\"got {num_trusted} good signatures\")\n\n    if num_trusted < min_good_sigs:\n        log.info(\"Maybe you need to import \"\n                  f\"(`gpg --keyserver {args.keyserver} --recv-keys <key-id>`) \"\n                  \"some of the following keys: \")\n        log.info('')\n        for sig in unknown:\n            log.info(f\"    {sig.key} ({sig.name})\")\n        log.info('')\n        log.error(\n            \"not enough trusted sigs to meet threshold \"\n            f\"({num_trusted} vs. {min_good_sigs})\")\n\n        return (ReturnCode.NOT_ENOUGH_GOOD_SIGS, [], [], [], [])\n\n    for sig in good_trusted:\n        log.info(f\"GOOD SIGNATURE: {sig}\")\n\n    for sig in good_untrusted:\n        log.info(f\"GOOD SIGNATURE (untrusted): {sig}\")\n\n    for sig in [sig for sig in good if sig.status == 'expired']:\n        log.warning(f\"key {sig.key} for {sig.name} is expired\")\n\n    for sig in bad:\n        log.warning(f\"BAD SIGNATURE: {sig}\")\n\n    for sig in unknown:\n        log.warning(f\"UNKNOWN SIGNATURE: {sig}\")\n\n    return (ReturnCode.SUCCESS, good_trusted, good_untrusted, unknown, bad)\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "dataset/verify.py/verify_published_handler",
        "dataset/verify.py/verify_binaries_handler"
      ],
      "reference_who": [
        "dataset/verify.py/ReturnCode",
        "dataset/verify.py/indent",
        "dataset/verify.py/SigData",
        "dataset/verify.py/check_multisig"
      ],
      "special_reference_type": [
        false,
        false,
        false,
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "parse_sums_file",
      "md_content": [
        "**parse_sums_file**: The function of parse_sums_file is to extract hashes and filenames of binaries to verify from a specified hash file.\n\n**parameters**: The parameters of this Function.\n· sums_file_path: A string representing the path to the hash file that contains the hashes and binary filenames.\n· filename_filter: A list of strings used to filter the filenames extracted from the hash file. If empty, all entries will be returned.\n\n**Code Description**: The parse_sums_file function reads a hash file specified by the sums_file_path parameter. Each line in this file is expected to contain a hash followed by a binary filename, formatted as \"<hash> <binary_filename>\". The function processes the file line by line, splitting each line into its constituent parts. It checks if the filename_filter is empty or if any of the filters are present in the line. If either condition is met, the hash and filename are included in the returned list. The function ultimately returns a list of lists, where each inner list contains a hash and its corresponding binary filename.\n\nThis function is called by two different handlers within the dataset/verify.py module: verify_published_handler and verify_binaries_handler. In verify_published_handler, it is used to extract hashes and filenames after verifying the signature of the SHA256SUMS file. The extracted data is then filtered to remove binaries that are not hosted by the specified source. In verify_binaries_handler, parse_sums_file is utilized to extract hashes and filenames from a user-specified sums file, which are then verified against the provided binaries. Both handlers rely on the output of parse_sums_file to ensure that the correct binaries are being processed for verification.\n\n**Note**: It is important to ensure that the sums_file_path provided points to a valid file with the expected format. If the filename_filter is not used, the function will return all entries from the hash file, which may include unwanted binaries.\n\n**Output Example**: An example of the return value from parse_sums_file could be:\n```\n[\n    ['abc123hash', 'binary_file_1'],\n    ['def456hash', 'binary_file_2']\n]\n``` \nThis output indicates that two binaries, 'binary_file_1' and 'binary_file_2', have been extracted along with their corresponding hashes from the specified hash file."
      ],
      "code_start_line": 432,
      "code_end_line": 436,
      "params": [
        "sums_file_path",
        "filename_filter"
      ],
      "have_return": true,
      "code_content": "def parse_sums_file(sums_file_path: str, filename_filter: t.List[str]) -> t.List[t.List[str]]:\n    # extract hashes/filenames of binaries to verify from hash file;\n    # each line has the following format: \"<hash> <binary_filename>\"\n    with open(sums_file_path, 'r', encoding='utf8') as hash_file:\n        return [line.split()[:2] for line in hash_file if len(filename_filter) == 0 or any(f in line for f in filename_filter)]\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "dataset/verify.py/verify_published_handler",
        "dataset/verify.py/verify_binaries_handler"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "verify_binary_hashes",
      "md_content": [
        "**verify_binary_hashes**: The function of verify_binary_hashes is to verify the integrity of binary files by comparing their calculated SHA256 hashes against expected values.\n\n**parameters**: The parameters of this Function.\n· hashes_to_verify: A list of lists, where each inner list contains an expected hash and the corresponding binary filename to verify.\n\n**Code Description**: The verify_binary_hashes function is designed to ensure the integrity of binary files by calculating their SHA256 hashes and comparing them to expected values provided in the hashes_to_verify parameter. The function begins by initializing two lists: offending_files, which will store the names of files that fail the hash check, and files_to_hashes, which will map binary filenames to their calculated hashes.\n\nThe function iterates over each pair of expected hash and binary filename from the hashes_to_verify list. For each binary file, it opens the file in binary read mode and computes its SHA256 hash using the sha256 function. If the calculated hash does not match the expected hash, the filename is added to the offending_files list. If the hashes match, the filename and its calculated hash are stored in the files_to_hashes dictionary.\n\nAfter processing all files, if there are any offending files, the function logs a critical error message detailing which files failed the integrity check and returns a tuple containing ReturnCode.INTEGRITY_FAILURE and the files_to_hashes dictionary. If all files pass the integrity check, the function returns a tuple with ReturnCode.SUCCESS and the files_to_hashes dictionary.\n\nThis function is called by verify_published_handler and verify_binaries_handler functions within the same module. In verify_published_handler, it is invoked after downloading binaries and verifying their signatures, ensuring that the downloaded files are intact and have not been tampered with. In verify_binaries_handler, it is called after verifying the signature of the SHA256SUMS file and extracting the hashes to verify, ensuring that the specified binaries match their expected hashes.\n\n**Note**: It is important to handle the return values of this function appropriately in the calling functions to ensure that any integrity failures are logged and managed correctly. This practice is crucial for maintaining the robustness of the verification process.\n\n**Output Example**: A possible return value of the function could be:\n- If all hashes match: (ReturnCode.SUCCESS, {'binary1': 'calculated_hash1', 'binary2': 'calculated_hash2'})\n- If there are integrity failures: (ReturnCode.INTEGRITY_FAILURE, {'binary1': 'calculated_hash1'})"
      ],
      "code_start_line": 439,
      "code_end_line": 458,
      "params": [
        "hashes_to_verify"
      ],
      "have_return": true,
      "code_content": "def verify_binary_hashes(hashes_to_verify: t.List[t.List[str]]) -> t.Tuple[ReturnCode, t.Dict[str, str]]:\n    offending_files = []\n    files_to_hashes = {}\n\n    for hash_expected, binary_filename in hashes_to_verify:\n        with open(binary_filename, 'rb') as binary_file:\n            hash_calculated = sha256(binary_file.read()).hexdigest()\n        if hash_calculated != hash_expected:\n            offending_files.append(binary_filename)\n        else:\n            files_to_hashes[binary_filename] = hash_calculated\n\n    if offending_files:\n        joined_files = '\\n'.join(offending_files)\n        log.critical(\n            \"Hashes don't match.\\n\"\n            f\"Offending files:\\n{joined_files}\")\n        return (ReturnCode.INTEGRITY_FAILURE, files_to_hashes)\n\n    return (ReturnCode.SUCCESS, files_to_hashes)\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "dataset/verify.py/verify_published_handler",
        "dataset/verify.py/verify_binaries_handler"
      ],
      "reference_who": [
        "dataset/verify.py/ReturnCode"
      ],
      "special_reference_type": [
        true
      ]
    },
    {
      "type": "FunctionDef",
      "name": "verify_published_handler",
      "md_content": [
        "**verify_published_handler**: The function of verify_published_handler is to verify the integrity and authenticity of published Bitcoin binaries based on a specified version.\n\n**parameters**: The parameters of this Function.\n· args: An instance of argparse.Namespace that contains command-line arguments, including the version of the Bitcoin release to verify, cleanup options, and host requirements for signature verification.\n\n**Code Description**: The verify_published_handler function orchestrates the process of verifying published Bitcoin binaries by performing several key operations. It begins by establishing a working directory in the system's temporary directory, specifically named according to the version provided in the arguments.\n\nThe function first attempts to parse the version string using the parse_version_string function, which extracts the base version, release candidate (if applicable), and operating system information. If parsing fails, it logs an error and returns a ReturnCode indicating a bad version.\n\nNext, the function constructs the remote directory path for the binaries and their associated signatures based on the parsed version information. It then creates the working directory and changes the current working directory to this new location.\n\nThe function retrieves signature files from specified hosts using the get_files_from_hosts_and_compare function, which ensures that the signatures are identical across the hosts. If the retrieval is unsuccessful, it returns the corresponding error code.\n\nFollowing this, the function checks if the version is suitable for multi-signature verification. If the version is below 22.0, it logs an error indicating that single signature verification is not supported and returns a bad version code.\n\nThe function then retrieves checksum files in a similar manner and verifies the integrity of the SHA256SUMS file against its signature using the verify_shasums_signature function. If this verification fails, it cleans up the working directory if necessary and returns the appropriate error code.\n\nOnce the signatures and checksums are verified, the function extracts the hashes and filenames of the binaries to be verified using the parse_sums_file function. It filters out any binaries that are not hosted by the specified source.\n\nThe function proceeds to download the binaries using the download_with_wget function, logging the status of each download. If any downloads fail, it returns an error code indicating the failure.\n\nFinally, the function verifies the integrity of the downloaded binaries by comparing their hashes against the expected values using the verify_binary_hashes function. If all checks pass, it outputs the results, either in JSON format or as a simple list of verified binaries, and returns a success code.\n\nThis function is called by the main function of the module, which sets up the command-line interface for the script. The main function defines a subcommand \"pub\" that links directly to verify_published_handler, allowing users to invoke this verification process through command-line arguments.\n\n**Note**: It is essential to ensure that the version string provided adheres to the expected format, and that the necessary network access is available to retrieve files from the specified hosts. Additionally, users should be aware of the cleanup option, which determines whether temporary files are removed after the verification process.\n\n**Output Example**: A possible return value of the function could be ReturnCode.SUCCESS, indicating that all binaries have been successfully verified, along with a printed list of verified binaries or a JSON output containing detailed verification results."
      ],
      "code_start_line": 461,
      "code_end_line": 567,
      "params": [
        "args"
      ],
      "have_return": true,
      "code_content": "def verify_published_handler(args: argparse.Namespace) -> ReturnCode:\n    WORKINGDIR = Path(tempfile.gettempdir()) / f\"bitcoin_verify_binaries.{args.version}\"\n\n    def cleanup():\n        log.info(\"cleaning up files\")\n        os.chdir(Path.home())\n        shutil.rmtree(WORKINGDIR)\n\n    # determine remote dir dependent on provided version string\n    try:\n        version_base, version_rc, os_filter = parse_version_string(args.version)\n        version_tuple = [int(i) for i in version_base.split('.')]\n    except Exception as e:\n        log.debug(e)\n        log.error(f\"unable to parse version; expected format is {VERSION_FORMAT}\")\n        log.error(f\"  e.g. {VERSION_EXAMPLE}\")\n        return ReturnCode.BAD_VERSION\n\n    remote_dir = f\"/bin/{VERSIONPREFIX}{version_base}/\"\n    if version_rc:\n        remote_dir += f\"test.{version_rc}/\"\n    remote_sigs_path = remote_dir + SIGNATUREFILENAME\n    remote_sums_path = remote_dir + SUMS_FILENAME\n\n    # create working directory\n    os.makedirs(WORKINGDIR, exist_ok=True)\n    os.chdir(WORKINGDIR)\n\n    hosts = [HOST1, HOST2]\n\n    got_sig_status = get_files_from_hosts_and_compare(\n        hosts, remote_sigs_path, SIGNATUREFILENAME, args.require_all_hosts)\n    if got_sig_status != ReturnCode.SUCCESS:\n        return got_sig_status\n\n    # Multi-sig verification is available after 22.0.\n    if version_tuple[0] < 22:\n        log.error(\"Version too old - single sig not supported. Use a previous \"\n                  \"version of this script from the repo.\")\n        return ReturnCode.BAD_VERSION\n\n    got_sums_status = get_files_from_hosts_and_compare(\n        hosts, remote_sums_path, SUMS_FILENAME, args.require_all_hosts)\n    if got_sums_status != ReturnCode.SUCCESS:\n        return got_sums_status\n\n    # Verify the signature on the SHA256SUMS file\n    sigs_status, good_trusted, good_untrusted, unknown, bad = verify_shasums_signature(SIGNATUREFILENAME, SUMS_FILENAME, args)\n    if sigs_status != ReturnCode.SUCCESS:\n        if sigs_status == ReturnCode.INTEGRITY_FAILURE:\n            cleanup()\n        return sigs_status\n\n    # Extract hashes and filenames\n    hashes_to_verify = parse_sums_file(SUMS_FILENAME, [os_filter])\n    if not hashes_to_verify:\n        log.error(\"no files matched the platform specified\")\n        return ReturnCode.NO_BINARIES_MATCH\n\n    # remove binaries that are known not to be hosted by bitcoincore.org\n    fragments_to_remove = ['-unsigned', '-debug', '-codesignatures']\n    for fragment in fragments_to_remove:\n        nobinaries = [i for i in hashes_to_verify if fragment in i[1]]\n        if nobinaries:\n            remove_str = ', '.join(i[1] for i in nobinaries)\n            log.info(\n                f\"removing *{fragment} binaries ({remove_str}) from verification \"\n                f\"since {HOST1} does not host *{fragment} binaries\")\n            hashes_to_verify = [i for i in hashes_to_verify if fragment not in i[1]]\n\n    # download binaries\n    for _, binary_filename in hashes_to_verify:\n        log.info(f\"downloading {binary_filename} to {WORKINGDIR}\")\n        success, output = download_with_wget(\n            HOST1 + remote_dir + binary_filename, binary_filename)\n\n        if not success:\n            log.error(\n                f\"failed to download {binary_filename}\\n\"\n                f\"wget output:\\n{indent(output)}\")\n            return ReturnCode.BINARY_DOWNLOAD_FAILED\n\n    # verify hashes\n    hashes_status, files_to_hashes = verify_binary_hashes(hashes_to_verify)\n    if hashes_status != ReturnCode.SUCCESS:\n        return hashes_status\n\n\n    if args.cleanup:\n        cleanup()\n    else:\n        log.info(f\"did not clean up {WORKINGDIR}\")\n\n    if args.json:\n        output = {\n            'good_trusted_sigs': [str(s) for s in good_trusted],\n            'good_untrusted_sigs': [str(s) for s in good_untrusted],\n            'unknown_sigs': [str(s) for s in unknown],\n            'bad_sigs': [str(s) for s in bad],\n            'verified_binaries': files_to_hashes,\n        }\n        print(json.dumps(output, indent=2))\n    else:\n        for filename in files_to_hashes:\n            print(f\"VERIFIED: {filename}\")\n\n    return ReturnCode.SUCCESS\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "dataset/verify.py/main"
      ],
      "reference_who": [
        "dataset/verify.py/ReturnCode",
        "dataset/verify.py/indent",
        "dataset/verify.py/parse_version_string",
        "dataset/verify.py/download_with_wget",
        "dataset/verify.py/get_files_from_hosts_and_compare",
        "dataset/verify.py/verify_shasums_signature",
        "dataset/verify.py/parse_sums_file",
        "dataset/verify.py/verify_binary_hashes"
      ],
      "special_reference_type": [
        true,
        false,
        false,
        false,
        false,
        false,
        false,
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "cleanup",
      "md_content": [
        "**cleanup**: The function of cleanup is to remove temporary files and reset the working directory.\n\n**parameters**: The parameters of this Function.\n· There are no parameters for this function.\n\n**Code Description**: The cleanup function is designed to perform a cleanup operation by removing temporary files and resetting the working environment. The function begins by logging an informational message indicating that the cleanup process is starting. This is done using the `log.info` method, which helps in tracking the execution flow and debugging if necessary.\n\nNext, the function changes the current working directory to the user's home directory using `os.chdir(Path.home())`. This step is crucial as it ensures that any subsequent file operations are performed in a known and safe location, preventing accidental modifications to files in other directories.\n\nFinally, the function deletes the directory specified by the `WORKINGDIR` variable and all its contents using `shutil.rmtree(WORKINGDIR)`. This method is powerful as it recursively removes a directory and all its files and subdirectories, effectively cleaning up any temporary files that may have been created during the execution of the program.\n\n**Note**: It is important to ensure that the `WORKINGDIR` variable is correctly defined and points to the intended directory before calling this function. Additionally, users should be cautious when using `shutil.rmtree` as it permanently deletes files and directories without recovery options."
      ],
      "code_start_line": 464,
      "code_end_line": 467,
      "params": [],
      "have_return": false,
      "code_content": "    def cleanup():\n        log.info(\"cleaning up files\")\n        os.chdir(Path.home())\n        shutil.rmtree(WORKINGDIR)\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "verify_binaries_handler",
      "md_content": [
        "**verify_binaries_handler**: The function of verify_binaries_handler is to verify the integrity and authenticity of specified binary files against a SHA256SUMS file and its associated GPG signature.\n\n**parameters**: The parameters of this Function.\n· args: An instance of argparse.Namespace containing command-line arguments that influence the behavior of the function, including paths to binary files, SHA256SUMS file, and signature file.\n\n**Code Description**: The verify_binaries_handler function is designed to handle the verification process of binary files by checking their signatures and hashes against a provided SHA256SUMS file. The function begins by creating a mapping of binary file names to their respective paths based on the input arguments. If a signature file is specified, it uses that; otherwise, it defaults to assuming the signature file is the SHA256SUMS file with an \".asc\" suffix.\n\nThe function then calls verify_shasums_signature to validate the signature of the SHA256SUMS file. If the signature verification fails, the function returns the corresponding ReturnCode. Following successful signature verification, it extracts the hashes and filenames from the SHA256SUMS file using the parse_sums_file function. If no matching hashes are found for the specified binaries, it logs an error and returns a ReturnCode indicating that no binaries match.\n\nNext, the function ensures that all specified binaries are accounted for by comparing the extracted hashes with the provided binaries. If any binaries are missing, it logs an error. The function then proceeds to verify the integrity of the binaries by calling verify_binary_hashes, which checks the calculated SHA256 hashes against the expected values. If any integrity checks fail, it returns the appropriate ReturnCode.\n\nDepending on the command-line arguments, the function outputs the results in either a human-readable format or as JSON. It concludes by returning ReturnCode.SUCCESS if all operations are successful.\n\nThis function is called by the main function within the same module, specifically when the \"bin\" command is invoked. It serves as a critical component in the verification process, ensuring that the binaries are both authentic and intact before they are used.\n\n**Note**: When using the verify_binaries_handler function, it is essential to provide valid paths for the SHA256SUMS file and the binaries to be verified. Additionally, the GPG environment must be correctly configured for signature verification to succeed.\n\n**Output Example**: A possible return value from the function could be:\nReturnCode.SUCCESS"
      ],
      "code_start_line": 570,
      "code_end_line": 634,
      "params": [
        "args"
      ],
      "have_return": true,
      "code_content": "def verify_binaries_handler(args: argparse.Namespace) -> ReturnCode:\n    binary_to_basename = {}\n    for file in args.binary:\n        binary_to_basename[PurePath(file).name] = file\n\n    sums_sig_path = None\n    if args.sums_sig_file:\n        sums_sig_path = Path(args.sums_sig_file)\n    else:\n        log.info(f\"No signature file specified, assuming it is {args.sums_file}.asc\")\n        sums_sig_path = Path(args.sums_file).with_suffix(\".asc\")\n\n    # Verify the signature on the SHA256SUMS file\n    sigs_status, good_trusted, good_untrusted, unknown, bad = verify_shasums_signature(str(sums_sig_path), args.sums_file, args)\n    if sigs_status != ReturnCode.SUCCESS:\n        return sigs_status\n\n    # Extract hashes and filenames\n    hashes_to_verify = parse_sums_file(args.sums_file, [k for k, n in binary_to_basename.items()])\n    if not hashes_to_verify:\n        log.error(f\"No files in {args.sums_file} match the specified binaries\")\n        return ReturnCode.NO_BINARIES_MATCH\n\n    # Make sure all files are accounted for\n    sums_file_path = Path(args.sums_file)\n    missing_files = []\n    files_to_hash = []\n    if len(binary_to_basename) > 0:\n        for file_hash, file in hashes_to_verify:\n            files_to_hash.append([file_hash, binary_to_basename[file]])\n            del binary_to_basename[file]\n        if len(binary_to_basename) > 0:\n            log.error(f\"Not all specified binaries are in {args.sums_file}\")\n            return ReturnCode.NO_BINARIES_MATCH\n    else:\n        log.info(f\"No binaries specified, assuming all files specified in {args.sums_file} are located relatively\")\n        for file_hash, file in hashes_to_verify:\n            file_path = Path(sums_file_path.parent.joinpath(file))\n            if file_path.exists():\n                files_to_hash.append([file_hash, str(file_path)])\n            else:\n                missing_files.append(file)\n\n    # verify hashes\n    hashes_status, files_to_hashes = verify_binary_hashes(files_to_hash)\n    if hashes_status != ReturnCode.SUCCESS:\n        return hashes_status\n\n    if args.json:\n        output = {\n            'good_trusted_sigs': [str(s) for s in good_trusted],\n            'good_untrusted_sigs': [str(s) for s in good_untrusted],\n            'unknown_sigs': [str(s) for s in unknown],\n            'bad_sigs': [str(s) for s in bad],\n            'verified_binaries': files_to_hashes,\n            \"missing_binaries\": missing_files,\n        }\n        print(json.dumps(output, indent=2))\n    else:\n        for filename in files_to_hashes:\n            print(f\"VERIFIED: {filename}\")\n        for filename in missing_files:\n            print(f\"MISSING: {filename}\")\n\n    return ReturnCode.SUCCESS\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "dataset/verify.py/main"
      ],
      "reference_who": [
        "dataset/verify.py/ReturnCode",
        "dataset/verify.py/verify_shasums_signature",
        "dataset/verify.py/parse_sums_file",
        "dataset/verify.py/verify_binary_hashes"
      ],
      "special_reference_type": [
        true,
        false,
        false,
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "main",
      "md_content": [
        "**main**: The function of main is to serve as the entry point for the command-line interface of the verification tool, allowing users to specify various options and commands for verifying Bitcoin binaries and published releases.\n\n**parameters**: The parameters of this Function.\n· None (The function does not take any parameters directly; it utilizes argparse to handle command-line arguments.)\n\n**Code Description**: The main function initializes a command-line interface using the argparse library, which facilitates user interaction with the verification tool. It begins by creating an ArgumentParser instance, which is configured with a description derived from the module's docstring. The function then defines several command-line arguments that users can specify when executing the script.\n\nKey arguments include:\n- `-v` or `--verbose`: Enables verbose output if specified.\n- `-q` or `--quiet`: Suppresses output messages if specified.\n- `--import-keys`: Prompts the user to import unknown builder keys if specified.\n- `--min-good-sigs`: Sets the minimum number of good signatures required for successful verification, defaulting to 3 unless overridden by an environment variable.\n- `--keyserver`: Specifies the keyserver to use for key retrieval, with a default value set to 'hkps://keys.openpgp.org'.\n- `--trusted-keys`: Accepts a comma-separated list of trusted signer GPG keys.\n- `--json`: Outputs the results in JSON format if specified.\n\nThe function also establishes subcommands for the verification process:\n1. `pub`: This subcommand is linked to the `verify_published_handler` function, which handles the verification of published Bitcoin releases based on a specified version.\n2. `bin`: This subcommand is associated with the `verify_binaries_handler` function, which verifies local binary files against a SHA256SUMS file and its signature.\n\nAfter parsing the command-line arguments, the function checks the verbosity level and adjusts the logging settings accordingly. It then invokes the appropriate handler function (either `verify_published_handler` or `verify_binaries_handler`) based on the user's command, passing the parsed arguments to it.\n\nThe main function plays a crucial role in orchestrating the verification process by setting up the necessary configurations and delegating tasks to the respective handler functions. It ensures that users can easily interact with the tool and customize their verification experience through command-line options.\n\n**Note**: Users should ensure that they provide valid command-line arguments and that the necessary environment variables are set for optimal functionality. It is also important to follow the expected formats for version strings and file paths to avoid errors during execution.\n\n**Output Example**: The function does not return a value directly; instead, it triggers the execution of the specified verification process, which may result in output such as verification results printed to the console or in JSON format, depending on the user's command-line options."
      ],
      "code_start_line": 637,
      "code_end_line": 709,
      "params": [],
      "have_return": true,
      "code_content": "def main():\n    parser = argparse.ArgumentParser(description=__doc__)\n    parser.add_argument(\n        '-v', '--verbose', action='store_true',\n        default=bool_from_env('BINVERIFY_VERBOSE'),\n    )\n    parser.add_argument(\n        '-q', '--quiet', action='store_true',\n        default=bool_from_env('BINVERIFY_QUIET'),\n    )\n    parser.add_argument(\n        '--import-keys', action='store_true',\n        default=bool_from_env('BINVERIFY_IMPORTKEYS'),\n        help='if specified, ask to import each unknown builder key'\n    )\n    parser.add_argument(\n        '--min-good-sigs', type=int, action='store', nargs='?',\n        default=int(os.environ.get('BINVERIFY_MIN_GOOD_SIGS', 3)),\n        help=(\n            'The minimum number of good signatures to require successful termination.'),\n    )\n    parser.add_argument(\n        '--keyserver', action='store', nargs='?',\n        default=os.environ.get('BINVERIFY_KEYSERVER', 'hkps://keys.openpgp.org'),\n        help='which keyserver to use',\n    )\n    parser.add_argument(\n        '--trusted-keys', action='store', nargs='?',\n        default=os.environ.get('BINVERIFY_TRUSTED_KEYS', ''),\n        help='A list of trusted signer GPG keys, separated by commas. Not \"trusted keys\" in the GPG sense.',\n    )\n    parser.add_argument(\n        '--json', action='store_true',\n        default=bool_from_env('BINVERIFY_JSON'),\n        help='If set, output the result as JSON',\n    )\n\n    subparsers = parser.add_subparsers(title=\"Commands\", required=True, dest=\"command\")\n\n    pub_parser = subparsers.add_parser(\"pub\", help=\"Verify a published release.\")\n    pub_parser.set_defaults(func=verify_published_handler)\n    pub_parser.add_argument(\n        'version', type=str, help=(\n            f'version of the bitcoin release to download; of the format '\n            f'{VERSION_FORMAT}. Example: {VERSION_EXAMPLE}')\n    )\n    pub_parser.add_argument(\n        '--cleanup', action='store_true',\n        default=bool_from_env('BINVERIFY_CLEANUP'),\n        help='if specified, clean up files afterwards'\n    )\n    pub_parser.add_argument(\n        '--require-all-hosts', action='store_true',\n        default=bool_from_env('BINVERIFY_REQUIRE_ALL_HOSTS'),\n        help=(\n            f'If set, require all hosts ({HOST1}, {HOST2}) to provide signatures. '\n            '(Sometimes bitcoin.org lags behind bitcoincore.org.)')\n    )\n\n    bin_parser = subparsers.add_parser(\"bin\", help=\"Verify local binaries.\")\n    bin_parser.set_defaults(func=verify_binaries_handler)\n    bin_parser.add_argument(\"--sums-sig-file\", \"-s\", help=\"Path to the SHA256SUMS.asc file to verify\")\n    bin_parser.add_argument(\"sums_file\", help=\"Path to the SHA256SUMS file to verify\")\n    bin_parser.add_argument(\n        \"binary\", nargs=\"*\",\n        help=\"Path to a binary distribution file to verify. Can be specified multiple times for multiple files to verify.\"\n    )\n\n    args = parser.parse_args()\n    if args.quiet:\n        log.setLevel(logging.WARNING)\n\n    return args.func(args)\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "dataset/verify.py/bool_from_env",
        "dataset/verify.py/verify_published_handler",
        "dataset/verify.py/verify_binaries_handler"
      ],
      "special_reference_type": [
        false,
        false,
        false
      ]
    }
  ]
}